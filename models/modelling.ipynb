{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLEASE READ: DATA DOWNLOAD\n",
    "For ease of running this notebook, you can simply download all the files in the `Feature Sets` folder from Google Drive into the `/data/curated` directory. Then run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/curated/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/curated/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/curated/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/curated/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/curated/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/curated/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/curated/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain/domain_four_bed_house_rent.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/population.csv')\n",
    "education = pd.read_csv('../data/curated/education.csv')\n",
    "urban_landmarks = pd.read_csv('../data/curated/urban_landmarks.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')\n",
    "\n",
    "# Livability Index data \n",
    "livability_one_bed_flat = pd.read_csv('../data/curated/livability/livability_one_bed_flat.csv')\n",
    "livability_two_bed_flat = pd.read_csv('../data/curated/livability/livability_two_bed_flat.csv')\n",
    "livability_three_bed_flat = pd.read_csv('../data/curated/livability/livability_three_bed_flat.csv')\n",
    "livability_two_bed_house = pd.read_csv('../data/curated/livability/livability_two_bed_house.csv')\n",
    "livability_three_bed_house = pd.read_csv('../data/curated/livability/livability_three_bed_house.csv')\n",
    "livability_four_bed_house = pd.read_csv('../data/curated/livability/livability_four_bed_house.csv')\n",
    "livability_all_properties = pd.read_csv('../data/curated/livability/livability_all_properties.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_df(df):\n",
    "    \"\"\"\n",
    "    This function cleans the domain dataframes by removing\n",
    "    the 'Unnamed:' column, renaming median_rent to 'sep_median'\n",
    "    (for a standardised column name as in rental history dfs) and\n",
    "    also creates a year column and inputs the relevant year that\n",
    "    the data is from - 2024. \n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'sep_median'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the clean_domain_df function to all the domain dataframes\n",
    "domain_one_bed_flat = clean_domain_df(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_df(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_df(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_df(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_df(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_df(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_df(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties into the rental history dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_sep_2024_rental_data(rental_history_df, domain_df):\n",
    "    \"\"\"\n",
    "    This function retrieves all the median rental prices in \n",
    "    September from the domain dataframes and then imputes\n",
    "    them into the rental history dataframes where the year\n",
    "    is 2024 and month is September. \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge rental_history_df with domain_df on 'suburb' to keep all years from rental_history_df\n",
    "    merged_df = pd.merge(rental_history_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "\n",
    "    # Filter the dataframe to keep only the suburbs that appear 9 or more times\n",
    "    suburb_counts = merged_df['suburb'].value_counts()\n",
    "    suburbs_to_keep = suburb_counts[suburb_counts >= 9].index\n",
    "    merged_df = merged_df[merged_df['suburb'].isin(suburbs_to_keep)]\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Apply the function to each dataset \n",
    "one_bed_flat, domain_one_bed_flat = impute_sep_2024_rental_data(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_sep_2024_rental_data(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_sep_2024_rental_data(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_sep_2024_rental_data(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_sep_2024_rental_data(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_sep_2024_rental_data(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_sep_2024_rental_data(all_properties, domain_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, livability_one_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "one_bed_flat_merged = one_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, livability_two_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_flat_merged = two_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, livability_three_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_flat_merged = three_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, livability_two_bed_house, on=['suburb', 'year'], how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_house_merged = two_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, livability_three_bed_house, on=['suburb', 'year'], how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_house_merged = three_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, livability_four_bed_house, on=['suburb', 'year'], how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "four_bed_house_merged =four_bed_house_merged.drop_duplicates()\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, livability_all_properties, on=['suburb', 'year'], how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "all_properties_merged = all_properties_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_suburb_cols(df):\n",
    "    \"\"\"\n",
    "    This function removes all duplicates of the suburb column name \n",
    "    from the merged dataframes. The duplicate suburb column name \n",
    "    could be 'Unnamed', 'sa2_name' or 'gazetted_locality'.\n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_suburb_cols(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_suburb_cols(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_suburb_cols(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_suburb_cols(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_suburb_cols(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_suburb_cols(four_bed_house_merged)\n",
    "all_properties_merged = clean_suburb_cols(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_before_2025(df, median_columns):\n",
    "    \"\"\"\n",
    "    This function removes all suburbs that has missing median rent values in the data \n",
    "    from the years before 2025. This excludes suburbs which would have had NaN values\n",
    "    in our training set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows where year is less than 2025 and NaN values exist in any of the median columns\n",
    "    return df[~((df['year'] < 2025) & (df[median_columns].isnull().any(axis=1)))]\n",
    "\n",
    "# Define the median columns to check\n",
    "median_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# Call the function for each dataframe and reassign the cleaned data\n",
    "one_bed_flat_merged = remove_nan_before_2025(one_bed_flat_merged, median_columns)\n",
    "two_bed_flat_merged = remove_nan_before_2025(two_bed_flat_merged, median_columns)\n",
    "three_bed_flat_merged = remove_nan_before_2025(three_bed_flat_merged, median_columns)\n",
    "two_bed_house_merged = remove_nan_before_2025(two_bed_house_merged, median_columns)\n",
    "three_bed_house_merged = remove_nan_before_2025(three_bed_house_merged, median_columns)\n",
    "four_bed_house_merged = remove_nan_before_2025(four_bed_house_merged, median_columns)\n",
    "all_properties_merged = remove_nan_before_2025(all_properties_merged, median_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Properties Dataframe for Visualisation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_dataframes():\n",
    "    \"\"\"\n",
    "    This function saves all merged dataframes to the defined path.\n",
    "    \"\"\"\n",
    "    # Define the base path\n",
    "    base_path = '../data/curated/merged_feature_set'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    # Save each dataframe to a CSV file\n",
    "    one_bed_flat_merged.to_csv(os.path.join(base_path, 'one_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_flat_merged.to_csv(os.path.join(base_path, 'two_bed_flat_merged.csv'), index=False)\n",
    "    three_bed_flat_merged.to_csv(os.path.join(base_path, 'three_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_house_merged.to_csv(os.path.join(base_path, 'two_bed_house_merged.csv'), index=False)\n",
    "    three_bed_house_merged.to_csv(os.path.join(base_path, 'three_bed_house_merged.csv'), index=False)\n",
    "    four_bed_house_merged.to_csv(os.path.join(base_path, 'four_bed_house_merged.csv'), index=False)\n",
    "    all_properties_merged.to_csv(os.path.join(base_path, 'all_properties_merged.csv'), index=False)\n",
    "\n",
    "# Call the function\n",
    "save_merged_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    This function splits the dataframe into training, validation, and testing sets\n",
    "    based on the 'year' column:\n",
    "    - Training set: 2016-2021\n",
    "    - Validation set: 2022-2024\n",
    "    - Testing set: 2025-2027\n",
    "\n",
    "    It retains the year and suburb columns for each dataframe. This is why we call \n",
    "    these dataframes '_labels'. \n",
    "\n",
    "    The function returns:\n",
    "    - X_train, X_val, X_test: Feature sets\n",
    "    - y_train, y_val, y_test: Target sets\n",
    "    \"\"\"\n",
    "    # Define the year ranges\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Define target columns (excluding suburb and year from the drop)\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Keep suburb and year in the features\n",
    "    X = df.drop(columns=target_columns)  # This keeps 'suburb' and 'year' in X\n",
    "    y = df[['suburb', 'year'] + target_columns]  # Target includes suburb, year, and target columns\n",
    "\n",
    "    # Split into train, validation, and test sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    y_train = y[y['year'].isin(train_years)]\n",
    "    y_val = y[y['year'].isin(val_years)]\n",
    "    y_test = y[y['year'].isin(test_years)]\n",
    "\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels = train_val_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels = train_val_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels = train_val_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels = train_val_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels = train_val_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels = train_val_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels = train_val_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_labels(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    \"\"\"\n",
    "    This function removes the year and suburb columns from each \n",
    "    of the train, val and test dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drop the specified columns from X dataframes\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    # Drop the specified columns from y dataframes\n",
    "    y_train = y_train.drop(columns=['suburb', 'year'])\n",
    "    y_val = y_val.drop(columns=['suburb', 'year'])\n",
    "    y_test = y_test.drop(columns=['suburb', 'year'])\n",
    "    \n",
    "    # Return the modified dataframes without '_labels'\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# For each dataset, apply the function\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = drop_labels(\n",
    "    X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = drop_labels(\n",
    "    X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = drop_labels(\n",
    "    X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = drop_labels(\n",
    "    X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = drop_labels(\n",
    "    X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = drop_labels(\n",
    "    X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = drop_labels(\n",
    "    X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_feature_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    This function compares columns of the training, validation, and \n",
    "    testing feature dataframes (X). A dictionary is then returned\n",
    "    indicating if any columns are missing in each set or if all\n",
    "    the colums are the same. \n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "feature_dfs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_feature_columns(train, val, test) for name, (train, val, test) in feature_dfs.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1531         NaN         NaN         NaN         NaN\n",
      "1532         NaN         NaN         NaN         NaN\n",
      "1542         NaN         NaN         NaN         NaN\n",
      "1543         NaN         NaN         NaN         NaN\n",
      "1544         NaN         NaN         NaN         NaN\n",
      "\n",
      "[393 rows x 4 columns], 'y_test_two_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1990         NaN         NaN         NaN         NaN\n",
      "1991         NaN         NaN         NaN         NaN\n",
      "2001         NaN         NaN         NaN         NaN\n",
      "2002         NaN         NaN         NaN         NaN\n",
      "2003         NaN         NaN         NaN         NaN\n",
      "\n",
      "[501 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1444         NaN         NaN         NaN         NaN\n",
      "1445         NaN         NaN         NaN         NaN\n",
      "1455         NaN         NaN         NaN         NaN\n",
      "1456         NaN         NaN         NaN         NaN\n",
      "1457         NaN         NaN         NaN         NaN\n",
      "\n",
      "[369 rows x 4 columns], 'y_test_two_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1828         NaN         NaN         NaN         NaN\n",
      "1829         NaN         NaN         NaN         NaN\n",
      "1839         NaN         NaN         NaN         NaN\n",
      "1840         NaN         NaN         NaN         NaN\n",
      "1841         NaN         NaN         NaN         NaN\n",
      "\n",
      "[474 rows x 4 columns], 'y_test_three_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2308         NaN         NaN         NaN         NaN\n",
      "2309         NaN         NaN         NaN         NaN\n",
      "2319         NaN         NaN         NaN         NaN\n",
      "2320         NaN         NaN         NaN         NaN\n",
      "2321         NaN         NaN         NaN         NaN\n",
      "\n",
      "[585 rows x 4 columns], 'y_test_four_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "0            NaN         NaN         NaN         NaN\n",
      "1            NaN         NaN         NaN         NaN\n",
      "2            NaN         NaN         NaN         NaN\n",
      "12           NaN         NaN         NaN         NaN\n",
      "13           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2017         NaN         NaN         NaN         NaN\n",
      "2018         NaN         NaN         NaN         NaN\n",
      "2028         NaN         NaN         NaN         NaN\n",
      "2029         NaN         NaN         NaN         NaN\n",
      "2030         NaN         NaN         NaN         NaN\n",
      "\n",
      "[510 rows x 4 columns], 'y_test_all_properties':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2350         NaN         NaN         NaN         NaN\n",
      "2351         NaN         NaN         NaN         NaN\n",
      "2361         NaN         NaN         NaN         NaN\n",
      "2362         NaN         NaN         NaN         NaN\n",
      "2363         NaN         NaN         NaN         NaN\n",
      "\n",
      "[591 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    393\n",
      "jun_median    393\n",
      "mar_median    393\n",
      "sep_median    393\n",
      "dtype: int64, 'y_test_two_bed': dec_median    501\n",
      "jun_median    501\n",
      "mar_median    501\n",
      "sep_median    501\n",
      "dtype: int64, 'y_test_three_bed': dec_median    369\n",
      "jun_median    369\n",
      "mar_median    369\n",
      "sep_median    369\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    474\n",
      "jun_median    474\n",
      "mar_median    474\n",
      "sep_median    474\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    585\n",
      "jun_median    585\n",
      "mar_median    585\n",
      "sep_median    585\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    510\n",
      "jun_median    510\n",
      "mar_median    510\n",
      "sep_median    510\n",
      "dtype: int64, 'y_test_all_properties': dec_median    591\n",
      "jun_median    591\n",
      "mar_median    591\n",
      "sep_median    591\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe by columns \n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of all training, validation, and test sets\n",
    "ML_dfs = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "def rfecv_with_random_forest(X_train, X_val, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Apply Recursive Feature Elimination with Cross-Validation (RFECV) using\n",
    "    Random Forest as the estimator to automatically select the optimal number\n",
    "    of features, then use GridSearchCV to fine-tune hyperparameters on the reduced feature set.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training feature set\n",
    "    X_val: Validation feature set\n",
    "    X_test: Test feature set\n",
    "    y_train: Training labels (target)\n",
    "    \n",
    "    Returns:\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv: Reduced datasets\n",
    "    best_rf: Best tuned Random Forest model after feature selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Use RFECV to automatically select the optimal number of features\n",
    "    rfecv = RFECV(estimator=rf_model, step=1, cv=KFold(5), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the datasets based on the selected features\n",
    "    X_train_rfecv = rfecv.transform(X_train_scaled)\n",
    "    X_val_rfecv = rfecv.transform(X_val_scaled)\n",
    "    X_test_rfecv = rfecv.transform(X_test_scaled)\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV on the reduced feature set\n",
    "    rf_after_rfecv = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(rf_after_rfecv, param_grid, v, cv=5)\n",
    "    grid_search.fit(X_train_rfecv, y_train)\n",
    "    \n",
    "    # Select the best Random Forest model based on GridSearchCV\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # Get the selected feature indices and names\n",
    "    selected_features = rfecv.get_support(indices=True)\n",
    "    selected_feature_names = [X_train.columns[i] for i in selected_features]  # Use X_train columns\n",
    "\n",
    "    # Get the feature importance values from the trained Random Forest model (best_rf)\n",
    "    importances = best_rf.feature_importances_\n",
    "\n",
    "    # Pair selected features with their corresponding importance values\n",
    "    feature_importance_pairs = list(zip(selected_feature_names, importances))\n",
    "\n",
    "    # Sort the features by importance values in descending order\n",
    "    sorted_features = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted features with their importance values\n",
    "    print(f\"Optimal number of features selected: {rfecv.n_features_}\")\n",
    "    print(f\"Selected feature names: {selected_feature_names}\")\n",
    "    print(f\"Selected features sorted by importance:\")\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    return X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Type: one_bed_flat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProperty Type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproperty_types[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform feature selection and get the best model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mrfecv_with_random_forest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate on the validation set\u001b[39;00m\n\u001b[1;32m     18\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m best_rf\u001b[38;5;241m.\u001b[39mpredict(X_val_rfecv)\n",
      "Cell \u001b[0;32mIn[25], line 36\u001b[0m, in \u001b[0;36mrfecv_with_random_forest\u001b[0;34m(X_train, X_val, X_test, y_train)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Use RFECV to automatically select the optimal number of features\u001b[39;00m\n\u001b[1;32m     35\u001b[0m rfecv \u001b[38;5;241m=\u001b[39m RFECV(estimator\u001b[38;5;241m=\u001b[39mrf_model, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39mKFold(\u001b[38;5;241m5\u001b[39m), scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m \u001b[43mrfecv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Transform the datasets based on the selected features\u001b[39;00m\n\u001b[1;32m     39\u001b[0m X_train_rfecv \u001b[38;5;241m=\u001b[39m rfecv\u001b[38;5;241m.\u001b[39mtransform(X_train_scaled)\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:777\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    774\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m    775\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m--> 777\u001b[0m scores_features \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m scores, step_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mscores_features)\n\u001b[1;32m    783\u001b[0m step_n_features_rev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(step_n_features[\u001b[38;5;241m0\u001b[39m])[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialise the predictions dictionary and MAE list\n",
    "predictions_dict = {}\n",
    "mae_list = []\n",
    "\n",
    "# Property Types\n",
    "property_types = ['one_bed_flat', 'two_bed_flat', 'three_bed_flat', \n",
    "                  'two_bed_house', 'three_bed_house', 'four_bed_house', 'all_properties']\n",
    "\n",
    "# Loop through each set, perform RFE, and predict using the tuned Random Forest model\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    print(f\"Property Type: {property_types[i]}\")\n",
    "    # Perform feature selection and get the best model\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names = rfecv_with_random_forest(\n",
    "        X_train, X_val, X_test, y_train\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_rf.predict(X_val_rfecv)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Save MAE\n",
    "    mae_list.append(val_mae)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f\"Best n_estimators: {best_rf.n_estimators}, Best max_depth: {best_rf.max_depth}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Combine the training and validation sets for final model training\n",
    "    X_train_val_rfecv = np.vstack((X_train_rfecv, X_val_rfecv))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Retrain the model using the combined training and validation sets\n",
    "    best_rf.fit(X_train_val_rfecv, y_train_val)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_rf.predict(X_test_rfecv)\n",
    "    \n",
    "    # Store predictions in the dictionary with dataset index as key\n",
    "    predictions_dict[f'X_test_{i+1}_predictions'] = y_test_pred\n",
    "    \n",
    "    # Print the predictions for the test set\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path\n",
    "base_path = '../data/curated/predictions'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(base_path):\n",
    "    os.makedirs(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved one_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved four_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved all_properties with 'year', 'suburb', and predictions.\n"
     ]
    }
   ],
   "source": [
    "# Prediction Column Names\n",
    "column_names = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# labelled_dfs corresponds to the datasets with labels for each property type\n",
    "labelled_dfs = [X_test_one_bed_labels, X_test_two_bed_labels, X_test_three_bed_labels, \n",
    "                X_test_two_bed_house_labels, X_test_three_bed_house_labels, \n",
    "                X_test_four_bed_house_labels, X_test_all_properties_labels]\n",
    "\n",
    "# Iterate through the predictions dictionary and labelled DataFrames\n",
    "for i, (key, value) in enumerate(predictions_dict.items()):\n",
    "    # Select only the 'year' and 'suburb' columns from the labelled_dfs\n",
    "    labelled_df_subset = labelled_dfs[i][['suburb', 'year']]\n",
    "\n",
    "    # Initialise an empty DataFrame for the predictions\n",
    "    predictions_df = pd.DataFrame()\n",
    "\n",
    "    # value is a 2D array with multiple rows and 4 columns (n_samples, 4)\n",
    "    # Add each column using the custom names for the median values\n",
    "    for j in range(value.shape[1]):\n",
    "        predictions_df[column_names[j]] = value[:, j]\n",
    "\n",
    "    # Reset the index for both DataFrames to ensure proper alignment\n",
    "    labelled_df_subset = labelled_df_subset.reset_index(drop=True)\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the 'year' and 'suburb' columns with the predictions_df\n",
    "    labelled_with_predictions = pd.concat([labelled_df_subset, predictions_df], axis=1)\n",
    "\n",
    "    # Save the new DataFrame with only 'year', 'suburb', and predictions to a CSV file\n",
    "    labelled_with_predictions.to_csv(f\"../data/curated/predictions/{property_types[i]}_predictions.csv\", index=False)\n",
    "\n",
    "    # Print confirmation\n",
    "    print(f\"Saved {property_types[i]} with 'year', 'suburb', and predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE list has been saved to '../data/curated/predictions/mae.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert the MAE list to a pandas DataFrame\n",
    "mae_df = pd.DataFrame(mae_list, columns=['MAE'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mae_df.to_csv(\"../data/curated/predictions/mae.csv\", index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"MAE list has been saved to '../data/curated/predictions/mae.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
