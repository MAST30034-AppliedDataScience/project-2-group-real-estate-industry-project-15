{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')\n",
    "\n",
    "# Livability Index data \n",
    "livability_one_bed_flat = pd.read_csv('../data/curated/livability/livability_one_bed_flat.csv')\n",
    "livability_two_bed_flat = pd.read_csv('../data/curated/livability/livability_two_bed_flat.csv')\n",
    "livability_three_bed_flat = pd.read_csv('../data/curated/livability/livability_three_bed_flat.csv')\n",
    "livability_two_bed_house = pd.read_csv('../data/curated/livability/livability_two_bed_house.csv')\n",
    "livability_three_bed_house = pd.read_csv('../data/curated/livability/livability_three_bed_house.csv')\n",
    "livability_four_bed_house = pd.read_csv('../data/curated/livability/livability_four_bed_house.csv')\n",
    "livability_all_properties = pd.read_csv('../data/curated/livability/livability_all_properties.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_df(df):\n",
    "    \"\"\"\n",
    "    This function cleans the domain dataframes by removing\n",
    "    the 'Unnamed:' column, renaming median_rent to 'sep_median'\n",
    "    (for a standardised column name as in rental history dfs) and\n",
    "    also creates a year column and inputs the relevant year that\n",
    "    the data is from - 2024. \n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'sep_median'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the clean_domain_df function to all the domain dataframes\n",
    "domain_one_bed_flat = clean_domain_df(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_df(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_df(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_df(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_df(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_df(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_df(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties into the rental history dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_sep_2024_rental_data(rental_history_df, domain_df):\n",
    "    \"\"\"\n",
    "    This function retrieves all the median rental prices in \n",
    "    September from the domain dataframes and then imputes\n",
    "    them into the rental history dataframes where the year\n",
    "    is 2024 and month is September. \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge rental_history_df with domain_df on 'suburb' to keep all years from rental_history_df\n",
    "    merged_df = pd.merge(rental_history_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "\n",
    "    # Filter the dataframe to keep only the suburbs that appear 9 or more times\n",
    "    suburb_counts = merged_df['suburb'].value_counts()\n",
    "    suburbs_to_keep = suburb_counts[suburb_counts >= 9].index\n",
    "    merged_df = merged_df[merged_df['suburb'].isin(suburbs_to_keep)]\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Apply the function to each dataset \n",
    "one_bed_flat, domain_one_bed_flat = impute_sep_2024_rental_data(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_sep_2024_rental_data(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_sep_2024_rental_data(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_sep_2024_rental_data(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_sep_2024_rental_data(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_sep_2024_rental_data(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_sep_2024_rental_data(all_properties, domain_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, livability_one_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "one_bed_flat_merged = one_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, livability_two_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_flat_merged = two_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, livability_three_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_flat_merged = three_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, livability_two_bed_house, on=['suburb', 'year'], how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_house_merged = two_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, livability_three_bed_house, on=['suburb', 'year'], how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_house_merged = three_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, livability_four_bed_house, on=['suburb', 'year'], how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "four_bed_house_merged =four_bed_house_merged.drop_duplicates()\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, livability_all_properties, on=['suburb', 'year'], how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "all_properties_merged = all_properties_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_suburb_cols(df):\n",
    "    \"\"\"\n",
    "    This function removes all duplicated of the suburb column name \n",
    "    from the merged dataframes. The duplicate suburb column name \n",
    "    could be 'Unnamed', 'sa2_name' or 'gazetted_locality'.\n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_suburb_cols(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_suburb_cols(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_suburb_cols(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_suburb_cols(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_suburb_cols(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_suburb_cols(four_bed_house_merged)\n",
    "all_properties_merged = clean_suburb_cols(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_before_2025(df, median_columns):\n",
    "    # Drop rows where year is less than 2025 and NaN values exist in any of the median columns\n",
    "    return df[~((df['year'] < 2025) & (df[median_columns].isnull().any(axis=1)))]\n",
    "\n",
    "# Define the median columns to check\n",
    "median_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# Call the function for each dataframe and reassign the cleaned data\n",
    "one_bed_flat_merged = remove_nan_before_2025(one_bed_flat_merged, median_columns)\n",
    "two_bed_flat_merged = remove_nan_before_2025(two_bed_flat_merged, median_columns)\n",
    "three_bed_flat_merged = remove_nan_before_2025(three_bed_flat_merged, median_columns)\n",
    "two_bed_house_merged = remove_nan_before_2025(two_bed_house_merged, median_columns)\n",
    "three_bed_house_merged = remove_nan_before_2025(three_bed_house_merged, median_columns)\n",
    "four_bed_house_merged = remove_nan_before_2025(four_bed_house_merged, median_columns)\n",
    "all_properties_merged = remove_nan_before_2025(all_properties_merged, median_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Properties Dataframe for Visualisation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_dataframes():\n",
    "    # Define the base path\n",
    "    base_path = '../data/curated/merged_feature_set'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    # Save each dataframe to a CSV file\n",
    "    one_bed_flat_merged.to_csv(os.path.join(base_path, 'one_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_flat_merged.to_csv(os.path.join(base_path, 'two_bed_flat_merged.csv'), index=False)\n",
    "    three_bed_flat_merged.to_csv(os.path.join(base_path, 'three_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_house_merged.to_csv(os.path.join(base_path, 'two_bed_house_merged.csv'), index=False)\n",
    "    three_bed_house_merged.to_csv(os.path.join(base_path, 'three_bed_house_merged.csv'), index=False)\n",
    "    four_bed_house_merged.to_csv(os.path.join(base_path, 'four_bed_house_merged.csv'), index=False)\n",
    "    all_properties_merged.to_csv(os.path.join(base_path, 'all_properties_merged.csv'), index=False)\n",
    "\n",
    "# Call the function\n",
    "save_merged_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    This function splits the dataframe into training, validation, and testing sets\n",
    "    based on the 'year' column:\n",
    "    - Training set: 2016-2021\n",
    "    - Validation set: 2022-2024\n",
    "    - Testing set: 2025-2027\n",
    "\n",
    "    It also merges additional columns that are not part of the \n",
    "    features specific to years or target columns back with the \n",
    "    respective sets based on matching suburbs.\n",
    "\n",
    "    The function returns:\n",
    "    - X_train, X_val, X_test: Feature sets\n",
    "    - y_train, y_val, y_test: Target sets\n",
    "    \"\"\"\n",
    "    # Define the year ranges\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Define target columns (excluding suburb and year from the drop)\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Keep suburb and year in the features\n",
    "    X = df.drop(columns=target_columns)  # This keeps 'suburb' and 'year' in X\n",
    "    y = df[['suburb', 'year'] + target_columns]  # Target includes suburb, year, and target columns\n",
    "\n",
    "    # Split into train, validation, and test sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    y_train = y[y['year'].isin(train_years)]\n",
    "    y_val = y[y['year'].isin(val_years)]\n",
    "    y_test = y[y['year'].isin(test_years)]\n",
    "\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels = train_val_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels = train_val_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels = train_val_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels = train_val_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels = train_val_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels = train_val_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels = train_val_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_labels(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Drop the specified columns from X dataframes\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    # Drop the specified columns from y dataframes\n",
    "    y_train = y_train.drop(columns=['suburb', 'year'])\n",
    "    y_val = y_val.drop(columns=['suburb', 'year'])\n",
    "    y_test = y_test.drop(columns=['suburb', 'year'])\n",
    "    \n",
    "    # Return the modified dataframes without '_labels'\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# For each dataset, apply the function\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = drop_labels(\n",
    "    X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = drop_labels(\n",
    "    X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = drop_labels(\n",
    "    X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = drop_labels(\n",
    "    X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = drop_labels(\n",
    "    X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = drop_labels(\n",
    "    X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = drop_labels(\n",
    "    X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_feature_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    This function compares columns of the training, validation, and \n",
    "    testing feature dataframes (X). A dictionary is then returned\n",
    "    indicating if any columns are missing in each set or if all\n",
    "    the colums are the same. \n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "feature_dfs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_feature_columns(train, val, test) for name, (train, val, test) in feature_dfs.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1531         NaN         NaN         NaN         NaN\n",
      "1532         NaN         NaN         NaN         NaN\n",
      "1542         NaN         NaN         NaN         NaN\n",
      "1543         NaN         NaN         NaN         NaN\n",
      "1544         NaN         NaN         NaN         NaN\n",
      "\n",
      "[393 rows x 4 columns], 'y_test_two_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1990         NaN         NaN         NaN         NaN\n",
      "1991         NaN         NaN         NaN         NaN\n",
      "2001         NaN         NaN         NaN         NaN\n",
      "2002         NaN         NaN         NaN         NaN\n",
      "2003         NaN         NaN         NaN         NaN\n",
      "\n",
      "[501 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1444         NaN         NaN         NaN         NaN\n",
      "1445         NaN         NaN         NaN         NaN\n",
      "1455         NaN         NaN         NaN         NaN\n",
      "1456         NaN         NaN         NaN         NaN\n",
      "1457         NaN         NaN         NaN         NaN\n",
      "\n",
      "[369 rows x 4 columns], 'y_test_two_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1828         NaN         NaN         NaN         NaN\n",
      "1829         NaN         NaN         NaN         NaN\n",
      "1839         NaN         NaN         NaN         NaN\n",
      "1840         NaN         NaN         NaN         NaN\n",
      "1841         NaN         NaN         NaN         NaN\n",
      "\n",
      "[474 rows x 4 columns], 'y_test_three_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2308         NaN         NaN         NaN         NaN\n",
      "2309         NaN         NaN         NaN         NaN\n",
      "2319         NaN         NaN         NaN         NaN\n",
      "2320         NaN         NaN         NaN         NaN\n",
      "2321         NaN         NaN         NaN         NaN\n",
      "\n",
      "[585 rows x 4 columns], 'y_test_four_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "0            NaN         NaN         NaN         NaN\n",
      "1            NaN         NaN         NaN         NaN\n",
      "2            NaN         NaN         NaN         NaN\n",
      "12           NaN         NaN         NaN         NaN\n",
      "13           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2017         NaN         NaN         NaN         NaN\n",
      "2018         NaN         NaN         NaN         NaN\n",
      "2028         NaN         NaN         NaN         NaN\n",
      "2029         NaN         NaN         NaN         NaN\n",
      "2030         NaN         NaN         NaN         NaN\n",
      "\n",
      "[510 rows x 4 columns], 'y_test_all_properties':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2350         NaN         NaN         NaN         NaN\n",
      "2351         NaN         NaN         NaN         NaN\n",
      "2361         NaN         NaN         NaN         NaN\n",
      "2362         NaN         NaN         NaN         NaN\n",
      "2363         NaN         NaN         NaN         NaN\n",
      "\n",
      "[591 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    393\n",
      "jun_median    393\n",
      "mar_median    393\n",
      "sep_median    393\n",
      "dtype: int64, 'y_test_two_bed': dec_median    501\n",
      "jun_median    501\n",
      "mar_median    501\n",
      "sep_median    501\n",
      "dtype: int64, 'y_test_three_bed': dec_median    369\n",
      "jun_median    369\n",
      "mar_median    369\n",
      "sep_median    369\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    474\n",
      "jun_median    474\n",
      "mar_median    474\n",
      "sep_median    474\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    585\n",
      "jun_median    585\n",
      "mar_median    585\n",
      "sep_median    585\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    510\n",
      "jun_median    510\n",
      "mar_median    510\n",
      "sep_median    510\n",
      "dtype: int64, 'y_test_all_properties': dec_median    591\n",
      "jun_median    591\n",
      "mar_median    591\n",
      "sep_median    591\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe by columns \n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of all training, validation, and test sets\n",
    "ML_dfs = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "def rfecv_with_random_forest(X_train, X_val, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Apply Recursive Feature Elimination with Cross-Validation (RFECV) using\n",
    "    Random Forest as the estimator to automatically select the optimal number\n",
    "    of features, then use GridSearchCV to fine-tune hyperparameters on the reduced feature set.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training feature set\n",
    "    X_val: Validation feature set\n",
    "    X_test: Test feature set\n",
    "    y_train: Training labels (target)\n",
    "    \n",
    "    Returns:\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv: Reduced datasets\n",
    "    best_rf: Best tuned Random Forest model after feature selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Use RFECV to automatically select the optimal number of features\n",
    "    rfecv = RFECV(estimator=rf_model, step=1, cv=KFold(5), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the datasets based on the selected features\n",
    "    X_train_rfecv = rfecv.transform(X_train_scaled)\n",
    "    X_val_rfecv = rfecv.transform(X_val_scaled)\n",
    "    X_test_rfecv = rfecv.transform(X_test_scaled)\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV on the reduced feature set\n",
    "    rf_after_rfecv = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(rf_after_rfecv, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train_rfecv, y_train)\n",
    "    \n",
    "    # Select the best Random Forest model based on GridSearchCV\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # Get the selected feature indices and names\n",
    "    selected_features = rfecv.get_support(indices=True)\n",
    "    selected_feature_names = [X_train.columns[i] for i in selected_features]  # Use X_train columns\n",
    "\n",
    "    # Get the feature importance values from the trained Random Forest model (best_rf)\n",
    "    importances = best_rf.feature_importances_\n",
    "\n",
    "    # Pair selected features with their corresponding importance values\n",
    "    feature_importance_pairs = list(zip(selected_feature_names, importances))\n",
    "\n",
    "    # Sort the features by importance values in descending order\n",
    "    sorted_features = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted features with their importance values\n",
    "    print(f\"Optimal number of features selected: {rfecv.n_features_}\")\n",
    "    print(f\"Selected feature names: {selected_feature_names}\")\n",
    "    print(f\"Selected features sorted by importance:\")\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    return X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Type: one_bed_flat\n",
      "Optimal number of features selected: 71\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_1_149_tot', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1000_1249_tot', 'hi_1250_1499_tot', 'hi_1500_1749_tot', 'hi_1750_1999_tot', 'hi_2000_2499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'erp', 'livability', 'primary_school_count', 'secondary_school_count', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'has_secondary_school', 'total_education_count', 'commercial_areas', 'forest', 'industrial_areas', 'nature_reserve', 'park', 'residential_areas', 'retail_areas', 'landuse_mode', 'cycleway', 'main_roads', 'motorway', 'residential_roads', 'track', 'walking_paths', 'roads_mode', 'beach', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'food_and_beverage', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pofw_count', 'distance_to_fire_station', 'distance_to_hospital', 'distance_to_hotel', 'distance_to_kindergarten', 'distance_to_library', 'distance_to_mall', 'distance_to_park', 'distance_to_police', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd', 'furnished_count', 'unfurnished_count', 'pets_allowed', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7165\n",
      "hi_4000_more_tot: 0.0702\n",
      "distance_to_hotel: 0.0090\n",
      "offence_division_4: 0.0088\n",
      "hi_1500_1749_tot: 0.0085\n",
      "main_roads: 0.0080\n",
      "pets_allowed: 0.0077\n",
      "offence_division_6: 0.0075\n",
      "offence_division_3: 0.0070\n",
      "industrial_areas: 0.0065\n",
      "hi_3500_3999_tot: 0.0061\n",
      "retail_areas: 0.0057\n",
      "hi_3000_3499_tot: 0.0055\n",
      "erp: 0.0053\n",
      "offence_division_2: 0.0053\n",
      "livability: 0.0053\n",
      "hi_150_299_tot: 0.0052\n",
      "hi_500_649_tot: 0.0050\n",
      "hi_1_149_tot: 0.0047\n",
      "motorway: 0.0046\n",
      "nearest_transport_avg_distance: 0.0045\n",
      "offence_division_5: 0.0043\n",
      "hi_300_399_tot: 0.0042\n",
      "avg_secondary_school_rank: 0.0039\n",
      "walking_paths: 0.0038\n",
      "offence_division_1: 0.0036\n",
      "hi_400_499_tot: 0.0035\n",
      "residential_areas: 0.0033\n",
      "tourism_and_attractions: 0.0032\n",
      "distance_to_mall: 0.0029\n",
      "hi_2500_2999_tot: 0.0029\n",
      "accommodation: 0.0028\n",
      "hi_1000_1249_tot: 0.0026\n",
      "hi_1750_1999_tot: 0.0026\n",
      "hi_800_999_tot: 0.0026\n",
      "distance_to_supermarket: 0.0025\n",
      "hi_2000_2499_tot: 0.0025\n",
      "avg_primary_school_rank: 0.0023\n",
      "hi_650_799_tot: 0.0023\n",
      "financial_institutions: 0.0022\n",
      "culture_and_leisure: 0.0021\n",
      "roads_mode: 0.0021\n",
      "forest: 0.0020\n",
      "hi_1250_1499_tot: 0.0019\n",
      "park: 0.0019\n",
      "distance_to_fire_station: 0.0018\n",
      "distance_to_hospital: 0.0016\n",
      "public_facilities: 0.0015\n",
      "healthcare: 0.0015\n",
      "shopping_and_retail: 0.0013\n",
      "pofw_count: 0.0013\n",
      "cycleway: 0.0012\n",
      "distance_to_police: 0.0011\n",
      "landuse_mode: 0.0011\n",
      "distance_to_restaurant: 0.0011\n",
      "residential_roads: 0.0011\n",
      "commercial_areas: 0.0010\n",
      "distance_to_park: 0.0009\n",
      "food_and_beverage: 0.0009\n",
      "unfurnished_count: 0.0008\n",
      "distance_to_library: 0.0008\n",
      "total_education_count: 0.0008\n",
      "nature_reserve: 0.0007\n",
      "pets_not_allowed: 0.0007\n",
      "distance_to_kindergarten: 0.0007\n",
      "beach: 0.0007\n",
      "track: 0.0006\n",
      "secondary_school_count: 0.0006\n",
      "primary_school_count: 0.0005\n",
      "furnished_count: 0.0004\n",
      "has_secondary_school: 0.0002\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 4156.6377, R^2: 0.2723, Validation MAE: 47.4839\n",
      "Predictions for 2025-2027: [[470.555  466.765  463.135  488.125 ]\n",
      " [470.94   467.295  463.845  488.0375]\n",
      " [472.085  468.14   464.39   489.6875]\n",
      " ...\n",
      " [324.725  321.025  320.615  328.0925]\n",
      " [326.315  321.455  321.09   330.9825]\n",
      " [330.09   325.735  325.25   337.4375]]\n",
      "\n",
      "\n",
      "Property Type: two_bed_flat\n",
      "Optimal number of features selected: 30\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_1750_1999_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'livability', 'avg_secondary_school_rank', 'industrial_areas', 'nature_reserve', 'residential_areas', 'main_roads', 'beach', 'accommodation', 'financial_institutions', 'food_and_beverage', 'pofw_count', 'distance_to_restaurant', 'nearest_transport_avg_distance', 'distance_to_cbd', 'unfurnished_count']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7767\n",
      "hi_4000_more_tot: 0.0447\n",
      "main_roads: 0.0117\n",
      "financial_institutions: 0.0112\n",
      "offence_division_6: 0.0091\n",
      "hi_3000_3499_tot: 0.0085\n",
      "residential_areas: 0.0082\n",
      "hi_3500_3999_tot: 0.0080\n",
      "offence_division_1: 0.0078\n",
      "beach: 0.0077\n",
      "offence_division_2: 0.0073\n",
      "industrial_areas: 0.0068\n",
      "hi_150_299_tot: 0.0068\n",
      "avg_secondary_school_rank: 0.0063\n",
      "accommodation: 0.0060\n",
      "offence_division_5: 0.0058\n",
      "hi_1750_1999_tot: 0.0057\n",
      "nearest_transport_avg_distance: 0.0057\n",
      "hi_2500_2999_tot: 0.0054\n",
      "offence_division_4: 0.0054\n",
      "distance_to_restaurant: 0.0052\n",
      "hi_300_399_tot: 0.0051\n",
      "food_and_beverage: 0.0048\n",
      "hi_650_799_tot: 0.0047\n",
      "livability: 0.0046\n",
      "offence_division_3: 0.0046\n",
      "hi_500_649_tot: 0.0046\n",
      "nature_reserve: 0.0041\n",
      "unfurnished_count: 0.0040\n",
      "pofw_count: 0.0035\n",
      "Best n_estimators: 200, Best max_depth: 15\n",
      "Validation MSE: 6226.4274, R^2: 0.2810, Validation MAE: 62.2933\n",
      "Predictions for 2025-2027: [[626.36166667 619.785      615.39833333 648.17083333]\n",
      " [627.34166667 620.685      616.36333333 649.37083333]\n",
      " [627.83166667 620.845      616.84333333 648.39583333]\n",
      " ...\n",
      " [472.42529622 465.67916634 462.36690628 514.30654664]\n",
      " [475.72535054 467.53082753 463.43998029 512.3098484 ]\n",
      " [482.15721002 471.06302408 465.74377815 508.19464952]]\n",
      "\n",
      "\n",
      "Property Type: three_bed_flat\n",
      "Optimal number of features selected: 65\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_1_149_tot', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1000_1249_tot', 'hi_1250_1499_tot', 'hi_1500_1749_tot', 'hi_1750_1999_tot', 'hi_2000_2499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'erp', 'livability', 'primary_school_count', 'secondary_school_count', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'commercial_areas', 'forest', 'industrial_areas', 'nature_reserve', 'park', 'residential_areas', 'retail_areas', 'landuse_mode', 'cycleway', 'main_roads', 'motorway', 'residential_roads', 'track', 'walking_paths', 'beach', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'food_and_beverage', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pofw_count', 'distance_to_kindergarten', 'distance_to_library', 'distance_to_mall', 'distance_to_park', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd', 'median_parkings', 'furnished_count', 'unfurnished_count', 'pets_allowed', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7193\n",
      "hi_4000_more_tot: 0.0578\n",
      "offence_division_1: 0.0419\n",
      "nearest_transport_avg_distance: 0.0239\n",
      "cycleway: 0.0169\n",
      "offence_division_5: 0.0106\n",
      "motorway: 0.0097\n",
      "main_roads: 0.0067\n",
      "offence_division_4: 0.0054\n",
      "shopping_and_retail: 0.0044\n",
      "unfurnished_count: 0.0043\n",
      "residential_areas: 0.0042\n",
      "commercial_areas: 0.0038\n",
      "hi_300_399_tot: 0.0037\n",
      "offence_division_3: 0.0037\n",
      "distance_to_kindergarten: 0.0036\n",
      "offence_division_6: 0.0034\n",
      "walking_paths: 0.0034\n",
      "pets_not_allowed: 0.0032\n",
      "industrial_areas: 0.0031\n",
      "hi_400_499_tot: 0.0030\n",
      "hi_150_299_tot: 0.0030\n",
      "avg_secondary_school_rank: 0.0027\n",
      "park: 0.0027\n",
      "residential_roads: 0.0025\n",
      "hi_3500_3999_tot: 0.0024\n",
      "retail_areas: 0.0024\n",
      "hi_1_149_tot: 0.0024\n",
      "offence_division_2: 0.0021\n",
      "food_and_beverage: 0.0020\n",
      "public_facilities: 0.0020\n",
      "hi_3000_3499_tot: 0.0018\n",
      "hi_800_999_tot: 0.0018\n",
      "hi_650_799_tot: 0.0018\n",
      "hi_2500_2999_tot: 0.0018\n",
      "erp: 0.0017\n",
      "hi_500_649_tot: 0.0016\n",
      "livability: 0.0016\n",
      "hi_1000_1249_tot: 0.0016\n",
      "tourism_and_attractions: 0.0015\n",
      "culture_and_leisure: 0.0015\n",
      "nature_reserve: 0.0014\n",
      "distance_to_park: 0.0014\n",
      "avg_primary_school_rank: 0.0013\n",
      "hi_2000_2499_tot: 0.0013\n",
      "track: 0.0013\n",
      "distance_to_restaurant: 0.0013\n",
      "accommodation: 0.0013\n",
      "distance_to_library: 0.0013\n",
      "hi_1750_1999_tot: 0.0012\n",
      "forest: 0.0012\n",
      "pofw_count: 0.0012\n",
      "distance_to_supermarket: 0.0012\n",
      "hi_1500_1749_tot: 0.0011\n",
      "beach: 0.0011\n",
      "hi_1250_1499_tot: 0.0010\n",
      "healthcare: 0.0008\n",
      "financial_institutions: 0.0007\n",
      "primary_school_count: 0.0006\n",
      "median_parkings: 0.0005\n",
      "furnished_count: 0.0005\n",
      "distance_to_mall: 0.0004\n",
      "landuse_mode: 0.0004\n",
      "secondary_school_count: 0.0003\n",
      "pets_allowed: 0.0002\n",
      "Best n_estimators: 100, Best max_depth: 15\n",
      "Validation MSE: 13992.4065, R^2: 0.5631, Validation MAE: 84.0331\n",
      "Predictions for 2025-2027: [[924.76810287 908.29573932 896.96704731 947.56324206]\n",
      " [926.34605159 910.91477778 899.7116627  953.34240873]\n",
      " [925.36926587 911.56465873 901.19975794 951.60526587]\n",
      " ...\n",
      " [875.04427381 862.94210317 845.28072222 900.4924881 ]\n",
      " [879.44844048 867.16335317 849.30322222 907.8487381 ]\n",
      " [883.21394048 870.61722222 853.52881746 913.84189286]]\n",
      "\n",
      "\n",
      "Property Type: two_bed_house\n",
      "Optimal number of features selected: 4\n",
      "Selected feature names: ['offence_division_1', 'offence_division_5', 'hi_4000_more_tot', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.8710\n",
      "hi_4000_more_tot: 0.0679\n",
      "offence_division_1: 0.0329\n",
      "offence_division_5: 0.0282\n",
      "Best n_estimators: 50, Best max_depth: 10\n",
      "Validation MSE: 5975.5923, R^2: 0.5843, Validation MAE: 59.0862\n",
      "Predictions for 2025-2027: [[632.60644179 626.88563897 622.6213284  634.61626   ]\n",
      " [630.97175151 625.10555133 621.41993676 631.91185504]\n",
      " [630.70175006 626.33554553 623.066879   633.02421655]\n",
      " ...\n",
      " [588.37224091 580.63567933 576.17173549 596.16842279]\n",
      " [588.86374383 580.87134116 576.42842367 595.65440891]\n",
      " [600.91543215 592.60778272 588.25170939 604.35217515]]\n",
      "\n",
      "\n",
      "Property Type: three_bed_house\n",
      "Optimal number of features selected: 19\n",
      "Selected feature names: ['offence_division_1', 'offence_division_5', 'hi_150_299_tot', 'hi_400_499_tot', 'hi_800_999_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_4000_more_tot', 'livability', 'industrial_areas', 'park', 'residential_areas', 'cycleway', 'main_roads', 'beach', 'accommodation', 'food_and_beverage', 'nearest_transport_avg_distance', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7817\n",
      "hi_4000_more_tot: 0.0619\n",
      "residential_areas: 0.0178\n",
      "main_roads: 0.0177\n",
      "accommodation: 0.0130\n",
      "nearest_transport_avg_distance: 0.0119\n",
      "beach: 0.0104\n",
      "food_and_beverage: 0.0102\n",
      "offence_division_1: 0.0100\n",
      "hi_2500_2999_tot: 0.0089\n",
      "park: 0.0083\n",
      "cycleway: 0.0082\n",
      "industrial_areas: 0.0068\n",
      "offence_division_5: 0.0065\n",
      "hi_400_499_tot: 0.0062\n",
      "hi_150_299_tot: 0.0055\n",
      "hi_3000_3499_tot: 0.0053\n",
      "hi_800_999_tot: 0.0050\n",
      "livability: 0.0047\n",
      "Best n_estimators: 200, Best max_depth: 10\n",
      "Validation MSE: 7799.7477, R^2: 0.7495, Validation MAE: 67.7739\n",
      "Predictions for 2025-2027: [[846.35374614 837.70515985 836.37887416 959.21209878]\n",
      " [866.15786578 857.40441497 855.61874569 975.02368636]\n",
      " [873.16986858 864.41105363 861.50737314 980.16011423]\n",
      " ...\n",
      " [703.21180323 689.0280927  686.04982832 737.44971268]\n",
      " [713.97384894 701.05923592 698.05802131 753.05946812]\n",
      " [724.06035607 712.66592396 709.13961983 776.97612503]]\n",
      "\n",
      "\n",
      "Property Type: four_bed_house\n",
      "Optimal number of features selected: 16\n",
      "Selected feature names: ['offence_division_1', 'offence_division_5', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_4000_more_tot', 'livability', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'park', 'residential_areas', 'cycleway', 'main_roads', 'accommodation', 'food_and_beverage', 'nearest_transport_avg_distance', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.6901\n",
      "hi_4000_more_tot: 0.0633\n",
      "residential_areas: 0.0458\n",
      "food_and_beverage: 0.0239\n",
      "hi_300_399_tot: 0.0231\n",
      "nearest_transport_avg_distance: 0.0193\n",
      "accommodation: 0.0186\n",
      "offence_division_1: 0.0169\n",
      "cycleway: 0.0158\n",
      "avg_secondary_school_rank: 0.0144\n",
      "hi_400_499_tot: 0.0138\n",
      "offence_division_5: 0.0125\n",
      "avg_primary_school_rank: 0.0112\n",
      "park: 0.0111\n",
      "livability: 0.0103\n",
      "main_roads: 0.0098\n",
      "Best n_estimators: 200, Best max_depth: 15\n",
      "Validation MSE: 16003.4136, R^2: 0.7709, Validation MAE: 93.4645\n",
      "Predictions for 2025-2027: [[1127.61693247 1100.92969828 1086.70069684 1136.88562356]\n",
      " [1219.71422414 1203.75907328 1197.60007184 1282.5789569 ]\n",
      " [1216.9733908  1206.27948994 1201.56278017 1286.6845819 ]\n",
      " ...\n",
      " [ 870.35222222  854.61756944  846.22076389  868.66134921]\n",
      " [ 875.40150794  861.92804563  853.43433532  871.61539683]\n",
      " [ 885.05150794  870.23304563  861.63933532  883.26914683]]\n",
      "\n",
      "\n",
      "Property Type: all_properties\n",
      "Optimal number of features selected: 16\n",
      "Selected feature names: ['offence_division_5', 'hi_1_149_tot', 'hi_300_399_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1250_1499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_4000_more_tot', 'residential_areas', 'main_roads', 'residential_roads', 'walking_paths', 'beach', 'distance_to_restaurant', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.4353\n",
      "hi_4000_more_tot: 0.2284\n",
      "hi_650_799_tot: 0.0459\n",
      "residential_areas: 0.0303\n",
      "hi_800_999_tot: 0.0255\n",
      "hi_1250_1499_tot: 0.0249\n",
      "hi_300_399_tot: 0.0236\n",
      "main_roads: 0.0224\n",
      "residential_roads: 0.0224\n",
      "walking_paths: 0.0221\n",
      "distance_to_restaurant: 0.0220\n",
      "hi_2500_2999_tot: 0.0217\n",
      "offence_division_5: 0.0201\n",
      "hi_3000_3499_tot: 0.0188\n",
      "hi_1_149_tot: 0.0186\n",
      "beach: 0.0179\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 7488.9293, R^2: 0.1767, Validation MAE: 67.0083\n",
      "Predictions for 2025-2027: [[588.27168056 576.89195238 574.1266369  622.73928571]\n",
      " [592.63100595 581.71103968 578.80036706 626.32663492]\n",
      " [609.14263294 599.63571032 597.03985714 651.73539484]\n",
      " ...\n",
      " [582.58312482 572.79697908 567.68558658 638.38604356]\n",
      " [595.37967641 586.03751479 581.32762626 663.17031142]\n",
      " [599.36643398 590.03337843 585.59141414 675.62533036]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialise the predictions dictionary and MAE list\n",
    "predictions_dict = {}\n",
    "mae_list = []\n",
    "\n",
    "# Property Types\n",
    "property_types = ['one_bed_flat', 'two_bed_flat', 'three_bed_flat', \n",
    "                  'two_bed_house', 'three_bed_house', 'four_bed_house', 'all_properties']\n",
    "\n",
    "# Loop through each set, perform RFE, and predict using the tuned Random Forest model\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    print(f\"Property Type: {property_types[i]}\")\n",
    "    # Perform feature selection and get the best model\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names = rfecv_with_random_forest(\n",
    "        X_train, X_val, X_test, y_train\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_rf.predict(X_val_rfecv)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    # Save MAE\n",
    "    mae_list.append(val_mae)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f\"Best n_estimators: {best_rf.n_estimators}, Best max_depth: {best_rf.max_depth}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Combine the training and validation sets for final model training\n",
    "    X_train_val_rfecv = np.vstack((X_train_rfecv, X_val_rfecv))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Retrain the model using the combined training and validation sets\n",
    "    best_rf.fit(X_train_val_rfecv, y_train_val)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_rf.predict(X_test_rfecv)\n",
    "    \n",
    "    # Store predictions in the dictionary with dataset index as key\n",
    "    predictions_dict[f'X_test_{i+1}_predictions'] = y_test_pred\n",
    "    \n",
    "    # Print the predictions for the test set\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved one_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved four_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved all_properties with 'year', 'suburb', and predictions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prediction Column Names\n",
    "column_names = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# labelled_dfs corresponds to the datasets with labels for each property type\n",
    "labelled_dfs = [X_test_one_bed_labels, X_test_two_bed_labels, X_test_three_bed_labels, \n",
    "                X_test_two_bed_house_labels, X_test_three_bed_house_labels, \n",
    "                X_test_four_bed_house_labels, X_test_all_properties_labels]\n",
    "\n",
    "# Iterate through the predictions dictionary and labelled DataFrames\n",
    "for i, (key, value) in enumerate(predictions_dict.items()):\n",
    "    # Select only the 'year' and 'suburb' columns from the labelled_dfs\n",
    "    labelled_df_subset = labelled_dfs[i][['suburb', 'year']]\n",
    "\n",
    "    # Initialise an empty DataFrame for the predictions\n",
    "    predictions_df = pd.DataFrame()\n",
    "\n",
    "    # value is a 2D array with multiple rows and 4 columns (n_samples, 4)\n",
    "    # Add each column using the custom names for the median values\n",
    "    for j in range(value.shape[1]):\n",
    "        predictions_df[column_names[j]] = value[:, j]\n",
    "        #print(labelled_df_subset.shape)\n",
    "\n",
    "    # Reset the index for both DataFrames to ensure proper alignment\n",
    "    labelled_df_subset = labelled_df_subset.reset_index(drop=True)\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the 'year' and 'suburb' columns with the predictions_df\n",
    "    labelled_with_predictions = pd.concat([labelled_df_subset, predictions_df], axis=1)\n",
    "\n",
    "    # Save the new DataFrame with only 'year', 'suburb', and predictions to a CSV file\n",
    "    labelled_with_predictions.to_csv(f\"../data/curated/predictions/{property_types[i]}_predictions.csv\", index=False)\n",
    "\n",
    "    # Print confirmation (optional)\n",
    "    print(f\"Saved {property_types[i]} with 'year', 'suburb', and predictions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE list has been saved to '../data/curated/predictions/mae.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert the MAE list to a pandas DataFrame\n",
    "mae_df = pd.DataFrame(mae_list, columns=['MAE'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mae_df.to_csv(\"../data/curated/predictions/mae.csv\", index=False)\n",
    "\n",
    "# Print confirmation\n",
    "print(\"MAE list has been saved to '../data/curated/predictions/mae.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
