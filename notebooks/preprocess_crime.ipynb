{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this notebook you should have already run the crime.py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/09/24 12:18:49 WARN Utils: Your hostname, Mok resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/09/24 12:18:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/24 12:18:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.executor.memory\", \"5g\")\n",
    "    .config(\"spark.driver.memory\", \"5g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_parquet(name, sdf, folder):\n",
    "    '''Input: take a spark dataframe, and a desired file name\n",
    "\n",
    "    A staging folder is created as creating a parquet creates meta data files which are unneeded\n",
    "    for analysis. Relevent parquet files are then moved into the curated folder\n",
    "    \n",
    "    Output: save the new spark data frame into the curated folder ready for analysis'''\n",
    "    \n",
    "    #now we want to move this file into the curated data\n",
    "    import shutil\n",
    "    import os\n",
    "    \n",
    "    folder_path = f'../data/staging'\n",
    "\n",
    "    # Create the folder, including any intermediate directories\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created successfully!\")\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\n",
    "\n",
    "    sdf.coalesce(1)\\\n",
    "        .write\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .parquet(f'../data/staging/{name}')\n",
    "\n",
    "\n",
    "\n",
    "    starting_directory = '../data/staging/'\n",
    "    output_directory = f'../data/{folder}/'\n",
    "\n",
    "    input_directory = os.path.join(starting_directory, name)\n",
    "\n",
    "    for file in os.listdir(input_directory):\n",
    "        if file.endswith(\".parquet\"):\n",
    "            \n",
    "            #rename the file first since currently they have a part-00000 name\n",
    "            part_file_path = os.path.join(input_directory, file)\n",
    "            new_file_path = os.path.join(input_directory, name)\n",
    "            os.rename(part_file_path, new_file_path)\n",
    "                \n",
    "            #move the file into the curated folder for models\n",
    "            source_file_path = os.path.join(input_directory, name)\n",
    "            destination_file_path = os.path.join(output_directory, name)\n",
    "        \n",
    "            shutil.move(source_file_path, destination_file_path)\n",
    "            shutil.rmtree(folder_path)\n",
    "\n",
    "    print(f\"Folder '{folder_path}' has been removed and file strucutre is cleaned!\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_raw_sdf = spark.read.parquet('../data/landing/crime.parquet')\n",
    "\n",
    "# we want to remove the subdivisions and LGAS as these are not relevent features \n",
    "offence_division_raw = crime_raw_sdf.drop(\"Offence Subdivision\", \"Offence Subgroup\", \"Year ending\", \"Local Government Area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offence_division_raw.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../data/staging' created successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#save this file as a parquet in the raw folder\n",
    "save_df_to_parquet(\"offence_division_raw.parquet\", offence_division_raw, \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------------------+-------------------+\n",
      "|Year|Suburb/Town Name|    Offence Division|total_offence_count|\n",
      "+----+----------------+--------------------+-------------------+\n",
      "|2023|     Bakery Hill|E Justice procedu...|                  6|\n",
      "|2023|    Lake Gardens|A Crimes against ...|                 12|\n",
      "|2023| Trafalgar South|B Property and de...|                  3|\n",
      "|2023|      Cheltenham|     C Drug offences|                 81|\n",
      "|2023|  Echuca Village|A Crimes against ...|                  5|\n",
      "|2023|           Patho|D Public order an...|                  4|\n",
      "|2023|     Maryborough|A Crimes against ...|                221|\n",
      "|2023|         Warrion|B Property and de...|                  2|\n",
      "|2023|       Ellaswood|E Justice procedu...|                  3|\n",
      "|2023|     Leitchville|D Public order an...|                  8|\n",
      "|2023|      Lake Mundi|D Public order an...|                 11|\n",
      "|2023|      Lethbridge|E Justice procedu...|                  8|\n",
      "|2023|    Golden Gully|E Justice procedu...|                  1|\n",
      "|2023|            Musk|B Property and de...|                  2|\n",
      "|2023|         Rainbow|D Public order an...|                  1|\n",
      "|2023|   Campbellfield|E Justice procedu...|                 67|\n",
      "|2023|       Moorabbin|    F Other offences|                  2|\n",
      "|2023|      Hernes Oak|     C Drug offences|                  1|\n",
      "|2023|      Wedderburn|A Crimes against ...|                 22|\n",
      "|2023|        Merrijig|B Property and de...|                 10|\n",
      "+----+----------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crime_raw_sdf = spark.read.parquet('../data/raw/offence_division_raw.parquet')\n",
    "\n",
    "crime_raw_sdf.createOrReplaceTempView(\"offence_data\")\n",
    "\n",
    "# SQL query to group by 'Year', 'Suburb/Town Name', and 'Offence Division' and sum 'Offence Count'\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        `Suburb/Town Name`,\n",
    "        `Offence Division`,\n",
    "        SUM(`Offence Count`) AS total_offence_count\n",
    "    FROM offence_data\n",
    "    GROUP BY Year, `Suburb/Town Name`, `Offence Division`\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "sdf = result_df.withColumn(\n",
    "    'Offence Division',\n",
    "    when(result_df['Offence Division'] == 'A Crimes against the person', 1)\n",
    "    .when(result_df['Offence Division'] == 'B Property and deception offences', 2)\n",
    "    .when(result_df['Offence Division'] == 'C Drug offences', 3)\n",
    "    .when(result_df['Offence Division'] == 'D Public order and security offences', 4)\n",
    "    .when(result_df['Offence Division'] == 'E Justice procedures offences', 5)\n",
    "    .when(result_df['Offence Division'] == 'F Other offences', 6)\n",
    "    .otherwise(None)  # Use None or a default value if there is an unrecognized category\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.show of +----+----------------+----------------+-------------------+\n",
       "|Year|Suburb/Town Name|Offence Division|total_offence_count|\n",
       "+----+----------------+----------------+-------------------+\n",
       "|2014|        Ryanston|               1|                  0|\n",
       "|2014|        Ryanston|               2|                  2|\n",
       "|2014|        Ryanston|               3|                  0|\n",
       "|2014|        Ryanston|               4|                  4|\n",
       "|2014|        Ryanston|               5|                  0|\n",
       "|2014|        Ryanston|               6|                  0|\n",
       "|2014|    Neerim South|               1|                  8|\n",
       "|2014|    Neerim South|               2|                 27|\n",
       "|2014|    Neerim South|               3|                  7|\n",
       "|2014|    Neerim South|               4|                  6|\n",
       "|2014|    Neerim South|               5|                  2|\n",
       "|2014|    Neerim South|               6|                  5|\n",
       "|2014|          Thoona|               1|                  3|\n",
       "|2014|          Thoona|               2|                  0|\n",
       "|2014|          Thoona|               3|                  1|\n",
       "|2014|          Thoona|               4|                  2|\n",
       "|2014|          Thoona|               5|                  0|\n",
       "|2014|          Thoona|               6|                  0|\n",
       "|2014|        Sea Lake|               1|                  6|\n",
       "|2014|        Sea Lake|               2|                 15|\n",
       "+----+----------------+----------------+-------------------+\n",
       "only showing top 20 rows\n",
       ">"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "years = [row['Year'] for row in sdf.select('Year').distinct().collect()]\n",
    "suburbs = [row['Suburb/Town Name'] for row in sdf.select('Suburb/Town Name').distinct().collect()]\n",
    "\n",
    "#Define all possible offence divisions\n",
    "offence_divisions = [1, 2, 3, 4, 5, 6]  # Based on your specified divisions\n",
    "\n",
    "#Create all combinations of Year, Suburb, and Offence Division\n",
    "combinations = list(product(years, suburbs, offence_divisions))\n",
    "\n",
    "# Create a DataFrame from these combinations\n",
    "comb_df = spark.createDataFrame(combinations, schema=['Year', 'Suburb/Town Name', 'Offence Division'])\n",
    "\n",
    "#Left join the original DataFrame with the combinations DataFrame\n",
    "full_df = comb_df.join(sdf, on=['Year', 'Suburb/Town Name', 'Offence Division'], how='left')\n",
    "\n",
    "#Fill missing offence counts with 0\n",
    "full_df = full_df.fillna({'total_offence_count': 0})\n",
    "\n",
    "full_df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------------+-------------------+\n",
      "|Year|   Suburb/Town Name|Offence Division|total_offence_count|\n",
      "+----+-------------------+----------------+-------------------+\n",
      "|2014|       Tonghi Creek|               5|                  0|\n",
      "|2014|         Callawadda|               1|                  2|\n",
      "|2014|         Chum Creek|               6|                  0|\n",
      "|2014|            Croydon|               3|                106|\n",
      "|2014|     Dockers Plains|               5|                  0|\n",
      "|2014|       Beech Forest|               4|                  0|\n",
      "|2014|           Toolondo|               4|                  0|\n",
      "|2014|          Newington|               6|                  0|\n",
      "|2014|       Sunset Strip|               2|                  6|\n",
      "|2014|          Benjeroop|               4|                  0|\n",
      "|2014|        Albert Park|               1|                 45|\n",
      "|2014|          East Sale|               6|                  0|\n",
      "|2014|Woodstock On Loddon|               5|                  0|\n",
      "|2014|       Doctors Flat|               5|                  0|\n",
      "|2014|      Carlton North|               4|                 28|\n",
      "|2014|    Blackburn South|               3|                 16|\n",
      "|2014|       Iguana Creek|               4|                  0|\n",
      "|2014|              Dooen|               3|                  1|\n",
      "|2014|              Dooen|               4|                  1|\n",
      "|2014|             Byaduk|               5|                  0|\n",
      "+----+-------------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "full_df.createOrReplaceTempView(\"offence_data\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        Year,\n",
    "        `Suburb/Town Name`,\n",
    "        `Offence Division`,\n",
    "        `total_offence_count`\n",
    "    FROM offence_data\n",
    "    GROUP BY Year, `Suburb/Town Name`, `Offence Division`, `total_offence_count`\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "check_df = spark.sql(query)\n",
    "\n",
    "# Show the result\n",
    "check_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:>                                                        (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+-------------------+-----------------------+-----------------+\n",
      "|Year|Suburb/Town Name|Offence Division|total_offence_count|prev_year_offence_count|difference_change|\n",
      "+----+----------------+----------------+-------------------+-----------------------+-----------------+\n",
      "|2014|        Abbeyard|               1|                  0|                   NULL|             NULL|\n",
      "|2015|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2016|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2017|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2018|        Abbeyard|               1|                  2|                      0|                2|\n",
      "|2019|        Abbeyard|               1|                  0|                      2|               -2|\n",
      "|2020|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2021|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2022|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2023|        Abbeyard|               1|                  0|                      0|                0|\n",
      "|2014|        Abbeyard|               2|                  0|                   NULL|             NULL|\n",
      "|2015|        Abbeyard|               2|                  0|                      0|                0|\n",
      "|2016|        Abbeyard|               2|                  1|                      0|                1|\n",
      "|2017|        Abbeyard|               2|                  0|                      1|               -1|\n",
      "|2018|        Abbeyard|               2|                  0|                      0|                0|\n",
      "|2019|        Abbeyard|               2|                  1|                      0|                1|\n",
      "|2020|        Abbeyard|               2|                  0|                      1|               -1|\n",
      "|2021|        Abbeyard|               2|                  1|                      0|                1|\n",
      "|2022|        Abbeyard|               2|                  0|                      1|               -1|\n",
      "|2023|        Abbeyard|               2|                  0|                      0|                0|\n",
      "+----+----------------+----------------+-------------------+-----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import lag, col\n",
    "\n",
    "#Create a window specification that partitions by 'Suburb/Town Name' and 'Offence Division' and orders by 'Year'\n",
    "window_spec = Window.partitionBy('Suburb/Town Name', 'Offence Division').orderBy('Year')\n",
    "\n",
    "#Use the lag function to get the previous year's total offence count\n",
    "df_with_prev_year = check_df.withColumn(\n",
    "    'prev_year_offence_count',\n",
    "    lag('total_offence_count').over(window_spec)\n",
    ")\n",
    "\n",
    "#Calculate the percentage change\n",
    "df_with_percentage_change = df_with_prev_year.withColumn(\n",
    "    'difference_change',\n",
    "    ((col('total_offence_count') - col('prev_year_offence_count')))\n",
    ")\n",
    "\n",
    "#Show the result\n",
    "df_with_percentage_change.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('year', IntegerType(), True), StructField('suburb', StringType(), True), StructField('offence_division', IntegerType(), True), StructField('total_offence_count', IntegerType(), False), StructField('prev_year_offence_count', IntegerType(), True), StructField('difference_change', IntegerType(), True)])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat, lit, to_date\n",
    "\n",
    "# Assuming 'Year' is an integer column, convert it to a DateType with the first day of the year\n",
    "df_schema_change = df_with_percentage_change\n",
    "\n",
    "df_schema_change = df_schema_change.withColumnRenamed(\"Year\", \"year\") \\\n",
    "       .withColumnRenamed(\"Suburb/Town Name\", \"suburb\")\\\n",
    "       .withColumnRenamed(\"Offence Division\", \"offence_division\")\n",
    "\n",
    "df_schema_change = df_schema_change.withColumn(\"offence_division\", col(\"offence_division\").cast(\"integer\")) \\\n",
    "       .withColumn(\"total_offence_count\", col(\"total_offence_count\").cast(\"integer\"))\\\n",
    "       .withColumn(\"prev_year_offence_count\", col(\"prev_year_offence_count\").cast(\"integer\"))\\\n",
    "       .withColumn(\"difference_change\", col(\"difference_change\").cast(\"integer\"))\\\n",
    "       .withColumn(\"year\", col(\"year\").cast(\"integer\"))\n",
    "\n",
    "df_schema_change.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2849\n"
     ]
    }
   ],
   "source": [
    "df_with_percentage_changed = df_schema_change.select(\"suburb\").distinct()\n",
    "print(df_with_percentage_changed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 118:=======>                                                 (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+----------------+-------------------+-----------------------+-----------------+\n",
      "|year|  suburb|offence_division|total_offence_count|prev_year_offence_count|difference_change|\n",
      "+----+--------+----------------+-------------------+-----------------------+-----------------+\n",
      "|2014|abbeyard|               1|                  0|                   NULL|             NULL|\n",
      "|2015|abbeyard|               1|                  0|                      0|                0|\n",
      "|2016|abbeyard|               1|                  0|                      0|                0|\n",
      "|2017|abbeyard|               1|                  0|                      0|                0|\n",
      "|2018|abbeyard|               1|                  2|                      0|                2|\n",
      "|2019|abbeyard|               1|                  0|                      2|               -2|\n",
      "|2020|abbeyard|               1|                  0|                      0|                0|\n",
      "|2021|abbeyard|               1|                  0|                      0|                0|\n",
      "|2022|abbeyard|               1|                  0|                      0|                0|\n",
      "|2023|abbeyard|               1|                  0|                      0|                0|\n",
      "|2014|abbeyard|               2|                  0|                   NULL|             NULL|\n",
      "|2015|abbeyard|               2|                  0|                      0|                0|\n",
      "|2016|abbeyard|               2|                  1|                      0|                1|\n",
      "|2017|abbeyard|               2|                  0|                      1|               -1|\n",
      "|2018|abbeyard|               2|                  0|                      0|                0|\n",
      "|2019|abbeyard|               2|                  1|                      0|                1|\n",
      "|2020|abbeyard|               2|                  0|                      1|               -1|\n",
      "|2021|abbeyard|               2|                  1|                      0|                1|\n",
      "|2022|abbeyard|               2|                  0|                      1|               -1|\n",
      "|2023|abbeyard|               2|                  0|                      0|                0|\n",
      "+----+--------+----------------+-------------------+-----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "df_schema_change = df_schema_change.withColumn(\"suburb\", lower(df_schema_change[\"suburb\"]))\n",
    "\n",
    "df_schema_change.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder '../data/staging' created successfully!\n",
      "Folder '../data/staging' has been removed and file strucutre is cleaned!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "save_df_to_parquet(\"crimes.parquet\", df_schema_change, \"curated\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed the parquet version of the crimes file!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "crime_df = pd.read_parquet('../data/curated/crimes.parquet')\n",
    "crime_df.to_csv('../data/curated/crimes.csv')\n",
    "\n",
    "os.remove('../data/curated/crimes.parquet')\n",
    "print(\"removed the parquet version of the crimes file!\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
