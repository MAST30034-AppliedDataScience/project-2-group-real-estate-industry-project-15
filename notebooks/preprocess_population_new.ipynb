{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = pd.read_csv('../data/landing/new_population.csv')\n",
    "\n",
    "# Combine first and second row to make the new column names\n",
    "population.columns = [f\"{col}_{str(population.iloc[0, idx])}\" for idx, col in enumerate(population.columns)]\n",
    "\n",
    "# Drop the first row since it's now part of the header\n",
    "population = population.drop(0).reset_index(drop=True)\n",
    "\n",
    "# Rename columns\n",
    "new_cols = [\n",
    "    \"gccsa_code\", \"gcsa_name\", \"sa4_code\", \"sa4_name\", \"sa3_code\", \"sa3_name\", \n",
    "    \"sa2_code\", \"sa2_name\", \"erp_2001\", \"erp_2002\", \"erp_2003\",\"erp_2004\",\n",
    "    \"erp_2005\",\"erp_2006\",\"erp_2007\",\"erp_2008\",\"erp_2009\",\"erp_2010\",\n",
    "    \"erp_2011\",\"erp_2012\",\"erp_2013\",\"erp_2014\",\"erp_2015\",\"erp_2016\",\n",
    "    \"erp_2017\",\"erp_2018\",\"erp_2019\", \"erp_2020\", \"erp_2021\", \"erp_2022\", \n",
    "    \"erp_2023\"\n",
    "]\n",
    "\n",
    "population.columns = new_cols\n",
    "\n",
    "# Remove nan value for gccsa_code\n",
    "population = population[~population['gccsa_code'].isna()]\n",
    "\n",
    "# Only filter data in Victoria\n",
    "population = population[population['gccsa_code'].str.contains('vic|mel', case=False)]\n",
    "\n",
    "# Drop columns we don't need\n",
    "drop_cols = [\"gccsa_code\", \"gcsa_name\", \"sa4_code\", \"sa4_name\", \"sa3_code\", \"sa3_name\"]\n",
    "population = population.drop(columns=drop_cols)\n",
    "\n",
    "# Lower suburb name\n",
    "population['sa2_name'] = population['sa2_name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directional modifiers and the word 'surrounds' to be removed\n",
    "directional_modifiers = [' - east', ' - west', ' - north', ' - south', ' - central', ' surrounds', ' (north)', ' (south)', ' (east)', ' (west)']\n",
    "pattern = '|'.join([re.escape(suffix) for suffix in directional_modifiers])\n",
    "population['sa2_name'] = population['sa2_name'].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Split sa2_name where multiple names are separated by hyphens\n",
    "population['sa2_name'] = population['sa2_name'].str.split(' - ')\n",
    "\n",
    "# Explode the lists into separate rows\n",
    "population_exploded = population.explode('sa2_name')\n",
    "population_exploded = population_exploded.reset_index(drop=True)\n",
    "\n",
    "# Mapping for the SA2 names to the correct suburbs\n",
    "sa2_name_mapping = {\n",
    "    'ballarat' : 'ballarat central',\n",
    "    'flemington racecourse' : 'flemington',\n",
    "    'southbank wharf' : 'south wharf',\n",
    "    'port melbourne industrial' : 'port melbourne',\n",
    "    'reservoir east' : 'reservoir',\n",
    "    'reservoir west' : 'reservoir',\n",
    "    'research warrandyte' : 'warrandyte',\n",
    "    'essendon airport' : 'essendon',\n",
    "    'gladstone parkmeadows' : 'gladstone park',\n",
    "    'craigieburn west' : 'craigieburn',\n",
    "    'wandin' : 'wandin north',\n",
    "    'pakenham east' : 'pakenham',\n",
    "    'pakenham west' : 'pakenham',\n",
    "    'narre warren west' : 'narre warren',\n",
    "    'berwick east' : 'berwick',\n",
    "    'berwick west' : 'berwick',\n",
    "    'point cook east' : 'point cook',\n",
    "    'point cook west' : 'point cook',\n",
    "    'truganina east' : 'truganina',\n",
    "    'truganina west' : 'truganina',\n",
    "    'melbourne cbd' : 'melbourne'\n",
    "}\n",
    "\n",
    "# Remove the \"(vic.)\" from sa2_name values\n",
    "population_exploded['sa2_name'] = population_exploded['sa2_name'].str.replace(r'\\s*\\(vic\\.\\)', '', regex=True)\n",
    "population_exploded['sa2_name'] = population_exploded['sa2_name'].replace(sa2_name_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp_cols = population.columns[population.columns.str.contains('erp')]\n",
    "population_exploded[erp_cols] = population_exploded[erp_cols].astype('int')\n",
    "\n",
    "# Create the aggregation dictionary\n",
    "aggregation_functions = {col: 'sum' for col in erp_cols}\n",
    "\n",
    "# Apply the groupby and aggregation\n",
    "population_grouped = population_exploded.groupby('sa2_name').agg(aggregation_functions).reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = [\"sa2_name\", \"2001\", \"2002\", \"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\n",
    "            \"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\n",
    "            \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "population_grouped.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = population_grouped.iloc[:, 1:24]  # Years 2001 to 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "predictions_arima = []\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    model = ARIMA(row.values, order=(1, 1, 1))\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=4)\n",
    "    predictions_arima.append(forecast)\n",
    "\n",
    "predictions_arima = np.array(predictions_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions_arima, columns=[2024, 2025, 2026, 2027])\n",
    "final_population = pd.concat([population_grouped, predictions_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all entries to integers\n",
    "numbered_columns = [col for col in final_population.columns if isinstance(col, int)]\n",
    "final_population[numbered_columns] = final_population[numbered_columns].astype(int)\n",
    "\n",
    "# Drop 2001 to 2015 columns\n",
    "columns_to_drop = [str(year) for year in range(2001, 2016)]\n",
    "final_population = final_population.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df = final_population.melt(id_vars=['sa2_name'], \n",
    "                             var_name='year', \n",
    "                             value_name='erp')\n",
    "\n",
    "# Convert 'year' column to int for consistency\n",
    "melted_df['year'] = melted_df['year'].astype(int)\n",
    "\n",
    "# Group by suburb and year\n",
    "melted_df.sort_values(by=['sa2_name', 'year'])\n",
    "\n",
    "# Save as csv\n",
    "melted_df.to_csv('../data/curated/final_population.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
