{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "demographics = pd.read_csv('../data/curated/demographics.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_merged = pd.merge(one_bed_flat, domain_one_bed_flat, on='suburb', how='inner')\n",
    "one_bed_merged = pd.merge(one_bed_merged, crimes, on='suburb', how='inner')\n",
    "one_bed_merged = pd.merge(one_bed_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "one_bed_merged = pd.merge(one_bed_merged, education, on='suburb', how='inner')\n",
    "one_bed_merged = pd.merge(one_bed_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_merged = pd.merge(one_bed_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_merged = pd.merge(two_bed_flat, domain_two_bed_flat, on='suburb', how='inner')\n",
    "two_bed_merged = pd.merge(two_bed_merged, crimes, on='suburb', how='inner')\n",
    "two_bed_merged = pd.merge(two_bed_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "two_bed_merged = pd.merge(two_bed_merged, education, on='suburb', how='inner')\n",
    "two_bed_merged = pd.merge(two_bed_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_merged = pd.merge(two_bed_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_merged = pd.merge(three_bed_flat, domain_three_bed_flat, on='suburb', how='inner')\n",
    "three_bed_merged = pd.merge(three_bed_merged, crimes, on='suburb', how='inner')\n",
    "three_bed_merged = pd.merge(three_bed_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "three_bed_merged = pd.merge(three_bed_merged, education, on='suburb', how='inner')\n",
    "three_bed_merged = pd.merge(three_bed_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_merged = pd.merge(three_bed_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, domain_two_bed_house, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, crimes, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, domain_three_bed_house, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, crimes, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, domain_four_bed_house, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, crimes, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, domain_all_properties, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, crimes, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, demographics, left_on='suburb', right_on='sa2_name', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_merged_df(df):\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_merged = clean_merged_df(one_bed_merged)\n",
    "two_bed_merged = clean_merged_df(two_bed_merged)\n",
    "three_bed_merged = clean_merged_df(three_bed_merged)\n",
    "two_bed_house_merged = clean_merged_df(two_bed_house_merged,)\n",
    "three_bed_house_merged = clean_merged_df(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_merged_df(four_bed_house_merged)\n",
    "all_properties_merged = clean_merged_df(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['suburb', 'mar_2016_median', 'jun_2016_median', 'sep_2016_median', 'dec_2016_median', 'mar_2017_median', 'jun_2017_median', 'sep_2017_median', 'dec_2017_median', 'mar_2018_median', 'jun_2018_median', 'sep_2018_median', 'dec_2018_median', 'mar_2019_median', 'jun_2019_median', 'sep_2019_median', 'dec_2019_median', 'mar_2020_median', 'jun_2020_median', 'sep_2020_median', 'dec_2020_median', 'mar_2021_median', 'jun_2021_median', 'sep_2021_median', 'dec_2021_median', 'mar_2022_median', 'jun_2022_median', 'sep_2022_median', 'dec_2022_median', 'mar_2023_median', 'jun_2023_median', 'sep_2023_median', 'dec_2023_median', 'mar_2024_median', 'median_rent', 'median_bath', 'median_parkings', 'furnished_count', 'unfurnished_count', 'pets_allowed', 'pets_not_allowed', 'num_properties', 'year', 'offence_division', 'total_offence_count', 'prev_year_offence_count', 'difference_change', 'erp_june_2022_count', 'erp_june_2023_count', 'erp_change_count', 'erp_change_percentage', 'natural_increase_count', 'net_internal_migration_count', 'net_overseas_migration_count', 'area_km2', 'pop_density_persons_km2', 'gov_age_pension_count_2023', 'gov_rent_assist_count_2023', 'personal_income_count_2020', 'personal_income_median_age_2020', 'personal_total_income_millions_2020', 'median_personal_total_income_2020', 'mean_personal_total_income_2020', 'gini_coef_2020', 'primary_school_count', 'secondary_school_count', 'tertiary_institutions_count', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'has_primary_school', 'has_secondary_school', 'total_education_count', 'commercial_areas', 'farmland', 'forest', 'industrial_areas', 'nature_reserve', 'park', 'residential_areas', 'retail_areas', 'landuse_mode', 'cycleway', 'main_roads', 'motorway', 'residential_roads', 'track', 'walking_paths', 'roads_mode', 'beach', 'cliff', 'peak', 'spring', 'nature_mode', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'food_and_beverage', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pois_mode', 'pofw_count', 'pofw_mode', 'distance_to_fire_station', 'distance_to_hospital', 'distance_to_hotel', 'distance_to_kindergarten', 'distance_to_library', 'distance_to_mall', 'distance_to_park', 'distance_to_police', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8928, 116)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns and shape\n",
    "print(list(all_properties_merged.columns))\n",
    "all_properties_merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_sets(df):\n",
    "    \"\"\"\n",
    "    Input the dataframe. The train sets include the data from the year of 2016-2021 and \n",
    "    the test sets include years from 2022-2024. The label sets (y) should only include the \n",
    "    median house prices for each month and year, whereas the predictor variables (X) should \n",
    "    include any engineered features correspondeiing to the years of the train and test sets \n",
    "    whilst also including the general features we created relevant to each suburb. Return\n",
    "    the following dataframes: X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Train set columns = 2016-2021, Test set columns = 2022-2024\n",
    "    train_years = [str(year) for year in range(2016, 2022)]\n",
    "    test_years = [str(year) for year in range(2022, 2025)]\n",
    "\n",
    "    # Filter for the respective columns for the training and test set \n",
    "    train_columns = [col for col in df.columns if any(year in col for year in train_years)]\n",
    "    test_columns = [col for col in df.columns if any(year in col for year in test_years)]\n",
    "\n",
    "     # Identify columns that do not contain any year and add them to the train and test sets \n",
    "    non_year_columns = [col for col in df.columns if not any(str(year) in col for year in range(2016, 2025))]\n",
    "\n",
    "     # Exclude the 'suburb' column if it exists\n",
    "    if 'suburb' in non_year_columns:\n",
    "        non_year_columns.remove('suburb')\n",
    "        \n",
    "    train_columns.extend(non_year_columns)\n",
    "    test_columns.extend(non_year_columns)\n",
    "\n",
    "    # Extract features and target columns\n",
    "    X_train = df[[col for col in train_columns if '_median' not in col]]\n",
    "    y_train = df[[col for col in train_columns if '_median' in col]]\n",
    "\n",
    "    X_test = df[[col for col in test_columns if '_median' not in col]]\n",
    "    y_test = df[[col for col in test_columns if '_median' in col]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Create training and test sets \n",
    "X_train_one_bed, X_test_one_bed, y_train_one_bed, y_test_one_bed = train_test_sets(one_bed_merged)\n",
    "X_train_two_bed, X_test_two_bed, y_train_two_bed, y_test_two_bed = train_test_sets(two_bed_merged)\n",
    "X_train_three_bed, X_test_three_bed, y_train_three_bed, y_test_three_bed = train_test_sets(three_bed_merged)\n",
    "X_train_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_test_two_bed_house = train_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_test_three_bed_house = train_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_test_four_bed_house = train_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties, X_test_all_properties, y_train_all_properties, y_test_all_properties = train_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values and their counts:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "missing_values = X_test_one_bed.isnull().sum()\n",
    "\n",
    "# Filter columns that have missing values\n",
    "columns_with_missing = missing_values[missing_values > 0]\n",
    "\n",
    "# Display the columns with missing values and their counts\n",
    "print(\"Columns with missing values and their counts:\")\n",
    "print(columns_with_missing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Fit Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.410e+05, tolerance: 2.749e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.436e+05, tolerance: 2.810e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.516e+05, tolerance: 2.858e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.966e+05, tolerance: 2.888e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.236e+05, tolerance: 2.965e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.520e+05, tolerance: 3.049e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.627e+05, tolerance: 3.208e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.557e+05, tolerance: 3.190e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e+06, tolerance: 3.315e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+06, tolerance: 3.264e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.079e+06, tolerance: 3.415e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.486e+05, tolerance: 3.409e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.656e+05, tolerance: 3.515e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.261e+05, tolerance: 3.336e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.678e+05, tolerance: 3.271e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.426e+05, tolerance: 3.217e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.947e+05, tolerance: 2.906e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.070e+05, tolerance: 2.530e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.762e+05, tolerance: 2.086e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.062e+04, tolerance: 1.507e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/Users/promaali/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.970e+04, tolerance: 1.390e+03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- erp_june_2022_count\n- erp_june_2023_count\n- gov_age_pension_count_2023\n- gov_rent_assist_count_2023\nFeature names seen at fit time, yet now missing:\n- gini_coef_2020\n- mean_personal_total_income_2020\n- median_personal_total_income_2020\n- personal_income_count_2020\n- personal_total_income_millions_2020\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m lasso\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlasso\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate the mean squared error and R² score\u001b[39;00m\n\u001b[1;32m     30\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_base.py:306\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Predict using the linear model.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m        Returns predicted values.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:1147\u001b[0m, in \u001b[0;36mElasticNet._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/linear_model/_base.py:285\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    283\u001b[0m     check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 285\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m     coef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m coef_\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/base.py:608\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    545\u001b[0m ):\n\u001b[1;32m    546\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    612\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/base.py:535\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- erp_june_2022_count\n- erp_june_2023_count\n- gov_age_pension_count_2023\n- gov_rent_assist_count_2023\nFeature names seen at fit time, yet now missing:\n- gini_coef_2020\n- mean_personal_total_income_2020\n- median_personal_total_income_2020\n- personal_income_count_2020\n- personal_total_income_millions_2020\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define a list of all training and test sets\n",
    "datasets = [\n",
    "    (X_train_one_bed, X_test_one_bed, y_train_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_test_two_bed, y_train_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_test_three_bed, y_train_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_test_all_properties, y_train_all_properties, y_test_all_properties),\n",
    "]\n",
    "\n",
    "# Define hyperparameters for Lasso\n",
    "alpha = 1.0  # Adjust this parameter based on your needs\n",
    "\n",
    "# Loop through each set and train the Lasso model\n",
    "for i, (X_train, X_test, y_train, y_test) in enumerate(datasets):\n",
    "    # Initialize the Lasso model\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    \n",
    "    # Train the model\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    \n",
    "    # Calculate the mean squared error and R² score\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"Coefficients: {lasso.coef_}\")\n",
    "    print(f\"Intercept: {lasso.intercept_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pca_feature_selection(X_train, X_test, variance_threshold=0.95):\n",
    "#     \"\"\"\n",
    "#     The function applies PCA for dimensionality reduction by fitting on the\n",
    "#     training set and transforming both the training and test sets. It keeps \n",
    "#     either a specified number of components or selects them based on a variance \n",
    "#     threshold. It returns the reduced training and test sets.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Standardise the data\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train_scaled = scaler.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#     # Initialise PCA, specifying the variance threshold\n",
    "#     pca_temp = PCA().fit(X_train_scaled)\n",
    "#     cumulative_variance = pca_temp.explained_variance_ratio_.cumsum()\n",
    "#     # Find the number of components to capture the specified variance\n",
    "#     n_components = next(i for i, total_variance in enumerate(cumulative_variance) if total_variance >= variance_threshold) + 1\n",
    "\n",
    "#     pca = PCA(n_components=n_components)\n",
    "\n",
    "#     # Fit PCA on the training set and transform both training and test sets\n",
    "#     X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "#     X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "#     print(f\"Number of components selected: {n_components}\")\n",
    "#     print(f\"Explained variance by selected components: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "#     return X_train_pca, X_test_pca\n",
    "\n",
    "# # Perform feature selection with PCA on the X sets \n",
    "# X_train_one_bed, X_test_one_bed = pca_feature_selection(X_train_one_bed, X_test_one_bed )\n",
    "# X_train_two_bed, X_test_two_bed = pca_feature_selection(X_train_two_bed, X_test_two_bed)\n",
    "# X_train_three_bed, X_test_three_bed = pca_feature_selection(X_train_three_bed, X_test_three_bed)\n",
    "# X_train_two_bed_house, X_test_two_bed_house = pca_feature_selection(X_train_two_bed_house, X_test_two_bed_house)\n",
    "# X_train_three_bed_house, X_test_three_bed_house = pca_feature_selection(X_train_three_bed_house, X_test_three_bed_house)\n",
    "# X_train_four_bed_house, X_test_four_bed_house = pca_feature_selection(X_train_four_bed_house, X_test_four_bed_house)\n",
    "# X_train_all_properties, X_test_all_properties = pca_feature_selection(X_train_all_properties, X_test_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Combined Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_feature_set(df, file_path):\n",
    "#      # Retrieve the directory \n",
    "#     directory = os.path.dirname(file_path)\n",
    "    \n",
    "#     # Create the directory if it does not exist\n",
    "#     if not os.path.exists(directory):\n",
    "#         os.makedirs(directory)\n",
    "\n",
    "#     # Save the filtered dataframe to CSV\n",
    "#     df.to_csv(file_path, index=False)\n",
    "\n",
    "# print(\"All Saved!\")\n",
    "\n",
    "# # Save all dataframes in the raw folder\n",
    "# one_bed_merged = save_feature_set(one_bed_merged, '../data/curated/combined_feature_set/one_bed_merged.csv')\n",
    "# two_bed_merged = save_feature_set(two_bed_merged, '../data/curated/combined_feature_set/two_bed_merged.csv')\n",
    "# three_bed_merged = save_feature_set(three_bed_merged, '../data/curated/combined_feature_set/three_bed_merged.csv')\n",
    "\n",
    "# two_bed_house_merged = save_feature_set(two_bed_house_merged, '../data/curated/combined_feature_set/two_bed_house_merged.csv')\n",
    "# three_bed_house_merged = save_feature_set(three_bed_house_merged, '../data/curated/combined_feature_set/three_bed_house_merged.csv')\n",
    "# four_bed_house_merged = save_feature_set(four_bed_house_merged, '../data/curated/combined_feature_set/four_bed_house_merged.csv')\n",
    "\n",
    "# all_properties_merged = save_feature_set(all_properties_merged, '../data/curated/combined_feature_set/all_properties_merged.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
