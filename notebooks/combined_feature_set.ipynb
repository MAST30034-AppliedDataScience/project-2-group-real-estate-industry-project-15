{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_data(df):\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'median_rent_sep'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage on your list of dataframes:\n",
    "domain_one_bed_flat = clean_domain_data(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_data(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_data(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_data(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_data(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_data(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_data(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suburb               0\n",
       "median_bath          0\n",
       "median_parkings      0\n",
       "furnished_count      0\n",
       "unfurnished_count    0\n",
       "pets_allowed         0\n",
       "pets_not_allowed     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def impute_values(main_df, domain_df):\n",
    "    # Merge main_df with domain_df on 'suburb' to keep all years from main_df\n",
    "    merged_df = pd.merge(main_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Impute for each dataset and drop the sep_median from domain DataFrames\n",
    "one_bed_flat, domain_one_bed_flat = impute_values(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_values(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_values(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_values(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_values(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_values(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_values(all_properties, domain_all_properties)\n",
    "\n",
    "# Example output to see the results\n",
    "domain_one_bed_flat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_median    2442\n",
       "jun_median    2442\n",
       "mar_median    2442\n",
       "sep_median    2442\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "#----- Check Null\n",
    "missing_values = one_bed_flat_merged.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_median    2442\n",
       "jun_median    2442\n",
       "mar_median    2442\n",
       "sep_median    2442\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_merged_df(df):\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_merged_df(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_merged_df(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_merged_df(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_merged_df(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_merged_df(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_merged_df(four_bed_house_merged)\n",
    "all_properties_merged = clean_merged_df(all_properties_merged)\n",
    "\n",
    "#----- Check Null\n",
    "missing_values = one_bed_flat_merged.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into training, validation, and testing sets based on the 'year' column:\n",
    "    - Training set includes data from the years 2016-2021.\n",
    "    - Validation set includes data from the years 2022-2024.\n",
    "    - Testing set includes data from the years 2025-2027.\n",
    "\n",
    "    The function merges additional columns that are not part of the feature or target columns \n",
    "    back with the respective sets based on matching suburbs.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input dataframe containing all required columns.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (pd.DataFrame): Training feature set.\n",
    "    - X_val (pd.DataFrame): Validation feature set.\n",
    "    - X_test (pd.DataFrame): Testing feature set.\n",
    "    - y_train (pd.DataFrame): Training target set.\n",
    "    - y_val (pd.DataFrame): Validation target set.\n",
    "    - y_test (pd.DataFrame): Testing target set.\n",
    "    \"\"\"\n",
    "    # Define the year ranges for training, validation, and testing sets\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Columns to include in X and y splits\n",
    "    feature_columns = ['suburb', 'year', 'offence_division', 'total_offence_count', 'erp']\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Split feature and target data\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Split the dataframe into training, validation, and testing sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    # Align target sets with the corresponding feature sets\n",
    "    y_train = y.loc[X_train.index]\n",
    "    y_val = y.loc[X_val.index]\n",
    "    y_test = y.loc[X_test.index]\n",
    "\n",
    "    # Extract other columns not in feature_columns or target_columns, including 'suburb'\n",
    "    other_columns = df.drop(columns=feature_columns + target_columns).columns\n",
    "    other_data = df[other_columns].copy()\n",
    "    other_data['suburb'] = df['suburb']  # Ensure 'suburb' is included\n",
    "\n",
    "    # Merge the 'other' data back with the matching suburbs, irrespective of the year\n",
    "    X_train = X_train.merge(other_data, on='suburb', how='left')\n",
    "    X_val = X_val.merge(other_data, on='suburb', how='left')\n",
    "    X_test = X_test.merge(other_data, on='suburb', how='left')\n",
    "\n",
    "    # Drop 'suburb' and 'year' from the feature sets\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    # Remove rows with NaN values in y_train and y_val\n",
    "    valid_train_indices = y_train.dropna().index\n",
    "    X_train = X_train.loc[valid_train_indices]\n",
    "    y_train = y_train.loc[valid_train_indices]\n",
    "\n",
    "    valid_val_indices = y_val.dropna().index\n",
    "    X_val = X_val.loc[valid_val_indices]\n",
    "    y_val = y_val.loc[valid_val_indices]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = train_val_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = train_val_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = train_val_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = train_val_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = train_val_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = train_val_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = train_val_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 1244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    Compares columns of the training, validation, and testing dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    - train: DataFrame representing the training set.\n",
    "    - val: DataFrame representing the validation set.\n",
    "    - test: DataFrame representing the testing set.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary indicating columns missing in each set.\n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "dataframes_triplets = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_columns(train, val, test) for name, (train, val, test) in dataframes_triplets.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "54           NaN         NaN         NaN         NaN\n",
      "55           NaN         NaN         NaN         NaN\n",
      "56           NaN         NaN         NaN         NaN\n",
      "57           NaN         NaN         NaN         NaN\n",
      "58           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "9715         NaN         NaN         NaN         NaN\n",
      "9716         NaN         NaN         NaN         NaN\n",
      "9717         NaN         NaN         NaN         NaN\n",
      "9718         NaN         NaN         NaN         NaN\n",
      "9719         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2376 rows x 4 columns], 'y_test_two_bed':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "12307         NaN         NaN         NaN         NaN\n",
      "12308         NaN         NaN         NaN         NaN\n",
      "12309         NaN         NaN         NaN         NaN\n",
      "12310         NaN         NaN         NaN         NaN\n",
      "12311         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3024 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "54           NaN         NaN         NaN         NaN\n",
      "55           NaN         NaN         NaN         NaN\n",
      "56           NaN         NaN         NaN         NaN\n",
      "57           NaN         NaN         NaN         NaN\n",
      "58           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "8995         NaN         NaN         NaN         NaN\n",
      "8996         NaN         NaN         NaN         NaN\n",
      "8997         NaN         NaN         NaN         NaN\n",
      "8998         NaN         NaN         NaN         NaN\n",
      "8999         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2232 rows x 4 columns], 'y_test_two_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "11587         NaN         NaN         NaN         NaN\n",
      "11588         NaN         NaN         NaN         NaN\n",
      "11589         NaN         NaN         NaN         NaN\n",
      "11590         NaN         NaN         NaN         NaN\n",
      "11591         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2862 rows x 4 columns], 'y_test_three_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "14251         NaN         NaN         NaN         NaN\n",
      "14252         NaN         NaN         NaN         NaN\n",
      "14253         NaN         NaN         NaN         NaN\n",
      "14254         NaN         NaN         NaN         NaN\n",
      "14255         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3528 rows x 4 columns], 'y_test_four_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "12379         NaN         NaN         NaN         NaN\n",
      "12380         NaN         NaN         NaN         NaN\n",
      "12381         NaN         NaN         NaN         NaN\n",
      "12382         NaN         NaN         NaN         NaN\n",
      "12383         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3078 rows x 4 columns], 'y_test_all_properties':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "14467         NaN         NaN         NaN         NaN\n",
      "14468         NaN         NaN         NaN         NaN\n",
      "14469         NaN         NaN         NaN         NaN\n",
      "14470         NaN         NaN         NaN         NaN\n",
      "14471         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3564 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    2376\n",
      "jun_median    2376\n",
      "mar_median    2376\n",
      "sep_median    2376\n",
      "dtype: int64, 'y_test_two_bed': dec_median    3024\n",
      "jun_median    3024\n",
      "mar_median    3024\n",
      "sep_median    3024\n",
      "dtype: int64, 'y_test_three_bed': dec_median    2232\n",
      "jun_median    2232\n",
      "mar_median    2232\n",
      "sep_median    2232\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    2862\n",
      "jun_median    2862\n",
      "mar_median    2862\n",
      "sep_median    2862\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    3528\n",
      "jun_median    3528\n",
      "mar_median    3528\n",
      "sep_median    3528\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    3078\n",
      "jun_median    3078\n",
      "mar_median    3078\n",
      "sep_median    3078\n",
      "dtype: int64, 'y_test_all_properties': dec_median    3564\n",
      "jun_median    3564\n",
      "mar_median    3564\n",
      "sep_median    3564\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Identify rows with missing values\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe\n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "# Display the missing values summary\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.74\n",
      "Number of components selected: 3\n",
      "Explained variance by selected components: 0.82\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.74\n",
      "Number of components selected: 3\n",
      "Explained variance by selected components: 0.77\n",
      "Number of components selected: 4\n",
      "Explained variance by selected components: 0.83\n",
      "Number of components selected: 3\n",
      "Explained variance by selected components: 0.79\n",
      "Number of components selected: 3\n",
      "Explained variance by selected components: 0.71\n"
     ]
    }
   ],
   "source": [
    "def pca_feature_selection(X_train, X_val, X_test, variance_threshold=0.7):\n",
    "    \"\"\"\n",
    "    The function applies PCA for dimensionality reduction by fitting on the\n",
    "    training set and transforming both the training and test sets. It keeps \n",
    "    either a specified number of components or selects them based on a variance \n",
    "    threshold. It returns the reduced training and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialise PCA, specifying the variance threshold\n",
    "    pca_temp = PCA().fit(X_train_scaled)\n",
    "    cumulative_variance = pca_temp.explained_variance_ratio_.cumsum()\n",
    "    # Find the number of components to capture the specified variance\n",
    "    n_components = next(i for i, total_variance in enumerate(cumulative_variance) if total_variance >= variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the training set and transform both training and test sets\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Number of components selected: {n_components}\")\n",
    "    print(f\"Explained variance by selected components: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    return X_train_pca, X_val_pca, X_test_pca\n",
    "\n",
    "# Perform feature selection with PCA on the X sets \n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed = pca_feature_selection(X_train_one_bed, X_val_one_bed, X_test_one_bed)\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed = pca_feature_selection(X_train_two_bed, X_val_two_bed, X_test_two_bed)\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed = pca_feature_selection(X_train_three_bed, X_val_three_bed, X_test_three_bed)\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house = pca_feature_selection(X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house)\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house = pca_feature_selection(X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house)\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house = pca_feature_selection(X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house)\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties = pca_feature_selection(X_train_all_properties, X_val_all_properties, X_test_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 8053.8958, R^2: -0.4595\n",
      "Predictions for 2025-2027: [[310.30774929 308.82236715 307.34697064 309.47767535]\n",
      " [310.30774929 308.82236715 307.34697064 309.47767535]\n",
      " [310.30774929 308.82236715 307.34697064 309.47767535]\n",
      " ...\n",
      " [300.0366232  298.71998189 297.91785723 299.18394444]\n",
      " [300.0366232  298.71998189 297.91785723 299.18394444]\n",
      " [300.0366232  298.71998189 297.91785723 299.18394444]]\n",
      "\n",
      "Dataset 2:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 13337.8246, R^2: -0.5642\n",
      "Predictions for 2025-2027: [[389.56693218 386.62324256 385.12589965 388.22316099]\n",
      " [389.56693218 386.62324256 385.12589965 388.22316099]\n",
      " [389.56693218 386.62324256 385.12589965 388.22316099]\n",
      " ...\n",
      " [380.90730241 378.23738011 376.87906051 379.62562685]\n",
      " [380.90730241 378.23738011 376.87906051 379.62562685]\n",
      " [380.90730241 378.23738011 376.87906051 379.62562685]]\n",
      "\n",
      "Dataset 3:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 38599.2127, R^2: -0.2970\n",
      "Predictions for 2025-2027: [[517.49843377 512.72444883 509.83017845 514.97341085]\n",
      " [517.49843377 512.72444883 509.83017845 514.97341085]\n",
      " [517.49843377 512.72444883 509.83017845 514.97341085]\n",
      " ...\n",
      " [502.63778748 499.09948569 497.03606566 501.66460349]\n",
      " [502.63778748 499.09948569 497.03606566 501.66460349]\n",
      " [502.63778748 499.09948569 497.03606566 501.66460349]]\n",
      "\n",
      "Dataset 4:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 19087.2956, R^2: -0.3887\n",
      "Predictions for 2025-2027: [[426.26085331 421.33486571 419.78689057 424.33626262]\n",
      " [426.26085331 421.33486571 419.78689057 424.33626262]\n",
      " [426.26085331 421.33486571 419.78689057 424.33626262]\n",
      " ...\n",
      " [415.14771128 410.6515749  408.67858277 413.11715025]\n",
      " [415.14771128 410.6515749  408.67858277 413.11715025]\n",
      " [415.14771128 410.6515749  408.67858277 413.11715025]]\n",
      "\n",
      "Dataset 5:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 40182.1233, R^2: -0.2817\n",
      "Predictions for 2025-2027: [[504.09893322 497.93923259 494.42496228 501.63343018]\n",
      " [504.09893322 497.93923259 494.42496228 501.63343018]\n",
      " [504.09893322 497.93923259 494.42496228 501.63343018]\n",
      " ...\n",
      " [497.75224819 492.60973061 489.25158831 495.53597016]\n",
      " [497.75224819 492.60973061 489.25158831 495.53597016]\n",
      " [497.75224819 492.60973061 489.25158831 495.53597016]]\n",
      "\n",
      "Dataset 6:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 84200.4340, R^2: -0.2343\n",
      "Predictions for 2025-2027: [[656.59886373 653.65170319 649.90700097 656.21837555]\n",
      " [656.59886373 653.65170319 649.90700097 656.21837555]\n",
      " [656.59886373 653.65170319 649.90700097 656.21837555]\n",
      " ...\n",
      " [614.70372111 608.44965762 604.78894568 612.15743273]\n",
      " [614.70372111 608.44965762 604.78894568 612.15743273]\n",
      " [614.70372111 608.44965762 604.78894568 612.15743273]]\n",
      "\n",
      "Dataset 7:\n",
      "Best alpha: 10.0\n",
      "Validation MSE: 15900.0365, R^2: -0.8197\n",
      "Predictions for 2025-2027: [[420.67623875 415.64580936 414.50081772 418.19927546]\n",
      " [420.67623875 415.64580936 414.50081772 418.19927546]\n",
      " [420.67623875 415.64580936 414.50081772 418.19927546]\n",
      " ...\n",
      " [411.48210411 406.80249146 405.37263065 409.2326348 ]\n",
      " [411.48210411 406.80249146 405.37263065 409.2326348 ]\n",
      " [411.48210411 406.80249146 405.37263065 409.2326348 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a list of all training, validation, and test sets\n",
    "datasets = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]\n",
    "\n",
    "# Define hyperparameters for Lasso\n",
    "alpha_range = np.logspace(-4, 1, 10)  # Range of alpha values for tuning\n",
    "param_grid = {'alpha': alpha_range}\n",
    "\n",
    "# Loop through each set, tune the model on the validation set, and predict on the test set\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(datasets):\n",
    "    # Initialize the Lasso model\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    # Set up the GridSearchCV to tune the 'alpha' hyperparameter\n",
    "    grid_search = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Train the model using the training set and validate on the validation set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Select the best model based on the validation set\n",
    "    best_lasso = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_lasso.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_lasso.predict(X_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    print(f\"Best alpha: {best_lasso.alpha}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}\")\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
