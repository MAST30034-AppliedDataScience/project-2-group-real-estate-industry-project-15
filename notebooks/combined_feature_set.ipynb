{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income_2016 = pd.read_csv('../data/curated/income_2016.csv')\n",
    "income_2021 = pd.read_csv('../data/curated/income_2021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_data(df):\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'median_rent_sep'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage on your list of dataframes:\n",
    "domain_one_bed_flat = clean_domain_data(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_data(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_data(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_data(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_data(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_data(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_data(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "suburb               0\n",
       "median_bath          0\n",
       "median_parkings      0\n",
       "furnished_count      0\n",
       "unfurnished_count    0\n",
       "pets_allowed         0\n",
       "pets_not_allowed     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1065,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def impute_values(main_df, domain_df):\n",
    "    # Merge main_df with domain_df on 'suburb' to keep all years from main_df\n",
    "    merged_df = pd.merge(main_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Impute for each dataset and drop the sep_median from domain DataFrames\n",
    "one_bed_flat, domain_one_bed_flat = impute_values(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_values(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_values(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_values(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_values(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_values(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_values(all_properties, domain_all_properties)\n",
    "\n",
    "# Example output to see the results\n",
    "domain_one_bed_flat.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2016 2017 2018 2019 2020 2021 2022 2023 2024 2025 2026 2027]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dec_median    3024\n",
       "jun_median    3024\n",
       "mar_median    3024\n",
       "sep_median    3024\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1066,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "print(one_bed_flat_merged['year'].unique())\n",
    "# one_bed_flat_merged = pd.merge(one_bed_flat_merged, income_2016, on='suburb', how='inner')\n",
    "# one_bed_flat_merged = pd.merge(one_bed_flat_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "# two_bed_flat_merged = pd.merge(two_bed_flat_merged, income_2016, on='suburb', how='inner')\n",
    "# two_bed_flat_merged = pd.merge(two_bed_flat_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "# three_bed_flat_merged = pd.merge(three_bed_flat_merged, income_2016, on='suburb', how='inner')\n",
    "# three_bed_flat_merged = pd.merge(three_bed_flat_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "# two_bed_house_merged = pd.merge(two_bed_house_merged, income_2016, on='suburb', how='inner')\n",
    "# two_bed_house_merged = pd.merge(two_bed_house_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "# three_bed_house_merged = pd.merge(three_bed_house_merged, income_2016, on='suburb', how='inner')\n",
    "# three_bed_house_merged = pd.merge(three_bed_house_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "# four_bed_house_merged = pd.merge(four_bed_house_merged, income_2016, on='suburb', how='inner')\n",
    "# four_bed_house_merged = pd.merge(four_bed_house_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "# all_properties_merged = pd.merge(all_properties_merged, income_2016, on='suburb', how='inner')\n",
    "# all_properties_merged = pd.merge(all_properties_merged, income_2021, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "#----- Check Null\n",
    "missing_values = two_bed_flat_merged.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dec_median    2442\n",
       "jun_median    2442\n",
       "mar_median    2442\n",
       "sep_median    2442\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1067,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_merged_df(df):\n",
    "    \n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_merged_df(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_merged_df(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_merged_df(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_merged_df(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_merged_df(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_merged_df(four_bed_house_merged)\n",
    "all_properties_merged = clean_merged_df(all_properties_merged)\n",
    "\n",
    "#----- Check Null\n",
    "missing_values = one_bed_flat_merged.isnull().sum()\n",
    "columns_with_missing_values = missing_values[missing_values > 0]\n",
    "\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>offence_division</th>\n",
       "      <th>total_offence_count</th>\n",
       "      <th>erp</th>\n",
       "      <th>primary_school_count</th>\n",
       "      <th>secondary_school_count</th>\n",
       "      <th>tertiary_institutions_count</th>\n",
       "      <th>avg_primary_school_rank</th>\n",
       "      <th>avg_secondary_school_rank</th>\n",
       "      <th>has_primary_school</th>\n",
       "      <th>has_secondary_school</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_to_restaurant</th>\n",
       "      <th>distance_to_supermarket</th>\n",
       "      <th>nearest_transport_avg_distance</th>\n",
       "      <th>distance_to_cbd</th>\n",
       "      <th>median_bath</th>\n",
       "      <th>median_parkings</th>\n",
       "      <th>furnished_count</th>\n",
       "      <th>unfurnished_count</th>\n",
       "      <th>pets_allowed</th>\n",
       "      <th>pets_not_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>8770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>964.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9697</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601167</td>\n",
       "      <td>1.664871</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>125.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9698</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601167</td>\n",
       "      <td>1.664871</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>125.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9699</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601167</td>\n",
       "      <td>1.664871</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>125.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9700</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601167</td>\n",
       "      <td>1.664871</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>125.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15507.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>583.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.601167</td>\n",
       "      <td>1.664871</td>\n",
       "      <td>2.836667</td>\n",
       "      <td>125.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7062 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      offence_division  total_offence_count      erp  primary_school_count  \\\n",
       "0                  1.0                107.0   8770.0                     2   \n",
       "1                  1.0                107.0   8770.0                     2   \n",
       "2                  1.0                107.0   8770.0                     2   \n",
       "3                  1.0                107.0   8770.0                     2   \n",
       "4                  1.0                107.0   8770.0                     2   \n",
       "...                ...                  ...      ...                   ...   \n",
       "9697               6.0                 14.0  15507.0                     2   \n",
       "9698               6.0                 14.0  15507.0                     2   \n",
       "9699               6.0                 14.0  15507.0                     2   \n",
       "9700               6.0                 14.0  15507.0                     2   \n",
       "9701               6.0                 14.0  15507.0                     2   \n",
       "\n",
       "      secondary_school_count  tertiary_institutions_count  \\\n",
       "0                          0                            0   \n",
       "1                          0                            0   \n",
       "2                          0                            0   \n",
       "3                          0                            0   \n",
       "4                          0                            0   \n",
       "...                      ...                          ...   \n",
       "9697                       0                            0   \n",
       "9698                       0                            0   \n",
       "9699                       0                            0   \n",
       "9700                       0                            0   \n",
       "9701                       0                            0   \n",
       "\n",
       "      avg_primary_school_rank  avg_secondary_school_rank  has_primary_school  \\\n",
       "0                       964.5                        0.0                   1   \n",
       "1                       964.5                        0.0                   1   \n",
       "2                       964.5                        0.0                   1   \n",
       "3                       964.5                        0.0                   1   \n",
       "4                       964.5                        0.0                   1   \n",
       "...                       ...                        ...                 ...   \n",
       "9697                    583.5                        0.0                   1   \n",
       "9698                    583.5                        0.0                   1   \n",
       "9699                    583.5                        0.0                   1   \n",
       "9700                    583.5                        0.0                   1   \n",
       "9701                    583.5                        0.0                   1   \n",
       "\n",
       "      has_secondary_school  ...  distance_to_restaurant  \\\n",
       "0                        0  ...                1.141601   \n",
       "1                        0  ...                1.141601   \n",
       "2                        0  ...                1.141601   \n",
       "3                        0  ...                1.141601   \n",
       "4                        0  ...                1.141601   \n",
       "...                    ...  ...                     ...   \n",
       "9697                     0  ...                1.601167   \n",
       "9698                     0  ...                1.601167   \n",
       "9699                     0  ...                1.601167   \n",
       "9700                     0  ...                1.601167   \n",
       "9701                     0  ...                1.601167   \n",
       "\n",
       "      distance_to_supermarket  nearest_transport_avg_distance  \\\n",
       "0                    1.083238                        1.110000   \n",
       "1                    1.083238                        1.110000   \n",
       "2                    1.083238                        1.110000   \n",
       "3                    1.083238                        1.110000   \n",
       "4                    1.083238                        1.110000   \n",
       "...                       ...                             ...   \n",
       "9697                 1.664871                        2.836667   \n",
       "9698                 1.664871                        2.836667   \n",
       "9699                 1.664871                        2.836667   \n",
       "9700                 1.664871                        2.836667   \n",
       "9701                 1.664871                        2.836667   \n",
       "\n",
       "      distance_to_cbd  median_bath  median_parkings  furnished_count  \\\n",
       "0                5.55            1                0                2   \n",
       "1                5.55            1                0                2   \n",
       "2                5.55            1                0                2   \n",
       "3                5.55            1                0                2   \n",
       "4                5.55            1                0                2   \n",
       "...               ...          ...              ...              ...   \n",
       "9697           125.11            1                1                0   \n",
       "9698           125.11            1                1                0   \n",
       "9699           125.11            1                1                0   \n",
       "9700           125.11            1                1                0   \n",
       "9701           125.11            1                1                0   \n",
       "\n",
       "      unfurnished_count  pets_allowed  pets_not_allowed  \n",
       "0                    10             4                 8  \n",
       "1                    10             4                 8  \n",
       "2                    10             4                 8  \n",
       "3                    10             4                 8  \n",
       "4                    10             4                 8  \n",
       "...                 ...           ...               ...  \n",
       "9697                  1             0                 1  \n",
       "9698                  1             0                 1  \n",
       "9699                  1             0                 1  \n",
       "9700                  1             0                 1  \n",
       "9701                  1             0                 1  \n",
       "\n",
       "[7062 rows x 61 columns]"
      ]
     },
     "execution_count": 1068,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_test_sets(df):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into training and testing sets based on the 'year' column:\n",
    "    - Training set includes data from the years 2016-2024.\n",
    "    - Testing set includes data from the years 2025-2027.\n",
    "\n",
    "    The function merges additional columns that are not part of the feature or target columns \n",
    "    back with the respective training and testing sets based on matching suburbs.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input dataframe containing all required columns.\n",
    "\n",
    "    Returns:\n",
    "    - X_train (pd.DataFrame): Training feature set.\n",
    "    - X_test (pd.DataFrame): Testing feature set.\n",
    "    - y_train (pd.DataFrame): Training target set.\n",
    "    - y_test (pd.DataFrame): Testing target set.\n",
    "    \"\"\"\n",
    "    # Define the year ranges for training and testing sets\n",
    "    train_years = range(2016, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Columns to include in X and y splits\n",
    "    feature_columns = ['suburb', 'year', 'offence_division', 'total_offence_count', 'erp']\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Split feature and target data\n",
    "    X = df[feature_columns]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Split the dataframe into training and testing sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    # Align target sets with the corresponding feature sets\n",
    "    y_train = y.loc[X_train.index]\n",
    "    y_test = y.loc[X_test.index]\n",
    "\n",
    "    # Extract other columns not in feature_columns or target_columns, including 'suburb'\n",
    "    other_columns = df.drop(columns=feature_columns + target_columns).columns\n",
    "    other_data = df[other_columns].copy()\n",
    "    other_data['suburb'] = df['suburb']  # Ensure 'suburb' is included\n",
    "\n",
    "    # Merge the 'other' data back with the matching suburbs, irrespective of the year\n",
    "    X_train = X_train.merge(other_data, on='suburb', how='left')\n",
    "    X_test = X_test.merge(other_data, on='suburb', how='left')\n",
    "\n",
    "    # Drop 'suburb' and 'year' from the feature sets\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    # Remove rows with NaN values in y_train\n",
    "    valid_indices = y_train.dropna().index\n",
    "    X_train = X_train.loc[valid_indices]\n",
    "    y_train = y_train.loc[valid_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Create training and test sets for each property type\n",
    "X_train_one_bed, X_test_one_bed, y_train_one_bed, y_test_one_bed = train_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed, X_test_two_bed, y_train_two_bed, y_test_two_bed = train_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed, X_test_three_bed, y_train_three_bed, y_test_three_bed = train_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_test_two_bed_house = train_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_test_three_bed_house = train_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_test_four_bed_house = train_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties, X_test_all_properties, y_train_all_properties, y_test_all_properties = train_test_sets(all_properties_merged)\n",
    "\n",
    "X_train_one_bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1069,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in both train and test sets.',\n",
       " 'Two Bed': 'Columns are the same in both train and test sets.',\n",
       " 'Three Bed': 'Columns are the same in both train and test sets.',\n",
       " 'Two Bed House': 'Columns are the same in both train and test sets.',\n",
       " 'Three Bed House': 'Columns are the same in both train and test sets.',\n",
       " 'Four Bed House': 'Columns are the same in both train and test sets.',\n",
       " 'All Properties': 'Columns are the same in both train and test sets.'}"
      ]
     },
     "execution_count": 1069,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_columns(train, test):\n",
    "    \"\"\"\n",
    "    Compares columns of the training and testing dataframes.\n",
    "    \n",
    "    Parameters:\n",
    "    - train: DataFrame representing the training set.\n",
    "    - test: DataFrame representing the testing set.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary indicating columns missing in each set.\n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    columns_match = train.columns.equals(test.columns)\n",
    "    \n",
    "    if not columns_match:\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train = set(test.columns) - set(train.columns)\n",
    "        comparison_result = {\n",
    "            \"Columns missing in test set\": list(missing_in_test),\n",
    "            \"Columns missing in train set\": list(missing_in_train)\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in both train and test sets.\"\n",
    "    \n",
    "    return comparison_result\n",
    "\n",
    "# List of training and testing DataFrames to compare\n",
    "dataframes_pairs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each pair of training and testing sets\n",
    "comparison_results = {name: compare_columns(train, test) for name, (train, test) in dataframes_pairs.items()}\n",
    "\n",
    "# Display comparison results\n",
    "comparison_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "54           NaN         NaN         NaN         NaN\n",
      "55           NaN         NaN         NaN         NaN\n",
      "56           NaN         NaN         NaN         NaN\n",
      "57           NaN         NaN         NaN         NaN\n",
      "58           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "9715         NaN         NaN         NaN         NaN\n",
      "9716         NaN         NaN         NaN         NaN\n",
      "9717         NaN         NaN         NaN         NaN\n",
      "9718         NaN         NaN         NaN         NaN\n",
      "9719         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2376 rows x 4 columns], 'y_test_two_bed':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "12307         NaN         NaN         NaN         NaN\n",
      "12308         NaN         NaN         NaN         NaN\n",
      "12309         NaN         NaN         NaN         NaN\n",
      "12310         NaN         NaN         NaN         NaN\n",
      "12311         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3024 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "54           NaN         NaN         NaN         NaN\n",
      "55           NaN         NaN         NaN         NaN\n",
      "56           NaN         NaN         NaN         NaN\n",
      "57           NaN         NaN         NaN         NaN\n",
      "58           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "8995         NaN         NaN         NaN         NaN\n",
      "8996         NaN         NaN         NaN         NaN\n",
      "8997         NaN         NaN         NaN         NaN\n",
      "8998         NaN         NaN         NaN         NaN\n",
      "8999         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2232 rows x 4 columns], 'y_test_two_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "11587         NaN         NaN         NaN         NaN\n",
      "11588         NaN         NaN         NaN         NaN\n",
      "11589         NaN         NaN         NaN         NaN\n",
      "11590         NaN         NaN         NaN         NaN\n",
      "11591         NaN         NaN         NaN         NaN\n",
      "\n",
      "[2862 rows x 4 columns], 'y_test_three_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "14251         NaN         NaN         NaN         NaN\n",
      "14252         NaN         NaN         NaN         NaN\n",
      "14253         NaN         NaN         NaN         NaN\n",
      "14254         NaN         NaN         NaN         NaN\n",
      "14255         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3528 rows x 4 columns], 'y_test_four_bed_house':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "12379         NaN         NaN         NaN         NaN\n",
      "12380         NaN         NaN         NaN         NaN\n",
      "12381         NaN         NaN         NaN         NaN\n",
      "12382         NaN         NaN         NaN         NaN\n",
      "12383         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3078 rows x 4 columns], 'y_test_all_properties':        dec_median  jun_median  mar_median  sep_median\n",
      "54            NaN         NaN         NaN         NaN\n",
      "55            NaN         NaN         NaN         NaN\n",
      "56            NaN         NaN         NaN         NaN\n",
      "57            NaN         NaN         NaN         NaN\n",
      "58            NaN         NaN         NaN         NaN\n",
      "...           ...         ...         ...         ...\n",
      "14467         NaN         NaN         NaN         NaN\n",
      "14468         NaN         NaN         NaN         NaN\n",
      "14469         NaN         NaN         NaN         NaN\n",
      "14470         NaN         NaN         NaN         NaN\n",
      "14471         NaN         NaN         NaN         NaN\n",
      "\n",
      "[3564 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    2376\n",
      "jun_median    2376\n",
      "mar_median    2376\n",
      "sep_median    2376\n",
      "dtype: int64, 'y_test_two_bed': dec_median    3024\n",
      "jun_median    3024\n",
      "mar_median    3024\n",
      "sep_median    3024\n",
      "dtype: int64, 'y_test_three_bed': dec_median    2232\n",
      "jun_median    2232\n",
      "mar_median    2232\n",
      "sep_median    2232\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    2862\n",
      "jun_median    2862\n",
      "mar_median    2862\n",
      "sep_median    2862\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    3528\n",
      "jun_median    3528\n",
      "mar_median    3528\n",
      "sep_median    3528\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    3078\n",
      "jun_median    3078\n",
      "mar_median    3078\n",
      "sep_median    3078\n",
      "dtype: int64, 'y_test_all_properties': dec_median    3564\n",
      "jun_median    3564\n",
      "mar_median    3564\n",
      "sep_median    3564\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    # Identify rows with missing values\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe\n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "# Display the missing values summary\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.96\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.81\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.96\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.96\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.75\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.83\n",
      "Number of components selected: 2\n",
      "Explained variance by selected components: 0.78\n"
     ]
    }
   ],
   "source": [
    "def pca_feature_selection(X_train, X_test, variance_threshold=0.7):\n",
    "    \"\"\"\n",
    "    The function applies PCA for dimensionality reduction by fitting on the\n",
    "    training set and transforming both the training and test sets. It keeps \n",
    "    either a specified number of components or selects them based on a variance \n",
    "    threshold. It returns the reduced training and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialise PCA, specifying the variance threshold\n",
    "    pca_temp = PCA().fit(X_train_scaled)\n",
    "    cumulative_variance = pca_temp.explained_variance_ratio_.cumsum()\n",
    "    # Find the number of components to capture the specified variance\n",
    "    n_components = next(i for i, total_variance in enumerate(cumulative_variance) if total_variance >= variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the training set and transform both training and test sets\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Number of components selected: {n_components}\")\n",
    "    print(f\"Explained variance by selected components: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "# Perform feature selection with PCA on the X sets \n",
    "X_train_one_bed, X_test_one_bed = pca_feature_selection(X_train_one_bed, X_test_one_bed )\n",
    "X_train_two_bed, X_test_two_bed = pca_feature_selection(X_train_two_bed, X_test_two_bed)\n",
    "X_train_three_bed, X_test_three_bed = pca_feature_selection(X_train_three_bed, X_test_three_bed)\n",
    "X_train_two_bed_house, X_test_two_bed_house = pca_feature_selection(X_train_two_bed_house, X_test_two_bed_house)\n",
    "X_train_three_bed_house, X_test_three_bed_house = pca_feature_selection(X_train_three_bed_house, X_test_three_bed_house)\n",
    "X_train_four_bed_house, X_test_four_bed_house = pca_feature_selection(X_train_four_bed_house, X_test_four_bed_house)\n",
    "X_train_all_properties, X_test_all_properties = pca_feature_selection(X_train_all_properties, X_test_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Predictions for 2025-2027: [[328.87932938 323.657938   321.03655138 328.59659305]\n",
      " [328.87932938 323.657938   321.03655138 328.59659305]\n",
      " [328.87932938 323.657938   321.03655138 328.59659305]\n",
      " ...\n",
      " [313.39225394 308.18721504 305.97285802 313.88934021]\n",
      " [313.39225394 308.18721504 305.97285802 313.88934021]\n",
      " [313.39225394 308.18721504 305.97285802 313.88934021]]\n",
      "\n",
      "Dataset 2:\n",
      "Predictions for 2025-2027: [[415.58430435 408.32233606 404.80747621 417.05636782]\n",
      " [415.58430435 408.32233606 404.80747621 417.05636782]\n",
      " [415.58430435 408.32233606 404.80747621 417.05636782]\n",
      " ...\n",
      " [401.92908586 394.54487049 391.19214922 402.11053931]\n",
      " [401.92908586 394.54487049 391.19214922 402.11053931]\n",
      " [401.92908586 394.54487049 391.19214922 402.11053931]]\n",
      "\n",
      "Dataset 3:\n",
      "Predictions for 2025-2027: [[555.64479225 545.81355905 541.60611239 558.8871566 ]\n",
      " [555.64479225 545.81355905 541.60611239 558.8871566 ]\n",
      " [555.64479225 545.81355905 541.60611239 558.8871566 ]\n",
      " ...\n",
      " [543.64606343 534.36644158 530.08386343 547.94763569]\n",
      " [543.64606343 534.36644158 530.08386343 547.94763569]\n",
      " [543.64606343 534.36644158 530.08386343 547.94763569]]\n",
      "\n",
      "Dataset 4:\n",
      "Predictions for 2025-2027: [[447.54189535 440.42003064 437.69967246 449.73798751]\n",
      " [447.54189535 440.42003064 437.69967246 449.73798751]\n",
      " [447.54189535 440.42003064 437.69967246 449.73798751]\n",
      " ...\n",
      " [441.52275049 434.60160901 431.58391661 443.76564266]\n",
      " [441.52275049 434.60160901 431.58391661 443.76564266]\n",
      " [441.52275049 434.60160901 431.58391661 443.76564266]]\n",
      "\n",
      "Dataset 5:\n",
      "Predictions for 2025-2027: [[549.20060598 539.8878532  535.60519868 551.39779031]\n",
      " [549.20060598 539.8878532  535.60519868 551.39779031]\n",
      " [549.20060598 539.8878532  535.60519868 551.39779031]\n",
      " ...\n",
      " [507.16798103 498.44215672 494.56044236 510.23964589]\n",
      " [507.16798103 498.44215672 494.56044236 510.23964589]\n",
      " [507.16798103 498.44215672 494.56044236 510.23964589]]\n",
      "\n",
      "Dataset 6:\n",
      "Predictions for 2025-2027: [[661.08580384 650.8007127  644.85679958 666.22361904]\n",
      " [661.08580384 650.8007127  644.85679958 666.22361904]\n",
      " [661.08580384 650.8007127  644.85679958 666.22361904]\n",
      " ...\n",
      " [646.06936831 634.65307586 629.15816208 649.40958255]\n",
      " [646.06936831 634.65307586 629.15816208 649.40958255]\n",
      " [646.06936831 634.65307586 629.15816208 649.40958255]]\n",
      "\n",
      "Dataset 7:\n",
      "Predictions for 2025-2027: [[458.46336585 449.66593755 446.49912185 461.7762763 ]\n",
      " [458.46336585 449.66593755 446.49912185 461.7762763 ]\n",
      " [458.46336585 449.66593755 446.49912185 461.7762763 ]\n",
      " ...\n",
      " [442.64632487 434.22150762 430.90705527 445.72790659]\n",
      " [442.64632487 434.22150762 430.90705527 445.72790659]\n",
      " [442.64632487 434.22150762 430.90705527 445.72790659]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define a list of all training and test sets, including sets for 2025-2027 with no y_test values\n",
    "datasets = [\n",
    "    (X_train_one_bed, X_test_one_bed, y_train_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_test_two_bed, y_train_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_test_three_bed, y_train_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_test_all_properties, y_train_all_properties, y_test_all_properties),\n",
    "]\n",
    "\n",
    "# Define hyperparameters for Lasso\n",
    "alpha = 1.0  # Adjust this parameter based on your needs\n",
    "\n",
    "# Loop through each set and train the Lasso model\n",
    "for i, (X_train, X_test, y_train, y_test) in enumerate(datasets):\n",
    "    # Initialize the Lasso model\n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    \n",
    "    # Train the model\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    \n",
    "    print(f\"Predictions for 2025-2027: {y_pred}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
