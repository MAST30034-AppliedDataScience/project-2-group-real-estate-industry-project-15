{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>suburb</th>\n",
       "      <th>livability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>0.325155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>0.322438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>0.320943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>0.314829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>abbotsford</td>\n",
       "      <td>0.313954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>2023</td>\n",
       "      <td>yarraville</td>\n",
       "      <td>0.506413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2360</th>\n",
       "      <td>2024</td>\n",
       "      <td>yarraville</td>\n",
       "      <td>0.506005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2361</th>\n",
       "      <td>2025</td>\n",
       "      <td>yarraville</td>\n",
       "      <td>0.505578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2362</th>\n",
       "      <td>2026</td>\n",
       "      <td>yarraville</td>\n",
       "      <td>0.505217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>2027</td>\n",
       "      <td>yarraville</td>\n",
       "      <td>0.504793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2364 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year      suburb  livability\n",
       "0     2016  abbotsford    0.325155\n",
       "1     2017  abbotsford    0.322438\n",
       "2     2018  abbotsford    0.320943\n",
       "3     2019  abbotsford    0.314829\n",
       "4     2020  abbotsford    0.313954\n",
       "...    ...         ...         ...\n",
       "2359  2023  yarraville    0.506413\n",
       "2360  2024  yarraville    0.506005\n",
       "2361  2025  yarraville    0.505578\n",
       "2362  2026  yarraville    0.505217\n",
       "2363  2027  yarraville    0.504793\n",
       "\n",
       "[2364 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')\n",
    "\n",
    "# Livability Index data \n",
    "livability_one_bed_flat = pd.read_csv('../data/curated/livability/livability_one_bed_flat.csv')\n",
    "livability_two_bed_flat = pd.read_csv('../data/curated/livability/livability_two_bed_flat.csv')\n",
    "livability_three_bed_flat = pd.read_csv('../data/curated/livability/livability_three_bed_flat.csv')\n",
    "livability_two_bed_house = pd.read_csv('../data/curated/livability/livability_two_bed_house.csv')\n",
    "livability_three_bed_house = pd.read_csv('../data/curated/livability/livability_three_bed_house.csv')\n",
    "livability_four_bed_house = pd.read_csv('../data/curated/livability/livability_four_bed_house.csv')\n",
    "livability_all_properties = pd.read_csv('../data/curated/livability/livability_all_properties.csv')\n",
    "livability_all_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_df(df):\n",
    "    \"\"\"\n",
    "    This function cleans the domain dataframes by removing\n",
    "    the 'Unnamed:' column, renaming median_rent to 'sep_median'\n",
    "    (for a standardised column name as in rental history dfs) and\n",
    "    also creates a year column and inputs the relevant year that\n",
    "    the data is from - 2024. \n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'sep_median'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the clean_domain_df function to all the domain dataframes\n",
    "domain_one_bed_flat = clean_domain_df(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_df(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_df(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_df(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_df(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_df(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_df(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties into the rental history dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_sep_2024_rental_data(rental_history_df, domain_df):\n",
    "    \"\"\"\n",
    "    This function retrieves all the median rental prices in \n",
    "    September from the domain dataframes and then imputes\n",
    "    them into the rental history dataframes where the year\n",
    "    is 2024 and month is September. \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge rental_history_df with domain_df on 'suburb' to keep all years from rental_history_df\n",
    "    merged_df = pd.merge(rental_history_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "\n",
    "    # Filter the dataframe to keep only the suburbs that appear 9 or more times\n",
    "    suburb_counts = merged_df['suburb'].value_counts()\n",
    "    suburbs_to_keep = suburb_counts[suburb_counts >= 9].index\n",
    "    merged_df = merged_df[merged_df['suburb'].isin(suburbs_to_keep)]\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Apply the function to each dataset \n",
    "one_bed_flat, domain_one_bed_flat = impute_sep_2024_rental_data(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_sep_2024_rental_data(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_sep_2024_rental_data(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_sep_2024_rental_data(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_sep_2024_rental_data(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_sep_2024_rental_data(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_sep_2024_rental_data(all_properties, domain_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, livability_one_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "one_bed_flat_merged = one_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, livability_two_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_flat_merged = two_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, livability_three_bed_flat, on=['suburb', 'year'], how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_flat_merged = three_bed_flat_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, livability_two_bed_house, on=['suburb', 'year'], how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "two_bed_house_merged = two_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, livability_three_bed_house, on=['suburb', 'year'], how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "three_bed_house_merged = three_bed_house_merged.drop_duplicates()\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, livability_four_bed_house, on=['suburb', 'year'], how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "four_bed_house_merged =four_bed_house_merged.drop_duplicates()\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, livability_all_properties, on=['suburb', 'year'], how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "all_properties_merged = all_properties_merged.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_suburb_cols(df):\n",
    "    \"\"\"\n",
    "    This function removes all duplicated of the suburb column name \n",
    "    from the merged dataframes. The duplicate suburb column name \n",
    "    could be 'Unnamed', 'sa2_name' or 'gazetted_locality'.\n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_suburb_cols(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_suburb_cols(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_suburb_cols(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_suburb_cols(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_suburb_cols(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_suburb_cols(four_bed_house_merged)\n",
    "all_properties_merged = clean_suburb_cols(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_before_2025(df, median_columns):\n",
    "    # Drop rows where year is less than 2025 and NaN values exist in any of the median columns\n",
    "    return df[~((df['year'] < 2025) & (df[median_columns].isnull().any(axis=1)))]\n",
    "\n",
    "# Define the median columns to check\n",
    "median_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# Call the function for each dataframe and reassign the cleaned data\n",
    "one_bed_flat_merged = remove_nan_before_2025(one_bed_flat_merged, median_columns)\n",
    "two_bed_flat_merged = remove_nan_before_2025(two_bed_flat_merged, median_columns)\n",
    "three_bed_flat_merged = remove_nan_before_2025(three_bed_flat_merged, median_columns)\n",
    "two_bed_house_merged = remove_nan_before_2025(two_bed_house_merged, median_columns)\n",
    "three_bed_house_merged = remove_nan_before_2025(three_bed_house_merged, median_columns)\n",
    "four_bed_house_merged = remove_nan_before_2025(four_bed_house_merged, median_columns)\n",
    "all_properties_merged = remove_nan_before_2025(all_properties_merged, median_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Properties Dataframe for Visualisation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_dataframes():\n",
    "    # Define the base path\n",
    "    base_path = '../data/curated/merged_feature_set'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    # Save each dataframe to a CSV file\n",
    "    one_bed_flat_merged.to_csv(os.path.join(base_path, 'one_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_flat_merged.to_csv(os.path.join(base_path, 'two_bed_flat_merged.csv'), index=False)\n",
    "    three_bed_flat_merged.to_csv(os.path.join(base_path, 'three_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_house_merged.to_csv(os.path.join(base_path, 'two_bed_house_merged.csv'), index=False)\n",
    "    three_bed_house_merged.to_csv(os.path.join(base_path, 'three_bed_house_merged.csv'), index=False)\n",
    "    four_bed_house_merged.to_csv(os.path.join(base_path, 'four_bed_house_merged.csv'), index=False)\n",
    "    all_properties_merged.to_csv(os.path.join(base_path, 'all_properties_merged.csv'), index=False)\n",
    "\n",
    "# Call the function\n",
    "save_merged_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    This function splits the dataframe into training, validation, and testing sets\n",
    "    based on the 'year' column:\n",
    "    - Training set: 2016-2021\n",
    "    - Validation set: 2022-2024\n",
    "    - Testing set: 2025-2027\n",
    "\n",
    "    It also merges additional columns that are not part of the \n",
    "    features specific to years or target columns back with the \n",
    "    respective sets based on matching suburbs.\n",
    "\n",
    "    The function returns:\n",
    "    - X_train, X_val, X_test: Feature sets\n",
    "    - y_train, y_val, y_test: Target sets\n",
    "    \"\"\"\n",
    "    # Define the year ranges\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Define target columns (excluding suburb and year from the drop)\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Keep suburb and year in the features\n",
    "    X = df.drop(columns=target_columns)  # This keeps 'suburb' and 'year' in X\n",
    "    y = df[['suburb', 'year'] + target_columns]  # Target includes suburb, year, and target columns\n",
    "\n",
    "    # Split into train, validation, and test sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    y_train = y[y['year'].isin(train_years)]\n",
    "    y_val = y[y['year'].isin(val_years)]\n",
    "    y_test = y[y['year'].isin(test_years)]\n",
    "\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels = train_val_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels = train_val_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels = train_val_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels = train_val_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels = train_val_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels = train_val_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels = train_val_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_labels(X_train, X_val, X_test, y_train, y_val, y_test):\n",
    "    # Drop the specified columns from X dataframes\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    # Drop the specified columns from y dataframes\n",
    "    y_train = y_train.drop(columns=['suburb', 'year'])\n",
    "    y_val = y_val.drop(columns=['suburb', 'year'])\n",
    "    y_test = y_test.drop(columns=['suburb', 'year'])\n",
    "    \n",
    "    # Return the modified dataframes without '_labels'\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# For each dataset, apply the function\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = drop_labels(\n",
    "    X_train_one_bed_labels, X_val_one_bed_labels, X_test_one_bed_labels, y_train_one_bed_labels, y_val_one_bed_labels, y_test_one_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = drop_labels(\n",
    "    X_train_two_bed_labels, X_val_two_bed_labels, X_test_two_bed_labels, y_train_two_bed_labels, y_val_two_bed_labels, y_test_two_bed_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = drop_labels(\n",
    "    X_train_three_bed_labels, X_val_three_bed_labels, X_test_three_bed_labels, y_train_three_bed_labels, y_val_three_bed_labels, y_test_three_bed_labels\n",
    ")\n",
    "\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = drop_labels(\n",
    "    X_train_two_bed_house_labels, X_val_two_bed_house_labels, X_test_two_bed_house_labels, y_train_two_bed_house_labels, y_val_two_bed_house_labels, y_test_two_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = drop_labels(\n",
    "    X_train_three_bed_house_labels, X_val_three_bed_house_labels, X_test_three_bed_house_labels, y_train_three_bed_house_labels, y_val_three_bed_house_labels, y_test_three_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = drop_labels(\n",
    "    X_train_four_bed_house_labels, X_val_four_bed_house_labels, X_test_four_bed_house_labels, y_train_four_bed_house_labels, y_val_four_bed_house_labels, y_test_four_bed_house_labels\n",
    ")\n",
    "\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = drop_labels(\n",
    "    X_train_all_properties_labels, X_val_all_properties_labels, X_test_all_properties_labels, y_train_all_properties_labels, y_val_all_properties_labels, y_test_all_properties_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_feature_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    This function compares columns of the training, validation, and \n",
    "    testing feature dataframes (X). A dictionary is then returned\n",
    "    indicating if any columns are missing in each set or if all\n",
    "    the colums are the same. \n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "feature_dfs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_feature_columns(train, val, test) for name, (train, val, test) in feature_dfs.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1531         NaN         NaN         NaN         NaN\n",
      "1532         NaN         NaN         NaN         NaN\n",
      "1542         NaN         NaN         NaN         NaN\n",
      "1543         NaN         NaN         NaN         NaN\n",
      "1544         NaN         NaN         NaN         NaN\n",
      "\n",
      "[393 rows x 4 columns], 'y_test_two_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1990         NaN         NaN         NaN         NaN\n",
      "1991         NaN         NaN         NaN         NaN\n",
      "2001         NaN         NaN         NaN         NaN\n",
      "2002         NaN         NaN         NaN         NaN\n",
      "2003         NaN         NaN         NaN         NaN\n",
      "\n",
      "[501 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1444         NaN         NaN         NaN         NaN\n",
      "1445         NaN         NaN         NaN         NaN\n",
      "1455         NaN         NaN         NaN         NaN\n",
      "1456         NaN         NaN         NaN         NaN\n",
      "1457         NaN         NaN         NaN         NaN\n",
      "\n",
      "[369 rows x 4 columns], 'y_test_two_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1828         NaN         NaN         NaN         NaN\n",
      "1829         NaN         NaN         NaN         NaN\n",
      "1839         NaN         NaN         NaN         NaN\n",
      "1840         NaN         NaN         NaN         NaN\n",
      "1841         NaN         NaN         NaN         NaN\n",
      "\n",
      "[474 rows x 4 columns], 'y_test_three_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2308         NaN         NaN         NaN         NaN\n",
      "2309         NaN         NaN         NaN         NaN\n",
      "2319         NaN         NaN         NaN         NaN\n",
      "2320         NaN         NaN         NaN         NaN\n",
      "2321         NaN         NaN         NaN         NaN\n",
      "\n",
      "[585 rows x 4 columns], 'y_test_four_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "0            NaN         NaN         NaN         NaN\n",
      "1            NaN         NaN         NaN         NaN\n",
      "2            NaN         NaN         NaN         NaN\n",
      "12           NaN         NaN         NaN         NaN\n",
      "13           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2017         NaN         NaN         NaN         NaN\n",
      "2018         NaN         NaN         NaN         NaN\n",
      "2028         NaN         NaN         NaN         NaN\n",
      "2029         NaN         NaN         NaN         NaN\n",
      "2030         NaN         NaN         NaN         NaN\n",
      "\n",
      "[510 rows x 4 columns], 'y_test_all_properties':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2350         NaN         NaN         NaN         NaN\n",
      "2351         NaN         NaN         NaN         NaN\n",
      "2361         NaN         NaN         NaN         NaN\n",
      "2362         NaN         NaN         NaN         NaN\n",
      "2363         NaN         NaN         NaN         NaN\n",
      "\n",
      "[591 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    393\n",
      "jun_median    393\n",
      "mar_median    393\n",
      "sep_median    393\n",
      "dtype: int64, 'y_test_two_bed': dec_median    501\n",
      "jun_median    501\n",
      "mar_median    501\n",
      "sep_median    501\n",
      "dtype: int64, 'y_test_three_bed': dec_median    369\n",
      "jun_median    369\n",
      "mar_median    369\n",
      "sep_median    369\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    474\n",
      "jun_median    474\n",
      "mar_median    474\n",
      "sep_median    474\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    585\n",
      "jun_median    585\n",
      "mar_median    585\n",
      "sep_median    585\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    510\n",
      "jun_median    510\n",
      "mar_median    510\n",
      "sep_median    510\n",
      "dtype: int64, 'y_test_all_properties': dec_median    591\n",
      "jun_median    591\n",
      "mar_median    591\n",
      "sep_median    591\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe by columns \n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of all training, validation, and test sets\n",
    "ML_dfs = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "def rfecv_with_random_forest(X_train, X_val, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Apply Recursive Feature Elimination with Cross-Validation (RFECV) using\n",
    "    Random Forest as the estimator to automatically select the optimal number\n",
    "    of features, then use GridSearchCV to fine-tune hyperparameters on the reduced feature set.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: Training feature set\n",
    "    X_val: Validation feature set\n",
    "    X_test: Test feature set\n",
    "    y_train: Training labels (target)\n",
    "    \n",
    "    Returns:\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv: Reduced datasets\n",
    "    best_rf: Best tuned Random Forest model after feature selection\n",
    "    \"\"\"\n",
    "    \n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Use RFECV to automatically select the optimal number of features\n",
    "    rfecv = RFECV(estimator=rf_model, step=1, cv=KFold(5), scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    rfecv.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Transform the datasets based on the selected features\n",
    "    X_train_rfecv = rfecv.transform(X_train_scaled)\n",
    "    X_val_rfecv = rfecv.transform(X_val_scaled)\n",
    "    X_test_rfecv = rfecv.transform(X_test_scaled)\n",
    "\n",
    "    # Perform hyperparameter tuning using GridSearchCV on the reduced feature set\n",
    "    rf_after_rfecv = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(rf_after_rfecv, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(X_train_rfecv, y_train)\n",
    "    \n",
    "    # Select the best Random Forest model based on GridSearchCV\n",
    "    best_rf = grid_search.best_estimator_\n",
    "\n",
    "    # Get the selected feature indices and names\n",
    "    selected_features = rfecv.get_support(indices=True)\n",
    "    selected_feature_names = [X_train.columns[i] for i in selected_features]  # Use X_train columns\n",
    "\n",
    "    # Get the feature importance values from the trained Random Forest model (best_rf)\n",
    "    importances = best_rf.feature_importances_\n",
    "\n",
    "    # Pair selected features with their corresponding importance values\n",
    "    feature_importance_pairs = list(zip(selected_feature_names, importances))\n",
    "\n",
    "    # Sort the features by importance values in descending order\n",
    "    sorted_features = sorted(feature_importance_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Print the sorted features with their importance values\n",
    "    print(f\"Optimal number of features selected: {rfecv.n_features_}\")\n",
    "    print(f\"Selected feature names: {selected_feature_names}\")\n",
    "    print(f\"Selected features sorted by importance:\")\n",
    "    for feature, importance in sorted_features:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "    return X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Type: one_bed_flat\n",
      "Optimal number of features selected: 53\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_1_149_tot', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1000_1249_tot', 'hi_1250_1499_tot', 'hi_1500_1749_tot', 'hi_1750_1999_tot', 'hi_2000_2499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'erp', 'livability', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'forest', 'industrial_areas', 'park', 'residential_areas', 'retail_areas', 'cycleway', 'main_roads', 'motorway', 'walking_paths', 'roads_mode', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pofw_count', 'distance_to_fire_station', 'distance_to_hotel', 'distance_to_mall', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd', 'pets_allowed', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7248\n",
      "hi_4000_more_tot: 0.0702\n",
      "distance_to_hotel: 0.0096\n",
      "offence_division_4: 0.0088\n",
      "main_roads: 0.0082\n",
      "pets_allowed: 0.0080\n",
      "hi_1500_1749_tot: 0.0080\n",
      "offence_division_6: 0.0072\n",
      "industrial_areas: 0.0070\n",
      "offence_division_3: 0.0068\n",
      "livability: 0.0067\n",
      "retail_areas: 0.0065\n",
      "hi_3500_3999_tot: 0.0059\n",
      "hi_3000_3499_tot: 0.0058\n",
      "offence_division_2: 0.0053\n",
      "hi_150_299_tot: 0.0052\n",
      "erp: 0.0049\n",
      "hi_500_649_tot: 0.0049\n",
      "motorway: 0.0049\n",
      "avg_secondary_school_rank: 0.0049\n",
      "nearest_transport_avg_distance: 0.0048\n",
      "hi_1_149_tot: 0.0043\n",
      "walking_paths: 0.0040\n",
      "residential_areas: 0.0039\n",
      "offence_division_5: 0.0038\n",
      "tourism_and_attractions: 0.0037\n",
      "hi_300_399_tot: 0.0037\n",
      "offence_division_1: 0.0034\n",
      "hi_400_499_tot: 0.0034\n",
      "distance_to_mall: 0.0033\n",
      "accommodation: 0.0033\n",
      "hi_2500_2999_tot: 0.0028\n",
      "distance_to_supermarket: 0.0027\n",
      "financial_institutions: 0.0025\n",
      "hi_1750_1999_tot: 0.0024\n",
      "culture_and_leisure: 0.0024\n",
      "forest: 0.0024\n",
      "avg_primary_school_rank: 0.0023\n",
      "hi_800_999_tot: 0.0023\n",
      "hi_1000_1249_tot: 0.0022\n",
      "hi_2000_2499_tot: 0.0021\n",
      "roads_mode: 0.0021\n",
      "park: 0.0021\n",
      "hi_650_799_tot: 0.0021\n",
      "distance_to_fire_station: 0.0020\n",
      "distance_to_restaurant: 0.0017\n",
      "shopping_and_retail: 0.0017\n",
      "hi_1250_1499_tot: 0.0017\n",
      "healthcare: 0.0016\n",
      "public_facilities: 0.0015\n",
      "pofw_count: 0.0014\n",
      "cycleway: 0.0014\n",
      "pets_not_allowed: 0.0013\n",
      "Best n_estimators: 200, Best max_depth: 15\n",
      "Validation MSE: 4137.7890, R^2: 0.2752, Validation MAE: 47.5337\n",
      "Predictions for 2025-2027: [[469.36823864 462.9922022  457.86488582 486.12445121]\n",
      " [469.11637356 462.99201371 458.05747511 486.4978143 ]\n",
      " [469.05170094 463.05016847 458.1970882  486.60169823]\n",
      " ...\n",
      " [324.73117804 319.19423601 318.20728909 328.34437977]\n",
      " [327.80988638 322.30599791 321.27280695 331.7176625 ]\n",
      " [333.4504633  327.66304919 326.38129413 339.13922386]]\n",
      "\n",
      "\n",
      "Property Type: two_bed_flat\n",
      "Optimal number of features selected: 29\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_1750_1999_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'livability', 'avg_secondary_school_rank', 'industrial_areas', 'nature_reserve', 'residential_areas', 'main_roads', 'beach', 'accommodation', 'financial_institutions', 'food_and_beverage', 'distance_to_restaurant', 'nearest_transport_avg_distance', 'distance_to_cbd', 'unfurnished_count']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7763\n",
      "hi_4000_more_tot: 0.0458\n",
      "main_roads: 0.0120\n",
      "financial_institutions: 0.0112\n",
      "hi_3000_3499_tot: 0.0092\n",
      "residential_areas: 0.0091\n",
      "offence_division_6: 0.0087\n",
      "hi_3500_3999_tot: 0.0082\n",
      "offence_division_1: 0.0077\n",
      "offence_division_2: 0.0075\n",
      "hi_150_299_tot: 0.0069\n",
      "beach: 0.0068\n",
      "industrial_areas: 0.0068\n",
      "accommodation: 0.0064\n",
      "avg_secondary_school_rank: 0.0062\n",
      "offence_division_5: 0.0062\n",
      "hi_1750_1999_tot: 0.0059\n",
      "offence_division_4: 0.0057\n",
      "nearest_transport_avg_distance: 0.0056\n",
      "hi_2500_2999_tot: 0.0054\n",
      "distance_to_restaurant: 0.0053\n",
      "hi_300_399_tot: 0.0052\n",
      "food_and_beverage: 0.0051\n",
      "hi_500_649_tot: 0.0050\n",
      "offence_division_3: 0.0049\n",
      "livability: 0.0046\n",
      "hi_650_799_tot: 0.0044\n",
      "nature_reserve: 0.0041\n",
      "unfurnished_count: 0.0040\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 6236.8546, R^2: 0.2798, Validation MAE: 62.2536\n",
      "Predictions for 2025-2027: [[625.16   620.43   616.725  647.7125]\n",
      " [624.71   620.13   616.475  645.8875]\n",
      " [624.015  619.565  616.665  644.7125]\n",
      " ...\n",
      " [473.6    468.13   465.78   520.1225]\n",
      " [477.295  470.555  467.42   514.0275]\n",
      " [483.99   475.375  471.415  512.11  ]]\n",
      "\n",
      "\n",
      "Property Type: three_bed_flat\n",
      "Optimal number of features selected: 30\n",
      "Selected feature names: ['offence_division_1', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_800_999_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'livability', 'avg_secondary_school_rank', 'commercial_areas', 'industrial_areas', 'park', 'residential_areas', 'cycleway', 'main_roads', 'motorway', 'walking_paths', 'public_facilities', 'shopping_and_retail', 'distance_to_kindergarten', 'nearest_transport_avg_distance', 'distance_to_cbd', 'unfurnished_count', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7253\n",
      "hi_4000_more_tot: 0.0573\n",
      "offence_division_1: 0.0445\n",
      "nearest_transport_avg_distance: 0.0243\n",
      "cycleway: 0.0195\n",
      "offence_division_5: 0.0122\n",
      "motorway: 0.0108\n",
      "main_roads: 0.0088\n",
      "residential_areas: 0.0063\n",
      "offence_division_4: 0.0062\n",
      "unfurnished_count: 0.0056\n",
      "shopping_and_retail: 0.0055\n",
      "hi_400_499_tot: 0.0052\n",
      "walking_paths: 0.0051\n",
      "hi_300_399_tot: 0.0048\n",
      "offence_division_3: 0.0044\n",
      "commercial_areas: 0.0043\n",
      "livability: 0.0043\n",
      "hi_150_299_tot: 0.0042\n",
      "hi_800_999_tot: 0.0042\n",
      "hi_2500_2999_tot: 0.0042\n",
      "distance_to_kindergarten: 0.0041\n",
      "industrial_areas: 0.0041\n",
      "offence_division_6: 0.0039\n",
      "pets_not_allowed: 0.0039\n",
      "public_facilities: 0.0039\n",
      "avg_secondary_school_rank: 0.0038\n",
      "hi_3500_3999_tot: 0.0034\n",
      "hi_3000_3499_tot: 0.0031\n",
      "park: 0.0029\n",
      "Best n_estimators: 200, Best max_depth: 15\n",
      "Validation MSE: 14109.1360, R^2: 0.5590, Validation MAE: 84.4058\n",
      "Predictions for 2025-2027: [[ 935.15779527  922.41489971  912.14484145  987.30280862]\n",
      " [ 936.54187861  924.02264971  913.89475812  999.79468362]\n",
      " [ 934.48109289  924.73856043  915.88773431 1001.12669255]\n",
      " ...\n",
      " [ 880.02207107  869.07666089  850.48695761  916.85060047]\n",
      " [ 883.56263059  872.79611923  854.54677309  922.81307071]\n",
      " [ 887.45535877  876.58854383  859.81545761  932.17651317]]\n",
      "\n",
      "\n",
      "Property Type: two_bed_house\n",
      "Optimal number of features selected: 70\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_1_149_tot', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1000_1249_tot', 'hi_1250_1499_tot', 'hi_1500_1749_tot', 'hi_1750_1999_tot', 'hi_2000_2499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'erp', 'livability', 'primary_school_count', 'secondary_school_count', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'total_education_count', 'commercial_areas', 'forest', 'industrial_areas', 'park', 'residential_areas', 'retail_areas', 'landuse_mode', 'cycleway', 'main_roads', 'motorway', 'residential_roads', 'track', 'walking_paths', 'beach', 'peak', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'food_and_beverage', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pofw_count', 'distance_to_fire_station', 'distance_to_hospital', 'distance_to_hotel', 'distance_to_kindergarten', 'distance_to_library', 'distance_to_mall', 'distance_to_park', 'distance_to_police', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd', 'median_bath', 'median_parkings', 'unfurnished_count', 'pets_allowed', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.8161\n",
      "hi_4000_more_tot: 0.0440\n",
      "offence_division_1: 0.0154\n",
      "offence_division_5: 0.0139\n",
      "hi_1750_1999_tot: 0.0083\n",
      "nearest_transport_avg_distance: 0.0083\n",
      "residential_areas: 0.0057\n",
      "hi_3000_3499_tot: 0.0050\n",
      "offence_division_6: 0.0035\n",
      "distance_to_park: 0.0035\n",
      "main_roads: 0.0032\n",
      "hi_150_299_tot: 0.0027\n",
      "hi_650_799_tot: 0.0025\n",
      "walking_paths: 0.0024\n",
      "unfurnished_count: 0.0023\n",
      "hi_3500_3999_tot: 0.0021\n",
      "cycleway: 0.0021\n",
      "offence_division_4: 0.0020\n",
      "offence_division_3: 0.0020\n",
      "park: 0.0020\n",
      "livability: 0.0020\n",
      "pofw_count: 0.0019\n",
      "accommodation: 0.0019\n",
      "distance_to_restaurant: 0.0019\n",
      "avg_secondary_school_rank: 0.0018\n",
      "shopping_and_retail: 0.0018\n",
      "offence_division_2: 0.0018\n",
      "forest: 0.0018\n",
      "erp: 0.0017\n",
      "distance_to_supermarket: 0.0017\n",
      "hi_2500_2999_tot: 0.0017\n",
      "hi_300_399_tot: 0.0016\n",
      "hi_1_149_tot: 0.0016\n",
      "hi_800_999_tot: 0.0015\n",
      "culture_and_leisure: 0.0015\n",
      "pets_not_allowed: 0.0015\n",
      "hi_500_649_tot: 0.0014\n",
      "tourism_and_attractions: 0.0014\n",
      "landuse_mode: 0.0014\n",
      "hi_1000_1249_tot: 0.0013\n",
      "hi_2000_2499_tot: 0.0012\n",
      "beach: 0.0012\n",
      "distance_to_kindergarten: 0.0012\n",
      "secondary_school_count: 0.0011\n",
      "retail_areas: 0.0011\n",
      "hi_1500_1749_tot: 0.0011\n",
      "hi_400_499_tot: 0.0011\n",
      "hi_1250_1499_tot: 0.0010\n",
      "peak: 0.0010\n",
      "distance_to_library: 0.0008\n",
      "commercial_areas: 0.0008\n",
      "industrial_areas: 0.0007\n",
      "residential_roads: 0.0007\n",
      "distance_to_hotel: 0.0007\n",
      "public_facilities: 0.0006\n",
      "avg_primary_school_rank: 0.0006\n",
      "food_and_beverage: 0.0005\n",
      "distance_to_mall: 0.0005\n",
      "total_education_count: 0.0005\n",
      "healthcare: 0.0004\n",
      "financial_institutions: 0.0004\n",
      "distance_to_fire_station: 0.0004\n",
      "primary_school_count: 0.0004\n",
      "distance_to_hospital: 0.0003\n",
      "distance_to_police: 0.0003\n",
      "median_parkings: 0.0003\n",
      "track: 0.0003\n",
      "pets_allowed: 0.0002\n",
      "motorway: 0.0002\n",
      "median_bath: 0.0002\n",
      "Best n_estimators: 200, Best max_depth: 15\n",
      "Validation MSE: 5636.6139, R^2: 0.6097, Validation MAE: 57.0566\n",
      "Predictions for 2025-2027: [[688.095      682.895      679.31       713.5225    ]\n",
      " [690.885      682.225      678.515      722.9025    ]\n",
      " [693.33       685.185      681.255      731.075     ]\n",
      " ...\n",
      " [575.535      568.98666667 566.01166667 591.25      ]\n",
      " [578.075      571.365      568.21       594.675     ]\n",
      " [584.29       577.16       573.81       597.09      ]]\n",
      "\n",
      "\n",
      "Property Type: three_bed_house\n",
      "Optimal number of features selected: 16\n",
      "Selected feature names: ['offence_division_1', 'offence_division_5', 'hi_150_299_tot', 'hi_400_499_tot', 'hi_2500_2999_tot', 'hi_4000_more_tot', 'industrial_areas', 'park', 'residential_areas', 'cycleway', 'main_roads', 'beach', 'accommodation', 'food_and_beverage', 'nearest_transport_avg_distance', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.7795\n",
      "hi_4000_more_tot: 0.0652\n",
      "residential_areas: 0.0193\n",
      "main_roads: 0.0185\n",
      "accommodation: 0.0135\n",
      "nearest_transport_avg_distance: 0.0128\n",
      "offence_division_1: 0.0114\n",
      "hi_2500_2999_tot: 0.0112\n",
      "food_and_beverage: 0.0111\n",
      "beach: 0.0100\n",
      "cycleway: 0.0089\n",
      "park: 0.0088\n",
      "offence_division_5: 0.0077\n",
      "hi_400_499_tot: 0.0077\n",
      "hi_150_299_tot: 0.0072\n",
      "industrial_areas: 0.0072\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 7647.7539, R^2: 0.7545, Validation MAE: 66.8975\n",
      "Predictions for 2025-2027: [[ 854.105   848.165   849.005   974.7175]\n",
      " [ 873.94    866.965   866.445   986.4325]\n",
      " [ 878.93    871.77    870.315  1005.285 ]\n",
      " ...\n",
      " [ 695.02    681.21    677.965   730.655 ]\n",
      " [ 704.19    690.47    687.325   739.735 ]\n",
      " [ 714.155   700.265   695.94    748.705 ]]\n",
      "\n",
      "\n",
      "Property Type: four_bed_house\n",
      "Optimal number of features selected: 16\n",
      "Selected feature names: ['offence_division_1', 'offence_division_5', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_4000_more_tot', 'livability', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'park', 'residential_areas', 'cycleway', 'main_roads', 'accommodation', 'food_and_beverage', 'nearest_transport_avg_distance', 'distance_to_cbd']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.6901\n",
      "hi_4000_more_tot: 0.0634\n",
      "residential_areas: 0.0460\n",
      "food_and_beverage: 0.0239\n",
      "hi_300_399_tot: 0.0227\n",
      "nearest_transport_avg_distance: 0.0196\n",
      "accommodation: 0.0186\n",
      "offence_division_1: 0.0175\n",
      "cycleway: 0.0159\n",
      "avg_secondary_school_rank: 0.0145\n",
      "hi_400_499_tot: 0.0133\n",
      "offence_division_5: 0.0124\n",
      "avg_primary_school_rank: 0.0112\n",
      "park: 0.0109\n",
      "livability: 0.0101\n",
      "main_roads: 0.0098\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 15954.7057, R^2: 0.7716, Validation MAE: 93.1890\n",
      "Predictions for 2025-2027: [[1113.5    1098.15   1085.955  1117.6425]\n",
      " [1198.135  1183.595  1172.03   1242.255 ]\n",
      " [1192.745  1178.25   1169.945  1236.7425]\n",
      " ...\n",
      " [ 879.14    862.975   855.145   877.71  ]\n",
      " [ 881.94    866.475   858.48    879.735 ]\n",
      " [ 888.915   873.165   865.555   889.955 ]]\n",
      "\n",
      "\n",
      "Property Type: all_properties\n",
      "Optimal number of features selected: 77\n",
      "Selected feature names: ['offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'hi_1_149_tot', 'hi_150_299_tot', 'hi_300_399_tot', 'hi_400_499_tot', 'hi_500_649_tot', 'hi_650_799_tot', 'hi_800_999_tot', 'hi_1000_1249_tot', 'hi_1250_1499_tot', 'hi_1500_1749_tot', 'hi_1750_1999_tot', 'hi_2000_2499_tot', 'hi_2500_2999_tot', 'hi_3000_3499_tot', 'hi_3500_3999_tot', 'hi_4000_more_tot', 'erp', 'livability', 'primary_school_count', 'secondary_school_count', 'tertiary_institutions_count', 'avg_primary_school_rank', 'avg_secondary_school_rank', 'total_education_count', 'commercial_areas', 'farmland', 'forest', 'industrial_areas', 'nature_reserve', 'park', 'residential_areas', 'retail_areas', 'landuse_mode', 'cycleway', 'main_roads', 'motorway', 'residential_roads', 'track', 'walking_paths', 'roads_mode', 'beach', 'peak', 'nature_mode', 'accommodation', 'culture_and_leisure', 'financial_institutions', 'food_and_beverage', 'healthcare', 'public_facilities', 'shopping_and_retail', 'tourism_and_attractions', 'pois_mode', 'pofw_count', 'distance_to_fire_station', 'distance_to_hospital', 'distance_to_hotel', 'distance_to_kindergarten', 'distance_to_library', 'distance_to_mall', 'distance_to_park', 'distance_to_police', 'distance_to_restaurant', 'distance_to_supermarket', 'nearest_transport_avg_distance', 'distance_to_cbd', 'median_bath', 'median_parkings', 'furnished_count', 'unfurnished_count', 'pets_allowed', 'pets_not_allowed']\n",
      "Selected features sorted by importance:\n",
      "distance_to_cbd: 0.4134\n",
      "hi_4000_more_tot: 0.2166\n",
      "hi_650_799_tot: 0.0357\n",
      "residential_areas: 0.0185\n",
      "hi_800_999_tot: 0.0177\n",
      "beach: 0.0163\n",
      "distance_to_restaurant: 0.0135\n",
      "hi_3000_3499_tot: 0.0133\n",
      "hi_2500_2999_tot: 0.0128\n",
      "hi_1250_1499_tot: 0.0117\n",
      "main_roads: 0.0116\n",
      "offence_division_5: 0.0096\n",
      "distance_to_kindergarten: 0.0095\n",
      "hi_150_299_tot: 0.0084\n",
      "hi_300_399_tot: 0.0083\n",
      "hi_1_149_tot: 0.0082\n",
      "residential_roads: 0.0075\n",
      "distance_to_library: 0.0075\n",
      "walking_paths: 0.0069\n",
      "hi_400_499_tot: 0.0067\n",
      "offence_division_1: 0.0065\n",
      "hi_1500_1749_tot: 0.0065\n",
      "hi_500_649_tot: 0.0060\n",
      "offence_division_6: 0.0060\n",
      "offence_division_2: 0.0056\n",
      "hi_1000_1249_tot: 0.0053\n",
      "hi_3500_3999_tot: 0.0051\n",
      "offence_division_4: 0.0050\n",
      "culture_and_leisure: 0.0049\n",
      "erp: 0.0047\n",
      "nearest_transport_avg_distance: 0.0046\n",
      "distance_to_supermarket: 0.0045\n",
      "roads_mode: 0.0044\n",
      "primary_school_count: 0.0042\n",
      "livability: 0.0041\n",
      "cycleway: 0.0040\n",
      "distance_to_hotel: 0.0038\n",
      "hi_1750_1999_tot: 0.0034\n",
      "hi_2000_2499_tot: 0.0033\n",
      "tourism_and_attractions: 0.0028\n",
      "offence_division_3: 0.0026\n",
      "unfurnished_count: 0.0025\n",
      "distance_to_hospital: 0.0024\n",
      "avg_secondary_school_rank: 0.0024\n",
      "industrial_areas: 0.0023\n",
      "pets_not_allowed: 0.0022\n",
      "distance_to_park: 0.0022\n",
      "accommodation: 0.0021\n",
      "avg_primary_school_rank: 0.0020\n",
      "motorway: 0.0020\n",
      "park: 0.0020\n",
      "retail_areas: 0.0019\n",
      "total_education_count: 0.0019\n",
      "financial_institutions: 0.0018\n",
      "forest: 0.0017\n",
      "median_parkings: 0.0015\n",
      "furnished_count: 0.0013\n",
      "nature_mode: 0.0012\n",
      "track: 0.0012\n",
      "pofw_count: 0.0011\n",
      "pets_allowed: 0.0011\n",
      "food_and_beverage: 0.0011\n",
      "median_bath: 0.0011\n",
      "healthcare: 0.0010\n",
      "commercial_areas: 0.0009\n",
      "distance_to_police: 0.0009\n",
      "farmland: 0.0009\n",
      "shopping_and_retail: 0.0009\n",
      "public_facilities: 0.0008\n",
      "distance_to_mall: 0.0008\n",
      "secondary_school_count: 0.0007\n",
      "distance_to_fire_station: 0.0006\n",
      "landuse_mode: 0.0006\n",
      "nature_reserve: 0.0005\n",
      "tertiary_institutions_count: 0.0004\n",
      "peak: 0.0003\n",
      "pois_mode: 0.0002\n",
      "Best n_estimators: 100, Best max_depth: None\n",
      "Validation MSE: 7441.3117, R^2: 0.1833, Validation MAE: 66.9744\n",
      "Predictions for 2025-2027: [[601.15  591.85  589.57  657.45 ]\n",
      " [600.4   591.65  589.12  657.375]\n",
      " [611.59  602.83  600.37  671.85 ]\n",
      " ...\n",
      " [559.91  551.73  548.95  598.65 ]\n",
      " [565.5   555.6   552.05  604.975]\n",
      " [566.75  556.9   553.85  605.525]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialise the predictions dictionary\n",
    "predictions_dict = {}\n",
    "\n",
    "# Property Types\n",
    "property_types = ['one_bed_flat', 'two_bed_flat', 'three_bed_flat', \n",
    "                  'two_bed_house', 'three_bed_house', 'four_bed_house', 'all_properties']\n",
    "\n",
    "# Loop through each set, perform RFE, and predict using the tuned Random Forest model\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    print(f\"Property Type: {property_types[i]}\")\n",
    "    # Perform feature selection and get the best model\n",
    "    X_train_rfecv, X_val_rfecv, X_test_rfecv, best_rf, selected_feature_names = rfecv_with_random_forest(\n",
    "        X_train, X_val, X_test, y_train\n",
    "    )\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_rf.predict(X_val_rfecv)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f\"Best n_estimators: {best_rf.n_estimators}, Best max_depth: {best_rf.max_depth}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Combine the training and validation sets for final model training\n",
    "    X_train_val_rfecv = np.vstack((X_train_rfecv, X_val_rfecv))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Retrain the model using the combined training and validation sets\n",
    "    best_rf.fit(X_train_val_rfecv, y_train_val)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_rf.predict(X_test_rfecv)\n",
    "    \n",
    "    # Store predictions in the dictionary with dataset index as key\n",
    "    predictions_dict[f'X_test_{i+1}_predictions'] = y_test_pred\n",
    "    \n",
    "    # Print the predictions for the test set\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[469.36823864, 462.9922022 , 457.86488582, 486.12445121],\n",
       "       [469.11637356, 462.99201371, 458.05747511, 486.4978143 ],\n",
       "       [469.05170094, 463.05016847, 458.1970882 , 486.60169823],\n",
       "       ...,\n",
       "       [324.73117804, 319.19423601, 318.20728909, 328.34437977],\n",
       "       [327.80988638, 322.30599791, 321.27280695, 331.7176625 ],\n",
       "       [333.4504633 , 327.66304919, 326.38129413, 339.13922386]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add predictions back to the training data \n",
    "predictions_dict['X_test_1_predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved one_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_flat with 'year', 'suburb', and predictions.\n",
      "Saved two_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved three_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved four_bed_house with 'year', 'suburb', and predictions.\n",
      "Saved all_properties with 'year', 'suburb', and predictions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prediction Column Names\n",
    "column_names = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# labelled_dfs corresponds to the datasets with labels for each property type\n",
    "labelled_dfs = [X_test_one_bed_labels, X_test_two_bed_labels, X_test_three_bed_labels, \n",
    "                X_test_two_bed_house_labels, X_test_three_bed_house_labels, \n",
    "                X_test_four_bed_house_labels, X_test_all_properties_labels]\n",
    "\n",
    "# Iterate through the predictions dictionary and labelled DataFrames\n",
    "for i, (key, value) in enumerate(predictions_dict.items()):\n",
    "    # Select only the 'year' and 'suburb' columns from the labelled_dfs\n",
    "    labelled_df_subset = labelled_dfs[i][['suburb', 'year']]\n",
    "\n",
    "    # Initialise an empty DataFrame for the predictions\n",
    "    predictions_df = pd.DataFrame()\n",
    "\n",
    "    # value is a 2D array with multiple rows and 4 columns (n_samples, 4)\n",
    "    # Add each column using the custom names for the median values\n",
    "    for j in range(value.shape[1]):\n",
    "        predictions_df[column_names[j]] = value[:, j]\n",
    "        #print(labelled_df_subset.shape)\n",
    "\n",
    "    # Reset the index for both DataFrames to ensure proper alignment\n",
    "    labelled_df_subset = labelled_df_subset.reset_index(drop=True)\n",
    "    predictions_df = predictions_df.reset_index(drop=True)\n",
    "    \n",
    "    # Concatenate the 'year' and 'suburb' columns with the predictions_df\n",
    "    labelled_with_predictions = pd.concat([labelled_df_subset, predictions_df], axis=1)\n",
    "\n",
    "    # Save the new DataFrame with only 'year', 'suburb', and predictions to a CSV file\n",
    "    labelled_with_predictions.to_csv(f\"../data/curated/predictions/{property_types[i]}_predictions.csv\", index=False)\n",
    "\n",
    "    # Print confirmation (optional)\n",
    "    print(f\"Saved {property_types[i]} with 'year', 'suburb', and predictions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
