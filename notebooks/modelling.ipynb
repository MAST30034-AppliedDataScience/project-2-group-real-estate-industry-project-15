{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_df(df):\n",
    "    \"\"\"\n",
    "    This function cleans the domain dataframes by removing\n",
    "    the 'Unnamed:' column, renaming median_rent to 'sep_median'\n",
    "    (for a standardised column name as in rental history dfs) and\n",
    "    also creates a year column and inputs the relevant year that\n",
    "    the data is from - 2024. \n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'sep_median'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the clean_domain_df function to all the domain dataframes\n",
    "domain_one_bed_flat = clean_domain_df(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_df(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_df(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_df(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_df(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_df(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_df(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties into the rental history dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_sep_2024_rental_data(rental_history_df, domain_df):\n",
    "    \"\"\"\n",
    "    This function retrieves all the median rental prices in \n",
    "    September from the domain dataframes and then imputes\n",
    "    them into the rental history dataframes where the year\n",
    "    is 2024 and month is September. \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge rental_history_df with domain_df on 'suburb' to keep all years from rental_history_df\n",
    "    merged_df = pd.merge(rental_history_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "\n",
    "    # Filter the dataframe to keep only the suburbs that appear 9 or more times\n",
    "    suburb_counts = merged_df['suburb'].value_counts()\n",
    "    suburbs_to_keep = suburb_counts[suburb_counts >= 9].index\n",
    "    merged_df = merged_df[merged_df['suburb'].isin(suburbs_to_keep)]\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Apply the function to each dataset \n",
    "one_bed_flat, domain_one_bed_flat = impute_sep_2024_rental_data(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_sep_2024_rental_data(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_sep_2024_rental_data(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_sep_2024_rental_data(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_sep_2024_rental_data(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_sep_2024_rental_data(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_sep_2024_rental_data(all_properties, domain_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suburb</th>\n",
       "      <th>year</th>\n",
       "      <th>dec_median</th>\n",
       "      <th>jun_median</th>\n",
       "      <th>mar_median</th>\n",
       "      <th>sep_median</th>\n",
       "      <th>offence_division_1</th>\n",
       "      <th>offence_division_2</th>\n",
       "      <th>offence_division_3</th>\n",
       "      <th>offence_division_4</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_to_restaurant</th>\n",
       "      <th>distance_to_supermarket</th>\n",
       "      <th>nearest_transport_avg_distance</th>\n",
       "      <th>distance_to_cbd</th>\n",
       "      <th>median_bath</th>\n",
       "      <th>median_parkings</th>\n",
       "      <th>furnished_count</th>\n",
       "      <th>unfurnished_count</th>\n",
       "      <th>pets_allowed</th>\n",
       "      <th>pets_not_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2016</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2017</td>\n",
       "      <td>400.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2018</td>\n",
       "      <td>410.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2019</td>\n",
       "      <td>420.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2020</td>\n",
       "      <td>390.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2023</td>\n",
       "      <td>295.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2024</td>\n",
       "      <td>325.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1584 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          suburb  year  dec_median  jun_median  mar_median  sep_median  \\\n",
       "0     abbotsford  2016       380.0       380.0       380.0       380.0   \n",
       "1     abbotsford  2017       400.0       390.0       390.0       395.0   \n",
       "2     abbotsford  2018       410.0       400.0       400.0       400.0   \n",
       "3     abbotsford  2019       420.0       420.0       410.0       420.0   \n",
       "4     abbotsford  2020       390.0       418.0       420.0       410.0   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "1615  yarraville  2023       295.0       270.0       275.0       280.0   \n",
       "1616  yarraville  2024       325.0       325.0       325.0       320.0   \n",
       "1617  yarraville  2025         NaN         NaN         NaN         NaN   \n",
       "1618  yarraville  2026         NaN         NaN         NaN         NaN   \n",
       "1619  yarraville  2027         NaN         NaN         NaN         NaN   \n",
       "\n",
       "      offence_division_1  offence_division_2  offence_division_3  \\\n",
       "0                  107.0              1065.0                76.0   \n",
       "1                  138.0              1019.0                64.0   \n",
       "2                  100.0              1162.0                88.0   \n",
       "3                  175.0              1053.0               178.0   \n",
       "4                  145.0               985.0               151.0   \n",
       "...                  ...                 ...                 ...   \n",
       "1615                96.0               633.0                68.0   \n",
       "1616               101.0               607.0                60.0   \n",
       "1617               102.0               604.0                63.0   \n",
       "1618               103.0               600.0                65.0   \n",
       "1619               104.0               597.0                68.0   \n",
       "\n",
       "      offence_division_4  ...  distance_to_restaurant  \\\n",
       "0                   59.0  ...                1.141601   \n",
       "1                   69.0  ...                1.141601   \n",
       "2                   84.0  ...                1.141601   \n",
       "3                  114.0  ...                1.141601   \n",
       "4                   89.0  ...                1.141601   \n",
       "...                  ...  ...                     ...   \n",
       "1615                31.0  ...                1.032864   \n",
       "1616                40.0  ...                1.032864   \n",
       "1617                40.0  ...                1.032864   \n",
       "1618                39.0  ...                1.032864   \n",
       "1619                38.0  ...                1.032864   \n",
       "\n",
       "      distance_to_supermarket  nearest_transport_avg_distance  \\\n",
       "0                    1.083238                        1.110000   \n",
       "1                    1.083238                        1.110000   \n",
       "2                    1.083238                        1.110000   \n",
       "3                    1.083238                        1.110000   \n",
       "4                    1.083238                        1.110000   \n",
       "...                       ...                             ...   \n",
       "1615                 0.937960                        1.826667   \n",
       "1616                 0.937960                        1.826667   \n",
       "1617                 0.937960                        1.826667   \n",
       "1618                 0.937960                        1.826667   \n",
       "1619                 0.937960                        1.826667   \n",
       "\n",
       "      distance_to_cbd  median_bath  median_parkings  furnished_count  \\\n",
       "0                5.55            1                0                2   \n",
       "1                5.55            1                0                2   \n",
       "2                5.55            1                0                2   \n",
       "3                5.55            1                0                2   \n",
       "4                5.55            1                0                2   \n",
       "...               ...          ...              ...              ...   \n",
       "1615            10.21            1                1                0   \n",
       "1616            10.21            1                1                0   \n",
       "1617            10.21            1                1                0   \n",
       "1618            10.21            1                1                0   \n",
       "1619            10.21            1                1                0   \n",
       "\n",
       "      unfurnished_count  pets_allowed  pets_not_allowed  \n",
       "0                    10             4                 8  \n",
       "1                    10             4                 8  \n",
       "2                    10             4                 8  \n",
       "3                    10             4                 8  \n",
       "4                    10             4                 8  \n",
       "...                 ...           ...               ...  \n",
       "1615                  1             0                 1  \n",
       "1616                  1             0                 1  \n",
       "1617                  1             0                 1  \n",
       "1618                  1             0                 1  \n",
       "1619                  1             0                 1  \n",
       "\n",
       "[1584 rows x 89 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "one_bed_flat_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_suburb_cols(df):\n",
    "    \"\"\"\n",
    "    This function removes all duplicated of the suburb column name \n",
    "    from the merged dataframes. The duplicate suburb column name \n",
    "    could be 'Unnamed', 'sa2_name' or 'gazetted_locality'.\n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_suburb_cols(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_suburb_cols(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_suburb_cols(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_suburb_cols(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_suburb_cols(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_suburb_cols(four_bed_house_merged)\n",
    "all_properties_merged = clean_suburb_cols(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_before_2025(df, median_columns):\n",
    "    # Drop rows where year is less than 2025 and NaN values exist in any of the median columns\n",
    "    return df[~((df['year'] < 2025) & (df[median_columns].isnull().any(axis=1)))]\n",
    "\n",
    "# Define the median columns to check\n",
    "median_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# Call the function for each dataframe and reassign the cleaned data\n",
    "one_bed_flat_merged = remove_nan_before_2025(one_bed_flat_merged, median_columns)\n",
    "two_bed_flat_merged = remove_nan_before_2025(two_bed_flat_merged, median_columns)\n",
    "three_bed_flat_merged = remove_nan_before_2025(three_bed_flat_merged, median_columns)\n",
    "two_bed_house_merged = remove_nan_before_2025(two_bed_house_merged, median_columns)\n",
    "three_bed_house_merged = remove_nan_before_2025(three_bed_house_merged, median_columns)\n",
    "four_bed_house_merged = remove_nan_before_2025(four_bed_house_merged, median_columns)\n",
    "all_properties_merged = remove_nan_before_2025(all_properties_merged, median_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Properties Dataframe for Visualisation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_merged_dataframes():\n",
    "    # Define the base path\n",
    "    base_path = '../data/curated/merged_feature_set'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(base_path):\n",
    "        os.makedirs(base_path)\n",
    "\n",
    "    # Save each dataframe to a CSV file\n",
    "    one_bed_flat_merged.to_csv(os.path.join(base_path, 'one_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_flat_merged.to_csv(os.path.join(base_path, 'two_bed_flat_merged.csv'), index=False)\n",
    "    three_bed_flat_merged.to_csv(os.path.join(base_path, 'three_bed_flat_merged.csv'), index=False)\n",
    "    two_bed_house_merged.to_csv(os.path.join(base_path, 'two_bed_house_merged.csv'), index=False)\n",
    "    three_bed_house_merged.to_csv(os.path.join(base_path, 'three_bed_house_merged.csv'), index=False)\n",
    "    four_bed_house_merged.to_csv(os.path.join(base_path, 'four_bed_house_merged.csv'), index=False)\n",
    "    all_properties_merged.to_csv(os.path.join(base_path, 'all_properties_merged.csv'), index=False)\n",
    "\n",
    "# Call the function\n",
    "save_merged_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suburb</th>\n",
       "      <th>year</th>\n",
       "      <th>offence_division_1</th>\n",
       "      <th>offence_division_2</th>\n",
       "      <th>offence_division_3</th>\n",
       "      <th>offence_division_4</th>\n",
       "      <th>offence_division_5</th>\n",
       "      <th>offence_division_6</th>\n",
       "      <th>erp</th>\n",
       "      <th>hi_1_149_tot</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_to_restaurant</th>\n",
       "      <th>distance_to_supermarket</th>\n",
       "      <th>nearest_transport_avg_distance</th>\n",
       "      <th>distance_to_cbd</th>\n",
       "      <th>median_bath</th>\n",
       "      <th>median_parkings</th>\n",
       "      <th>furnished_count</th>\n",
       "      <th>unfurnished_count</th>\n",
       "      <th>pets_allowed</th>\n",
       "      <th>pets_not_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6264</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2016</td>\n",
       "      <td>95.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10483.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2016</td>\n",
       "      <td>95.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10483.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6312</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2017</td>\n",
       "      <td>102.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10556.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6313</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2017</td>\n",
       "      <td>102.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10556.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6360</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2018</td>\n",
       "      <td>41.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10607.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2018</td>\n",
       "      <td>41.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10607.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2019</td>\n",
       "      <td>53.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10624.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2019</td>\n",
       "      <td>53.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10624.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6456</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2020</td>\n",
       "      <td>83.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10555.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6457</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2020</td>\n",
       "      <td>83.0</td>\n",
       "      <td>351.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10555.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2021</td>\n",
       "      <td>77.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10408.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>3.586667</td>\n",
       "      <td>78.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>newtown</td>\n",
       "      <td>2021</td>\n",
       "      <td>77.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10408.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.322679</td>\n",
       "      <td>1.081909</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>142.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       suburb  year  offence_division_1  offence_division_2  \\\n",
       "6264  newtown  2016                95.0               566.0   \n",
       "6265  newtown  2016                95.0               566.0   \n",
       "6312  newtown  2017               102.0               514.0   \n",
       "6313  newtown  2017               102.0               514.0   \n",
       "6360  newtown  2018                41.0               318.0   \n",
       "6361  newtown  2018                41.0               318.0   \n",
       "6408  newtown  2019                53.0               356.0   \n",
       "6409  newtown  2019                53.0               356.0   \n",
       "6456  newtown  2020                83.0               351.0   \n",
       "6457  newtown  2020                83.0               351.0   \n",
       "6504  newtown  2021                77.0               282.0   \n",
       "6505  newtown  2021                77.0               282.0   \n",
       "\n",
       "      offence_division_3  offence_division_4  offence_division_5  \\\n",
       "6264                22.0                29.0                38.0   \n",
       "6265                22.0                29.0                38.0   \n",
       "6312                28.0                19.0                92.0   \n",
       "6313                28.0                19.0                92.0   \n",
       "6360                34.0                16.0                35.0   \n",
       "6361                34.0                16.0                35.0   \n",
       "6408                35.0                21.0                27.0   \n",
       "6409                35.0                21.0                27.0   \n",
       "6456                38.0                25.0               101.0   \n",
       "6457                38.0                25.0               101.0   \n",
       "6504                22.0                25.0                65.0   \n",
       "6505                22.0                25.0                65.0   \n",
       "\n",
       "      offence_division_6      erp  hi_1_149_tot  ...  distance_to_restaurant  \\\n",
       "6264                 0.0  10483.0          17.0  ...                1.322679   \n",
       "6265                 0.0  10483.0          17.0  ...                1.322679   \n",
       "6312                 0.0  10556.0          21.0  ...                1.322679   \n",
       "6313                 0.0  10556.0          21.0  ...                1.322679   \n",
       "6360                 0.0  10607.0          25.0  ...                1.322679   \n",
       "6361                 0.0  10607.0          25.0  ...                1.322679   \n",
       "6408                 2.0  10624.0          30.0  ...                1.322679   \n",
       "6409                 2.0  10624.0          30.0  ...                1.322679   \n",
       "6456                17.0  10555.0          34.0  ...                1.322679   \n",
       "6457                17.0  10555.0          34.0  ...                1.322679   \n",
       "6504                 3.0  10408.0          39.0  ...                1.322679   \n",
       "6505                 3.0  10408.0          39.0  ...                1.322679   \n",
       "\n",
       "      distance_to_supermarket  nearest_transport_avg_distance  \\\n",
       "6264                 1.081909                        3.586667   \n",
       "6265                 1.081909                       27.500000   \n",
       "6312                 1.081909                        3.586667   \n",
       "6313                 1.081909                       27.500000   \n",
       "6360                 1.081909                        3.586667   \n",
       "6361                 1.081909                       27.500000   \n",
       "6408                 1.081909                        3.586667   \n",
       "6409                 1.081909                       27.500000   \n",
       "6456                 1.081909                        3.586667   \n",
       "6457                 1.081909                       27.500000   \n",
       "6504                 1.081909                        3.586667   \n",
       "6505                 1.081909                       27.500000   \n",
       "\n",
       "      distance_to_cbd  median_bath  median_parkings  furnished_count  \\\n",
       "6264            78.02            1                0                0   \n",
       "6265           142.83            1                0                0   \n",
       "6312            78.02            1                0                0   \n",
       "6313           142.83            1                0                0   \n",
       "6360            78.02            1                0                0   \n",
       "6361           142.83            1                0                0   \n",
       "6408            78.02            1                0                0   \n",
       "6409           142.83            1                0                0   \n",
       "6456            78.02            1                0                0   \n",
       "6457           142.83            1                0                0   \n",
       "6504            78.02            1                0                0   \n",
       "6505           142.83            1                0                0   \n",
       "\n",
       "      unfurnished_count  pets_allowed  pets_not_allowed  \n",
       "6264                  2             0                 2  \n",
       "6265                  2             0                 2  \n",
       "6312                  2             0                 2  \n",
       "6313                  2             0                 2  \n",
       "6360                  2             0                 2  \n",
       "6361                  2             0                 2  \n",
       "6408                  2             0                 2  \n",
       "6409                  2             0                 2  \n",
       "6456                  2             0                 2  \n",
       "6457                  2             0                 2  \n",
       "6504                  2             0                 2  \n",
       "6505                  2             0                 2  \n",
       "\n",
       "[12 rows x 83 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    This function first splits the dataframe into training, validation, \n",
    "    and testing sets based on the 'year' column:\n",
    "    - Training set includes data from the years 2016-2021.\n",
    "    - Validation set includes data from the years 2022-2024.\n",
    "    - Testing set includes data from the years 2025-2027.\n",
    "\n",
    "    It then merges additional columns that are not part of the \n",
    "    features specific to years or target columns back with the \n",
    "    respective sets based on matching suburbs.\n",
    "\n",
    "    The function then returns the follow dataframes:\n",
    "    - X_train: Training feature set.\n",
    "    - X_val: Validation feature set.\n",
    "    - X_test: Testing feature set.\n",
    "    - y_train: Training target set.\n",
    "    - y_val: Validation target set.\n",
    "    - y_test: Testing target set.\n",
    "    \"\"\"\n",
    "    # Define the year ranges for training, validation, and testing sets\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Columns to include in X (specific to the years) and y splits\n",
    "    feature_year_cols = ['suburb', 'year', 'offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'erp']\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Add the income columns that start with 'hi_' and end with '_tot' using regex\n",
    "    regex_pattern = r'^hi_.*_tot$'\n",
    "    hi_tot_cols = df.filter(regex=regex_pattern).columns.tolist()\n",
    "    # Combine to the feature_year_cols\n",
    "    feature_year_cols += hi_tot_cols\n",
    "\n",
    "    # Split features (specific to the years) and target data\n",
    "    X = df[feature_year_cols]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Split the dataframe into training, validation, and testing sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    # Align target sets with the corresponding feature sets\n",
    "    y_train = y.loc[X_train.index]\n",
    "    y_val = y.loc[X_val.index]\n",
    "    y_test = y.loc[X_test.index]\n",
    "\n",
    "    # Extract other columns not in feature_year_cols or target_columns, including 'suburb'\n",
    "    other_columns = df.drop(columns=feature_year_cols + target_columns).columns\n",
    "    other_data = df[other_columns].copy()\n",
    "    other_data['suburb'] = df['suburb']  # Ensure 'suburb' is included\n",
    "\n",
    "    # Merge the 'other' data back with the matching suburbs, irrespective of the year\n",
    "    X_train = X_train.merge(other_data, on='suburb', how='left')\n",
    "    X_val = X_val.merge(other_data, on='suburb', how='left')\n",
    "    X_test = X_test.merge(other_data, on='suburb', how='left')\n",
    "\n",
    "    # Drop duplicates from the X dfs\n",
    "    X_train = X_train.drop_duplicates()\n",
    "    X_val = X_val.drop_duplicates()\n",
    "    X_test = X_test.drop_duplicates()\n",
    "\n",
    "    # Now drop 'suburb' and 'year' from the feature sets\n",
    "    #X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = train_val_test_sets(one_bed_flat_merged)\n",
    "# X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = train_val_test_sets(two_bed_flat_merged)\n",
    "# X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = train_val_test_sets(three_bed_flat_merged)\n",
    "# X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = train_val_test_sets(two_bed_house_merged)\n",
    "# X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = train_val_test_sets(three_bed_house_merged)\n",
    "# X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = train_val_test_sets(four_bed_house_merged)\n",
    "# X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = train_val_test_sets(all_properties_merged)\n",
    "\n",
    "\n",
    "#print(X_train_one_bed['suburb'].value_counts())\n",
    "X_train_one_bed[X_train_one_bed['suburb'] == 'newtown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_feature_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    This function compares columns of the training, validation, and \n",
    "    testing feature dataframes (X). A dictionary is then returned\n",
    "    indicating if any columns are missing in each set or if all\n",
    "    the colums are the same. \n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "feature_dfs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_feature_columns(train, val, test) for name, (train, val, test) in feature_dfs.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1606         NaN         NaN         NaN         NaN\n",
      "1607         NaN         NaN         NaN         NaN\n",
      "1617         NaN         NaN         NaN         NaN\n",
      "1618         NaN         NaN         NaN         NaN\n",
      "1619         NaN         NaN         NaN         NaN\n",
      "\n",
      "[396 rows x 4 columns], 'y_test_two_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2038         NaN         NaN         NaN         NaN\n",
      "2039         NaN         NaN         NaN         NaN\n",
      "2049         NaN         NaN         NaN         NaN\n",
      "2050         NaN         NaN         NaN         NaN\n",
      "2051         NaN         NaN         NaN         NaN\n",
      "\n",
      "[504 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1486         NaN         NaN         NaN         NaN\n",
      "1487         NaN         NaN         NaN         NaN\n",
      "1497         NaN         NaN         NaN         NaN\n",
      "1498         NaN         NaN         NaN         NaN\n",
      "1499         NaN         NaN         NaN         NaN\n",
      "\n",
      "[372 rows x 4 columns], 'y_test_two_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1918         NaN         NaN         NaN         NaN\n",
      "1919         NaN         NaN         NaN         NaN\n",
      "1929         NaN         NaN         NaN         NaN\n",
      "1930         NaN         NaN         NaN         NaN\n",
      "1931         NaN         NaN         NaN         NaN\n",
      "\n",
      "[477 rows x 4 columns], 'y_test_three_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2362         NaN         NaN         NaN         NaN\n",
      "2363         NaN         NaN         NaN         NaN\n",
      "2373         NaN         NaN         NaN         NaN\n",
      "2374         NaN         NaN         NaN         NaN\n",
      "2375         NaN         NaN         NaN         NaN\n",
      "\n",
      "[588 rows x 4 columns], 'y_test_four_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2050         NaN         NaN         NaN         NaN\n",
      "2051         NaN         NaN         NaN         NaN\n",
      "2061         NaN         NaN         NaN         NaN\n",
      "2062         NaN         NaN         NaN         NaN\n",
      "2063         NaN         NaN         NaN         NaN\n",
      "\n",
      "[513 rows x 4 columns], 'y_test_all_properties':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2398         NaN         NaN         NaN         NaN\n",
      "2399         NaN         NaN         NaN         NaN\n",
      "2409         NaN         NaN         NaN         NaN\n",
      "2410         NaN         NaN         NaN         NaN\n",
      "2411         NaN         NaN         NaN         NaN\n",
      "\n",
      "[594 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    396\n",
      "jun_median    396\n",
      "mar_median    396\n",
      "sep_median    396\n",
      "dtype: int64, 'y_test_two_bed': dec_median    504\n",
      "jun_median    504\n",
      "mar_median    504\n",
      "sep_median    504\n",
      "dtype: int64, 'y_test_three_bed': dec_median    372\n",
      "jun_median    372\n",
      "mar_median    372\n",
      "sep_median    372\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    477\n",
      "jun_median    477\n",
      "mar_median    477\n",
      "sep_median    477\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    588\n",
      "jun_median    588\n",
      "mar_median    588\n",
      "sep_median    588\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    513\n",
      "jun_median    513\n",
      "mar_median    513\n",
      "sep_median    513\n",
      "dtype: int64, 'y_test_all_properties': dec_median    594\n",
      "jun_median    594\n",
      "mar_median    594\n",
      "sep_median    594\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe by columns \n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 25\n",
      "Explained variance by selected components: 0.91\n",
      "Number of components selected: 27\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 26\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 29\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 31\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 30\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 28\n",
      "Explained variance by selected components: 0.90\n"
     ]
    }
   ],
   "source": [
    "def pca_feature_selection(X_train, X_val, X_test, variance_threshold=0.9):\n",
    "    \"\"\"\n",
    "    The function applies PCA for dimensionality reduction by fitting on the\n",
    "    training set and transforming both the training and test sets. It keeps \n",
    "    a select number of components based on the defined variance \n",
    "    threshold. It returns the reduced training and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialise PCA, specifying the variance threshold\n",
    "    pca_temp = PCA().fit(X_train_scaled)\n",
    "    cumulative_variance = pca_temp.explained_variance_ratio_.cumsum()\n",
    "    # Find the number of components to capture the specified variance\n",
    "    n_components = next(i for i, total_variance in enumerate(cumulative_variance) if total_variance >= variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the training set and transform both training and test sets\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Number of components selected: {n_components}\")\n",
    "    print(f\"Explained variance by selected components: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    return X_train_pca, X_val_pca, X_test_pca\n",
    "\n",
    "# Perform feature selection with PCA on the X sets \n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed = pca_feature_selection(X_train_one_bed, X_val_one_bed, X_test_one_bed)\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed = pca_feature_selection(X_train_two_bed, X_val_two_bed, X_test_two_bed)\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed = pca_feature_selection(X_train_three_bed, X_val_three_bed, X_test_three_bed)\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house = pca_feature_selection(X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house)\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house = pca_feature_selection(X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house)\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house = pca_feature_selection(X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house)\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties = pca_feature_selection(X_train_all_properties, X_val_all_properties, X_test_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of all training, validation, and test sets\n",
    "ML_dfs = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Property Type 1:\n",
      "Best alpha: 14.384498882876601\n",
      "Validation MSE: 5009.7990, R^2: 0.1062, Validation MAE: 50.9746\n",
      "Predictions for 2025-2027: [[337.10077593 331.54180397 329.00791526 339.8711702 ]\n",
      " [338.18319765 332.60808884 330.0788589  341.04192229]\n",
      " [339.2812386  333.68968958 331.16527815 342.22898184]\n",
      " ...\n",
      " [348.031225   342.1558391  339.86347286 350.87026203]\n",
      " [349.9684035  344.0629259  341.79322604 352.95292375]\n",
      " [351.83256074 345.89876941 343.64991829 354.95118617]]\n",
      "\n",
      "Property Type 2:\n",
      "Best alpha: 14.384498882876601\n",
      "Validation MSE: 7408.4227, R^2: 0.1357, Validation MAE: 64.4800\n",
      "Predictions for 2025-2027: [[465.10747333 456.92593822 453.03895957 466.95530329]\n",
      " [466.43387411 458.20598785 454.30779374 468.35047402]\n",
      " [467.78071733 459.50585351 455.59631322 469.76729147]\n",
      " ...\n",
      " [466.62275067 458.19188075 454.51445258 468.35157212]\n",
      " [469.46867924 460.96294854 457.25958088 471.35687966]\n",
      " [472.21044027 463.63254342 459.90379657 474.25045943]]\n",
      "\n",
      "Property Type 3:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 16172.9058, R^2: 0.4786, Validation MAE: 87.6844\n",
      "Predictions for 2025-2027: [[698.68388312 692.39298795 687.11633003 709.38291541]\n",
      " [702.00017115 695.91937619 690.77304045 713.48503781]\n",
      " [705.36136663 699.49594737 694.4813606  717.64225444]\n",
      " ...\n",
      " [696.9528893  682.14563484 676.37470769 713.27227015]\n",
      " [701.7417441  687.31321412 681.74247648 719.14149522]\n",
      " [706.46285148 692.41172536 687.04102149 724.93800418]]\n",
      "\n",
      "Property Type 4:\n",
      "Best alpha: 5.455594781168514\n",
      "Validation MSE: 7772.7384, R^2: 0.4460, Validation MAE: 65.3768\n",
      "Predictions for 2025-2027: [[522.64182636 515.24427528 510.69992669 530.18037573]\n",
      " [524.89219522 517.50651412 512.93211125 532.90997733]\n",
      " [527.17438382 519.80241039 515.19766702 535.68141411]\n",
      " ...\n",
      " [561.27432007 552.73771913 547.75576754 568.36997308]\n",
      " [567.7413726  559.10094477 554.01894639 575.31727988]\n",
      " [574.10267896 565.35970186 560.18199495 582.12643419]]\n",
      "\n",
      "Property Type 5:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 14286.3811, R^2: 0.5296, Validation MAE: 87.1446\n",
      "Predictions for 2025-2027: [[670.74171214 661.37647988 656.56370322 675.45791881]\n",
      " [674.90693481 665.5997805  660.8502929  680.27859808]\n",
      " [679.06366283 669.81918554 665.13539803 685.10627528]\n",
      " ...\n",
      " [678.02082553 663.44458681 660.0329065  677.00092545]\n",
      " [689.04237423 674.1972237  670.75121441 688.62847117]\n",
      " [699.96239403 684.85836586 681.37639967 700.12028685]]\n",
      "\n",
      "Property Type 6:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 29121.6526, R^2: 0.5752, Validation MAE: 120.6593\n",
      "Predictions for 2025-2027: [[723.25536461 711.13986097 705.70103551 740.54758445]\n",
      " [729.82846332 717.69735379 712.23240482 747.91495618]\n",
      " [736.39742902 724.25626698 718.76681616 755.29233697]\n",
      " ...\n",
      " [849.76787452 833.47442215 827.54747822 850.30350796]\n",
      " [867.56709238 850.85703655 844.8064383  869.05915822]\n",
      " [885.26552816 868.14234628 861.96714105 887.65272765]]\n",
      "\n",
      "Property Type 7:\n",
      "Best alpha: 0.7847599703514607\n",
      "Validation MSE: 8610.2347, R^2: 0.0354, Validation MAE: 71.3527\n",
      "Predictions for 2025-2027: [[493.30082285 485.1103755  482.64947213 499.01753208]\n",
      " [495.67883617 487.61588814 485.17984304 502.11417894]\n",
      " [498.08474378 490.15445919 487.74463007 505.25178493]\n",
      " ...\n",
      " [516.91318943 510.09913805 506.22118369 525.71863113]\n",
      " [523.33806039 516.42556983 512.49159609 533.14053987]\n",
      " [529.56050753 522.54796265 518.55803523 540.31700279]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for Lasso\n",
    "alpha_range = np.logspace(-6, 2, 20) \n",
    "param_grid = {'alpha': alpha_range}\n",
    "\n",
    "# Loop through each set, tune the model on the validation set, and predict on the test set\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    # Initialise the Lasso model\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    # Set up the GridSearchCV to tune the 'alpha' hyperparameter\n",
    "    grid_search = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Train the model using the training set and validate on the validation set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Select the best model based on the validation set\n",
    "    best_lasso = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_lasso.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f\"Property Type {i+1}:\")\n",
    "    print(f\"Best alpha: {best_lasso.alpha}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Combine the training and validation sets for final model training\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "\n",
    "    # Retrain the model using the combined training and validation sets\n",
    "    best_lasso.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_lasso.predict(X_test)\n",
    "    \n",
    "    # Print the predictions for the test set\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Best n_estimators: 100, Best max_depth: None\n",
      "Validation MSE: 4239.1565, R^2: 0.2525, Validation MAE: 46.1966\n",
      "Predictions for 2025-2027: [[421.17  411.59  404.93  426.87 ]\n",
      " [420.39  410.82  403.48  423.24 ]\n",
      " [393.99  384.02  376.85  392.095]\n",
      " ...\n",
      " [336.06  332.37  330.71  334.93 ]\n",
      " [344.52  340.58  339.12  346.035]\n",
      " [368.31  361.58  359.24  373.585]]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the model using the training set and validate on the validation set\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Select the best model based on the validation set\u001b[39;00m\n\u001b[1;32m     24\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1019\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1014\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1019\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1573\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/model_selection/_search.py:965\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    961\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m     )\n\u001b[0;32m--> 965\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    988\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Desktop Documents/Unimelb 2024/Sem 2/Applied Data Science/GitHub/project-2-group-real-estate-industry-project-15/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Loop through each set, tune the model on the validation set, and predict on the test set\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    # Initialise the Random Forest model\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Set up the GridSearchCV to tune 'n_estimators' and 'max_depth'\n",
    "    grid_search = GridSearchCV(rf, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Train the model using the training set and validate on the validation set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Select the best model based on the validation set\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_rf.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Print the validation results\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    print(f\"Best n_estimators: {best_rf.n_estimators}, Best max_depth: {best_rf.max_depth}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "    # Combine the training and validation sets for final model training\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    \n",
    "    # Retrain the model using the combined training and validation sets\n",
    "    best_rf.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_rf.predict(X_test)\n",
    "    \n",
    "    # Print the predictions for the test set\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
