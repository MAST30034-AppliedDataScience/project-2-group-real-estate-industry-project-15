{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rental History Data\n",
    "one_bed_flat = pd.read_csv('../data/raw/rental_history/one_bed_flat.csv')\n",
    "two_bed_flat = pd.read_csv('../data/raw/rental_history/two_bed_flat.csv')\n",
    "three_bed_flat = pd.read_csv('../data/raw/rental_history/three_bed_flat.csv')\n",
    "two_bed_house = pd.read_csv('../data/raw/rental_history/two_bed_house.csv')\n",
    "three_bed_house = pd.read_csv('../data/raw/rental_history/three_bed_house.csv')\n",
    "four_bed_house = pd.read_csv('../data/raw/rental_history/four_bed_house.csv')\n",
    "all_properties = pd.read_csv('../data/raw/rental_history/all_properties.csv')\n",
    "\n",
    "# Domain Rental Data\n",
    "domain_one_bed_flat = pd.read_csv('../data/curated/domain_one_bed_flat_rent.csv')\n",
    "domain_two_bed_flat = pd.read_csv('../data/curated/domain_two_bed_flat_rent.csv')\n",
    "domain_three_bed_flat = pd.read_csv('../data/curated/domain_three_bed_flat_rent.csv')\n",
    "domain_two_bed_house = pd.read_csv('../data/curated/domain_two_bed_house_rent.csv')\n",
    "domain_three_bed_house = pd.read_csv('../data/curated/domain_three_bed_house_rent.csv')\n",
    "domain_four_bed_house = pd.read_csv('../data/curated/domain_four_bed_house.csv')\n",
    "domain_all_properties = pd.read_csv('../data/curated/domain_all_properties_rent.csv')\n",
    "\n",
    "# Other engineered feature sets \n",
    "crimes = pd.read_csv('../data/curated/crimes.csv')\n",
    "population = pd.read_csv('../data/curated/final_population.csv')\n",
    "education = pd.read_csv('../data/curated/education_df.csv')\n",
    "urban_landmarks = pd.read_csv('../data/raw/urban_landmarks_features.csv')\n",
    "pt_distances = pd.read_csv('../data/curated/suburb_transport_distances.csv')\n",
    "income = pd.read_csv('../data/curated/income.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Rental Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_domain_df(df):\n",
    "    \"\"\"\n",
    "    This function cleans the domain dataframes by removing\n",
    "    the 'Unnamed:' column, renaming median_rent to 'sep_median'\n",
    "    (for a standardised column name as in rental history dfs) and\n",
    "    also creates a year column and inputs the relevant year that\n",
    "    the data is from - 2024. \n",
    "    \"\"\"\n",
    "\n",
    "    # Drop columns that contain 'Unnamed:' in their name\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed:')]\n",
    "    \n",
    "    # Rename the 'median_rent' column to 'sep_median'\n",
    "    if 'median_rent' in df.columns:\n",
    "        df = df.rename(columns={'median_rent': 'sep_median'})\n",
    "    \n",
    "    # Add a 'year' column with value 2024 for each row\n",
    "    df['year'] = 2024\n",
    "\n",
    "    # Reorder columns to make 'year' the second column\n",
    "    cols = list(df.columns)\n",
    "    cols.insert(1, cols.pop(cols.index('year')))\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the clean_domain_df function to all the domain dataframes\n",
    "domain_one_bed_flat = clean_domain_df(domain_one_bed_flat)\n",
    "domain_two_bed_flat = clean_domain_df(domain_two_bed_flat)\n",
    "domain_three_bed_flat = clean_domain_df(domain_three_bed_flat)\n",
    "domain_two_bed_house = clean_domain_df(domain_two_bed_house)\n",
    "domain_three_bed_house = clean_domain_df(domain_three_bed_house)\n",
    "domain_four_bed_house = clean_domain_df(domain_four_bed_house)\n",
    "domain_all_properties = clean_domain_df(domain_all_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the Sep median price from scraped properties into the rental history dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_sep_2024_rental_data(rental_history_df, domain_df):\n",
    "    \"\"\"\n",
    "    This function retrieves all the median rental prices in \n",
    "    September from the domain dataframes and then imputes\n",
    "    them into the rental history dataframes where the year\n",
    "    is 2024 and month is September. \n",
    "    \"\"\"\n",
    "\n",
    "    # Merge rental_history_df with domain_df on 'suburb' to keep all years from rental_history_df\n",
    "    merged_df = pd.merge(rental_history_df, domain_df[['suburb', 'year', 'sep_median']],\n",
    "                         on=['suburb'], how='left', suffixes=('', '_domain'))\n",
    "    \n",
    "    # Replace sep_median values with domain values only for rows where year == 2024\n",
    "    condition = (merged_df['year'] == 2024) & merged_df['sep_median_domain'].notna()\n",
    "    merged_df.loc[condition, 'sep_median'] = merged_df.loc[condition, 'sep_median_domain']\n",
    "    \n",
    "    # Drop the domain-specific columns used for imputation\n",
    "    merged_df.drop(columns=['sep_median_domain', 'year_domain'], inplace=True)\n",
    "\n",
    "    # Filter the dataframe to keep only the suburbs that appear 9 or more times\n",
    "    suburb_counts = merged_df['suburb'].value_counts()\n",
    "    suburbs_to_keep = suburb_counts[suburb_counts >= 9].index\n",
    "    merged_df = merged_df[merged_df['suburb'].isin(suburbs_to_keep)]\n",
    "    \n",
    "    # Drop the sep_median column from the domain DataFrame\n",
    "    domain_df = domain_df.drop(columns=['year', 'sep_median', 'num_properties'], errors='ignore')\n",
    "    \n",
    "    return merged_df, domain_df\n",
    "\n",
    "# Apply the function to each dataset \n",
    "one_bed_flat, domain_one_bed_flat = impute_sep_2024_rental_data(one_bed_flat, domain_one_bed_flat)\n",
    "two_bed_flat, domain_two_bed_flat = impute_sep_2024_rental_data(two_bed_flat, domain_two_bed_flat)\n",
    "three_bed_flat, domain_three_bed_flat = impute_sep_2024_rental_data(three_bed_flat, domain_three_bed_flat)\n",
    "two_bed_house, domain_two_bed_house = impute_sep_2024_rental_data(two_bed_house, domain_two_bed_house)\n",
    "three_bed_house, domain_three_bed_house = impute_sep_2024_rental_data(three_bed_house, domain_three_bed_house)\n",
    "four_bed_house, domain_four_bed_house = impute_sep_2024_rental_data(four_bed_house, domain_four_bed_house)\n",
    "all_properties, domain_all_properties = impute_sep_2024_rental_data(all_properties, domain_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>suburb</th>\n",
       "      <th>year</th>\n",
       "      <th>dec_median</th>\n",
       "      <th>jun_median</th>\n",
       "      <th>mar_median</th>\n",
       "      <th>sep_median</th>\n",
       "      <th>offence_division_1</th>\n",
       "      <th>offence_division_2</th>\n",
       "      <th>offence_division_3</th>\n",
       "      <th>offence_division_4</th>\n",
       "      <th>...</th>\n",
       "      <th>distance_to_restaurant</th>\n",
       "      <th>distance_to_supermarket</th>\n",
       "      <th>nearest_transport_avg_distance</th>\n",
       "      <th>distance_to_cbd</th>\n",
       "      <th>median_bath</th>\n",
       "      <th>median_parkings</th>\n",
       "      <th>furnished_count</th>\n",
       "      <th>unfurnished_count</th>\n",
       "      <th>pets_allowed</th>\n",
       "      <th>pets_not_allowed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2016</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2017</td>\n",
       "      <td>400.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>395.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2018</td>\n",
       "      <td>410.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1162.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2019</td>\n",
       "      <td>420.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1053.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abbotsford</td>\n",
       "      <td>2020</td>\n",
       "      <td>390.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>985.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141601</td>\n",
       "      <td>1.083238</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2023</td>\n",
       "      <td>295.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2024</td>\n",
       "      <td>325.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>607.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.0</td>\n",
       "      <td>604.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>yarraville</td>\n",
       "      <td>2027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>104.0</td>\n",
       "      <td>597.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.032864</td>\n",
       "      <td>0.937960</td>\n",
       "      <td>1.826667</td>\n",
       "      <td>10.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1584 rows Ã— 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          suburb  year  dec_median  jun_median  mar_median  sep_median  \\\n",
       "0     abbotsford  2016       380.0       380.0       380.0       380.0   \n",
       "1     abbotsford  2017       400.0       390.0       390.0       395.0   \n",
       "2     abbotsford  2018       410.0       400.0       400.0       400.0   \n",
       "3     abbotsford  2019       420.0       420.0       410.0       420.0   \n",
       "4     abbotsford  2020       390.0       418.0       420.0       410.0   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "1615  yarraville  2023       295.0       270.0       275.0       280.0   \n",
       "1616  yarraville  2024       325.0       325.0       325.0       320.0   \n",
       "1617  yarraville  2025         NaN         NaN         NaN         NaN   \n",
       "1618  yarraville  2026         NaN         NaN         NaN         NaN   \n",
       "1619  yarraville  2027         NaN         NaN         NaN         NaN   \n",
       "\n",
       "      offence_division_1  offence_division_2  offence_division_3  \\\n",
       "0                  107.0              1065.0                76.0   \n",
       "1                  138.0              1019.0                64.0   \n",
       "2                  100.0              1162.0                88.0   \n",
       "3                  175.0              1053.0               178.0   \n",
       "4                  145.0               985.0               151.0   \n",
       "...                  ...                 ...                 ...   \n",
       "1615                96.0               633.0                68.0   \n",
       "1616               101.0               607.0                60.0   \n",
       "1617               102.0               604.0                63.0   \n",
       "1618               103.0               600.0                65.0   \n",
       "1619               104.0               597.0                68.0   \n",
       "\n",
       "      offence_division_4  ...  distance_to_restaurant  \\\n",
       "0                   59.0  ...                1.141601   \n",
       "1                   69.0  ...                1.141601   \n",
       "2                   84.0  ...                1.141601   \n",
       "3                  114.0  ...                1.141601   \n",
       "4                   89.0  ...                1.141601   \n",
       "...                  ...  ...                     ...   \n",
       "1615                31.0  ...                1.032864   \n",
       "1616                40.0  ...                1.032864   \n",
       "1617                40.0  ...                1.032864   \n",
       "1618                39.0  ...                1.032864   \n",
       "1619                38.0  ...                1.032864   \n",
       "\n",
       "      distance_to_supermarket  nearest_transport_avg_distance  \\\n",
       "0                    1.083238                        1.110000   \n",
       "1                    1.083238                        1.110000   \n",
       "2                    1.083238                        1.110000   \n",
       "3                    1.083238                        1.110000   \n",
       "4                    1.083238                        1.110000   \n",
       "...                       ...                             ...   \n",
       "1615                 0.937960                        1.826667   \n",
       "1616                 0.937960                        1.826667   \n",
       "1617                 0.937960                        1.826667   \n",
       "1618                 0.937960                        1.826667   \n",
       "1619                 0.937960                        1.826667   \n",
       "\n",
       "      distance_to_cbd  median_bath  median_parkings  furnished_count  \\\n",
       "0                5.55            1                0                2   \n",
       "1                5.55            1                0                2   \n",
       "2                5.55            1                0                2   \n",
       "3                5.55            1                0                2   \n",
       "4                5.55            1                0                2   \n",
       "...               ...          ...              ...              ...   \n",
       "1615            10.21            1                1                0   \n",
       "1616            10.21            1                1                0   \n",
       "1617            10.21            1                1                0   \n",
       "1618            10.21            1                1                0   \n",
       "1619            10.21            1                1                0   \n",
       "\n",
       "      unfurnished_count  pets_allowed  pets_not_allowed  \n",
       "0                    10             4                 8  \n",
       "1                    10             4                 8  \n",
       "2                    10             4                 8  \n",
       "3                    10             4                 8  \n",
       "4                    10             4                 8  \n",
       "...                 ...           ...               ...  \n",
       "1615                  1             0                 1  \n",
       "1616                  1             0                 1  \n",
       "1617                  1             0                 1  \n",
       "1618                  1             0                 1  \n",
       "1619                  1             0                 1  \n",
       "\n",
       "[1584 rows x 89 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one_bed_flat\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, education, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "one_bed_flat_merged = pd.merge(one_bed_flat_merged, domain_one_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in one_bed_flat and the population dataframe\n",
    "one_bed_flat_merged = one_bed_flat_merged[one_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_flat\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, education, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_flat_merged = pd.merge(two_bed_flat_merged, domain_two_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_flat and the population dataframe\n",
    "two_bed_flat_merged = two_bed_flat_merged[two_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_flat\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, education, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_flat_merged = pd.merge(three_bed_flat_merged, domain_three_bed_flat, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_flat and the population dataframe\n",
    "three_bed_flat_merged = three_bed_flat_merged[three_bed_flat_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge two_bed_house\n",
    "two_bed_house_merged = pd.merge(two_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, education, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "two_bed_house_merged = pd.merge(two_bed_house_merged, domain_two_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in two_bed_house and the population dataframe\n",
    "two_bed_house_merged = two_bed_house_merged[two_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge three_bed_house\n",
    "three_bed_house_merged = pd.merge(three_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, education, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "three_bed_house_merged = pd.merge(three_bed_house_merged, domain_three_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in three_bed_house and the population dataframe\n",
    "three_bed_house_merged = three_bed_house_merged[three_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge four_bed_house\n",
    "four_bed_house_merged = pd.merge(four_bed_house, crimes, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, income, on=['suburb', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, education, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, pt_distances, on='suburb', how='inner')\n",
    "four_bed_house_merged = pd.merge(four_bed_house_merged, domain_four_bed_house, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in four_bed_house and the population dataframe\n",
    "four_bed_house_merged = four_bed_house_merged[four_bed_house_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "# Merge all_properties\n",
    "all_properties_merged = pd.merge(all_properties, crimes, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, income, on=['suburb', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, population, left_on=['suburb', 'year'], right_on=['sa2_name', 'year'], how='outer')\n",
    "all_properties_merged = pd.merge(all_properties_merged, education, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, urban_landmarks, left_on='suburb', right_on='gazetted_locality', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, pt_distances, on='suburb', how='inner')\n",
    "all_properties_merged = pd.merge(all_properties_merged, domain_all_properties, on='suburb', how='inner')\n",
    "\n",
    "# Filter rows to retain only matching suburbs in all_properties and the population dataframe\n",
    "all_properties_merged = all_properties_merged[all_properties_merged['suburb'].isin(population['sa2_name'])]\n",
    "\n",
    "\n",
    "one_bed_flat_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all other suburb column names. Only keep the first suburb column \n",
    "def clean_suburb_cols(df):\n",
    "    \"\"\"\n",
    "    This function removes all duplicated of the suburb column name \n",
    "    from the merged dataframes. The duplicate suburb column name \n",
    "    could be 'Unnamed', 'sa2_name' or 'gazetted_locality'.\n",
    "    \"\"\"\n",
    "    df = df.loc[:, ~df.columns.str.contains('Unnamed')]  # removes the duplicate 'suburb' column\n",
    "    columns_to_drop = ['sa2_name', 'gazetted_locality']\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Clean the column names\n",
    "one_bed_flat_merged = clean_suburb_cols(one_bed_flat_merged)\n",
    "two_bed_flat_merged = clean_suburb_cols(two_bed_flat_merged)\n",
    "three_bed_flat_merged = clean_suburb_cols(three_bed_flat_merged)\n",
    "two_bed_house_merged = clean_suburb_cols(two_bed_house_merged)\n",
    "three_bed_house_merged = clean_suburb_cols(three_bed_house_merged)\n",
    "four_bed_house_merged = clean_suburb_cols(four_bed_house_merged)\n",
    "all_properties_merged = clean_suburb_cols(all_properties_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_before_2025(df, median_columns):\n",
    "    # Drop rows where year is less than 2025 and NaN values exist in any of the median columns\n",
    "    return df[~((df['year'] < 2025) & (df[median_columns].isnull().any(axis=1)))]\n",
    "\n",
    "# Define the median columns to check\n",
    "median_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "# Call the function for each dataframe and reassign the cleaned data\n",
    "one_bed_flat_merged = remove_nan_before_2025(one_bed_flat_merged, median_columns)\n",
    "two_bed_flat_merged = remove_nan_before_2025(two_bed_flat_merged, median_columns)\n",
    "three_bed_flat_merged = remove_nan_before_2025(three_bed_flat_merged, median_columns)\n",
    "two_bed_house_merged = remove_nan_before_2025(two_bed_house_merged, median_columns)\n",
    "three_bed_house_merged = remove_nan_before_2025(three_bed_house_merged, median_columns)\n",
    "four_bed_house_merged = remove_nan_before_2025(four_bed_house_merged, median_columns)\n",
    "all_properties_merged = remove_nan_before_2025(all_properties_merged, median_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save All Properties Dataframe for Visualisation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df to a CSV file\n",
    "all_properties_merged.to_csv('../data/curated/all_properties_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train & Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_sets(df):\n",
    "    \"\"\"\n",
    "    This function first splits the dataframe into training, validation, \n",
    "    and testing sets based on the 'year' column:\n",
    "    - Training set includes data from the years 2016-2021.\n",
    "    - Validation set includes data from the years 2022-2024.\n",
    "    - Testing set includes data from the years 2025-2027.\n",
    "\n",
    "    It then merges additional columns that are not part of the \n",
    "    features specific to years or target columns back with the \n",
    "    respective sets based on matching suburbs.\n",
    "\n",
    "    The function then returns the follow dataframes:\n",
    "    - X_train: Training feature set.\n",
    "    - X_val: Validation feature set.\n",
    "    - X_test: Testing feature set.\n",
    "    - y_train: Training target set.\n",
    "    - y_val: Validation target set.\n",
    "    - y_test: Testing target set.\n",
    "    \"\"\"\n",
    "    # Define the year ranges for training, validation, and testing sets\n",
    "    train_years = range(2016, 2022)\n",
    "    val_years = range(2022, 2025)\n",
    "    test_years = range(2025, 2028)\n",
    "\n",
    "    # Columns to include in X (specific to the years) and y splits\n",
    "    feature_year_cols = ['suburb', 'year', 'offence_division_1', 'offence_division_2', 'offence_division_3', 'offence_division_4', 'offence_division_5', 'offence_division_6', 'erp']\n",
    "    target_columns = ['dec_median', 'jun_median', 'mar_median', 'sep_median']\n",
    "\n",
    "    # Add the income columns that start with 'hi_' and end with '_tot' using regex\n",
    "    regex_pattern = r'^hi_.*_tot$'\n",
    "    hi_tot_cols = df.filter(regex=regex_pattern).columns.tolist()\n",
    "    # Combine to the feature_year_cols\n",
    "    feature_year_cols += hi_tot_cols\n",
    "\n",
    "    # Split features (specific to the years) and target data\n",
    "    X = df[feature_year_cols]\n",
    "    y = df[target_columns]\n",
    "\n",
    "    # Split the dataframe into training, validation, and testing sets based on the year\n",
    "    X_train = X[X['year'].isin(train_years)]\n",
    "    X_val = X[X['year'].isin(val_years)]\n",
    "    X_test = X[X['year'].isin(test_years)]\n",
    "\n",
    "    # Align target sets with the corresponding feature sets\n",
    "    y_train = y.loc[X_train.index]\n",
    "    y_val = y.loc[X_val.index]\n",
    "    y_test = y.loc[X_test.index]\n",
    "\n",
    "    # Extract other columns not in feature_year_cols or target_columns, including 'suburb'\n",
    "    other_columns = df.drop(columns=feature_year_cols + target_columns).columns\n",
    "    other_data = df[other_columns].copy()\n",
    "    other_data['suburb'] = df['suburb']  # Ensure 'suburb' is included\n",
    "\n",
    "    # Merge the 'other' data back with the matching suburbs, irrespective of the year\n",
    "    X_train = X_train.merge(other_data, on='suburb', how='left')\n",
    "    X_val = X_val.merge(other_data, on='suburb', how='left')\n",
    "    X_test = X_test.merge(other_data, on='suburb', how='left')\n",
    "\n",
    "    # Drop dupliactes from the X dfs\n",
    "    X_train = X_train.drop_duplicates()\n",
    "    X_val = X_val.drop_duplicates()\n",
    "    X_test = X_test.drop_duplicates()\n",
    "\n",
    "    # Now drop 'suburb' and 'year' from the feature sets\n",
    "    X_train = X_train.drop(columns=['suburb', 'year'])\n",
    "    X_val = X_val.drop(columns=['suburb', 'year'])\n",
    "    X_test = X_test.drop(columns=['suburb', 'year'])\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Create training, validation, and test sets for each property type\n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed = train_val_test_sets(one_bed_flat_merged)\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed = train_val_test_sets(two_bed_flat_merged)\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed = train_val_test_sets(three_bed_flat_merged)\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house = train_val_test_sets(two_bed_house_merged)\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house = train_val_test_sets(three_bed_house_merged)\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house = train_val_test_sets(four_bed_house_merged)\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties = train_val_test_sets(all_properties_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see all the X columns are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'One Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Two Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Three Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'Four Bed House': 'Columns are the same in all three sets (train, validation, test).',\n",
       " 'All Properties': 'Columns are the same in all three sets (train, validation, test).'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_feature_columns(train, val, test):\n",
    "    \"\"\"\n",
    "    This function compares columns of the training, validation, and \n",
    "    testing feature dataframes (X). A dictionary is then returned\n",
    "    indicating if any columns are missing in each set or if all\n",
    "    the colums are the same. \n",
    "    \"\"\"\n",
    "    comparison_result = {}\n",
    "    # Check if columns match between train, validation, and test sets\n",
    "    train_val_match = train.columns.equals(val.columns)\n",
    "    train_test_match = train.columns.equals(test.columns)\n",
    "    val_test_match = val.columns.equals(test.columns)\n",
    "    \n",
    "    if not (train_val_match and train_test_match and val_test_match):\n",
    "        missing_in_val = set(train.columns) - set(val.columns)\n",
    "        missing_in_train_val = set(val.columns) - set(train.columns)\n",
    "        missing_in_test = set(train.columns) - set(test.columns)\n",
    "        missing_in_train_test = set(test.columns) - set(train.columns)\n",
    "        missing_in_val_test = set(val.columns) - set(test.columns)\n",
    "        missing_in_test_val = set(test.columns) - set(val.columns)\n",
    "\n",
    "        comparison_result = {\n",
    "            \"Columns missing in validation set compared to train\": list(missing_in_val),\n",
    "            \"Columns missing in train set compared to validation\": list(missing_in_train_val),\n",
    "            \"Columns missing in test set compared to train\": list(missing_in_test),\n",
    "            \"Columns missing in train set compared to test\": list(missing_in_train_test),\n",
    "            \"Columns missing in test set compared to validation\": list(missing_in_val_test),\n",
    "            \"Columns missing in validation set compared to test\": list(missing_in_test_val),\n",
    "        }\n",
    "    else:\n",
    "        comparison_result = \"Columns are the same in all three sets (train, validation, test).\"\n",
    "\n",
    "    return comparison_result\n",
    "\n",
    "# List of training, validation, and testing DataFrames to compare\n",
    "feature_dfs = {\n",
    "    \"One Bed\": (X_train_one_bed, X_val_one_bed, X_test_one_bed),\n",
    "    \"Two Bed\": (X_train_two_bed, X_val_two_bed, X_test_two_bed),\n",
    "    \"Three Bed\": (X_train_three_bed, X_val_three_bed, X_test_three_bed),\n",
    "    \"Two Bed House\": (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house),\n",
    "    \"Three Bed House\": (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house),\n",
    "    \"Four Bed House\": (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house),\n",
    "    \"All Properties\": (X_train_all_properties, X_val_all_properties, X_test_all_properties)\n",
    "}\n",
    "\n",
    "# Compare columns for each triplet of training, validation, and testing sets\n",
    "comparison_results = {name: compare_feature_columns(train, val, test) for name, (train, val, test) in feature_dfs.items()}\n",
    "\n",
    "comparison_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes to check for missing values\n",
    "dataframes = {\n",
    "    'X_train_one_bed': X_train_one_bed,\n",
    "    'X_val_one_bed': X_val_one_bed,\n",
    "    'X_test_one_bed': X_test_one_bed,\n",
    "    'y_train_one_bed': y_train_one_bed,\n",
    "    'y_val_one_bed': y_val_one_bed,\n",
    "    'y_test_one_bed': y_test_one_bed,\n",
    "    \n",
    "    'X_train_two_bed': X_train_two_bed,\n",
    "    'X_val_two_bed': X_val_two_bed,\n",
    "    'X_test_two_bed': X_test_two_bed,\n",
    "    'y_train_two_bed': y_train_two_bed,\n",
    "    'y_val_two_bed': y_val_two_bed,\n",
    "    'y_test_two_bed': y_test_two_bed,\n",
    "    \n",
    "    'X_train_three_bed': X_train_three_bed,\n",
    "    'X_val_three_bed': X_val_three_bed,\n",
    "    'X_test_three_bed': X_test_three_bed,\n",
    "    'y_train_three_bed': y_train_three_bed,\n",
    "    'y_val_three_bed': y_val_three_bed,\n",
    "    'y_test_three_bed': y_test_three_bed,\n",
    "    \n",
    "    'X_train_two_bed_house': X_train_two_bed_house,\n",
    "    'X_val_two_bed_house': X_val_two_bed_house,\n",
    "    'X_test_two_bed_house': X_test_two_bed_house,\n",
    "    'y_train_two_bed_house': y_train_two_bed_house,\n",
    "    'y_val_two_bed_house': y_val_two_bed_house,\n",
    "    'y_test_two_bed_house': y_test_two_bed_house,\n",
    "    \n",
    "    'X_train_three_bed_house': X_train_three_bed_house,\n",
    "    'X_val_three_bed_house': X_val_three_bed_house,\n",
    "    'X_test_three_bed_house': X_test_three_bed_house,\n",
    "    'y_train_three_bed_house': y_train_three_bed_house,\n",
    "    'y_val_three_bed_house': y_val_three_bed_house,\n",
    "    'y_test_three_bed_house': y_test_three_bed_house,\n",
    "    \n",
    "    'X_train_four_bed_house': X_train_four_bed_house,\n",
    "    'X_val_four_bed_house': X_val_four_bed_house,\n",
    "    'X_test_four_bed_house': X_test_four_bed_house,\n",
    "    'y_train_four_bed_house': y_train_four_bed_house,\n",
    "    'y_val_four_bed_house': y_val_four_bed_house,\n",
    "    'y_test_four_bed_house': y_test_four_bed_house,\n",
    "    \n",
    "    'X_train_all_properties': X_train_all_properties,\n",
    "    'X_val_all_properties': X_val_all_properties,\n",
    "    'X_test_all_properties': X_test_all_properties,\n",
    "    'y_train_all_properties': y_train_all_properties,\n",
    "    'y_val_all_properties': y_val_all_properties,\n",
    "    'y_test_all_properties': y_test_all_properties,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_test_one_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1606         NaN         NaN         NaN         NaN\n",
      "1607         NaN         NaN         NaN         NaN\n",
      "1617         NaN         NaN         NaN         NaN\n",
      "1618         NaN         NaN         NaN         NaN\n",
      "1619         NaN         NaN         NaN         NaN\n",
      "\n",
      "[396 rows x 4 columns], 'y_test_two_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2038         NaN         NaN         NaN         NaN\n",
      "2039         NaN         NaN         NaN         NaN\n",
      "2049         NaN         NaN         NaN         NaN\n",
      "2050         NaN         NaN         NaN         NaN\n",
      "2051         NaN         NaN         NaN         NaN\n",
      "\n",
      "[504 rows x 4 columns], 'y_test_three_bed':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1486         NaN         NaN         NaN         NaN\n",
      "1487         NaN         NaN         NaN         NaN\n",
      "1497         NaN         NaN         NaN         NaN\n",
      "1498         NaN         NaN         NaN         NaN\n",
      "1499         NaN         NaN         NaN         NaN\n",
      "\n",
      "[372 rows x 4 columns], 'y_test_two_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "1918         NaN         NaN         NaN         NaN\n",
      "1919         NaN         NaN         NaN         NaN\n",
      "1929         NaN         NaN         NaN         NaN\n",
      "1930         NaN         NaN         NaN         NaN\n",
      "1931         NaN         NaN         NaN         NaN\n",
      "\n",
      "[477 rows x 4 columns], 'y_test_three_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2362         NaN         NaN         NaN         NaN\n",
      "2363         NaN         NaN         NaN         NaN\n",
      "2373         NaN         NaN         NaN         NaN\n",
      "2374         NaN         NaN         NaN         NaN\n",
      "2375         NaN         NaN         NaN         NaN\n",
      "\n",
      "[588 rows x 4 columns], 'y_test_four_bed_house':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2050         NaN         NaN         NaN         NaN\n",
      "2051         NaN         NaN         NaN         NaN\n",
      "2061         NaN         NaN         NaN         NaN\n",
      "2062         NaN         NaN         NaN         NaN\n",
      "2063         NaN         NaN         NaN         NaN\n",
      "\n",
      "[513 rows x 4 columns], 'y_test_all_properties':       dec_median  jun_median  mar_median  sep_median\n",
      "9            NaN         NaN         NaN         NaN\n",
      "10           NaN         NaN         NaN         NaN\n",
      "11           NaN         NaN         NaN         NaN\n",
      "21           NaN         NaN         NaN         NaN\n",
      "22           NaN         NaN         NaN         NaN\n",
      "...          ...         ...         ...         ...\n",
      "2398         NaN         NaN         NaN         NaN\n",
      "2399         NaN         NaN         NaN         NaN\n",
      "2409         NaN         NaN         NaN         NaN\n",
      "2410         NaN         NaN         NaN         NaN\n",
      "2411         NaN         NaN         NaN         NaN\n",
      "\n",
      "[594 rows x 4 columns]}\n",
      "{'y_test_one_bed': dec_median    396\n",
      "jun_median    396\n",
      "mar_median    396\n",
      "sep_median    396\n",
      "dtype: int64, 'y_test_two_bed': dec_median    504\n",
      "jun_median    504\n",
      "mar_median    504\n",
      "sep_median    504\n",
      "dtype: int64, 'y_test_three_bed': dec_median    372\n",
      "jun_median    372\n",
      "mar_median    372\n",
      "sep_median    372\n",
      "dtype: int64, 'y_test_two_bed_house': dec_median    477\n",
      "jun_median    477\n",
      "mar_median    477\n",
      "sep_median    477\n",
      "dtype: int64, 'y_test_three_bed_house': dec_median    588\n",
      "jun_median    588\n",
      "mar_median    588\n",
      "sep_median    588\n",
      "dtype: int64, 'y_test_four_bed_house': dec_median    513\n",
      "jun_median    513\n",
      "mar_median    513\n",
      "sep_median    513\n",
      "dtype: int64, 'y_test_all_properties': dec_median    594\n",
      "jun_median    594\n",
      "mar_median    594\n",
      "sep_median    594\n",
      "dtype: int64}\n"
     ]
    }
   ],
   "source": [
    "# Collecting rows with missing values for each dataframe\n",
    "missing_rows_summary = {}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    rows_with_missing = df[df.isnull().any(axis=1)]\n",
    "    if not rows_with_missing.empty:\n",
    "        missing_rows_summary[name] = rows_with_missing\n",
    "\n",
    "print(missing_rows_summary)\n",
    "\n",
    "# Check for missing values in each dataframe by columns \n",
    "missing_values_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0]\n",
    "    if not columns_with_missing.empty:\n",
    "        missing_values_summary[name] = columns_with_missing\n",
    "\n",
    "print(missing_values_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 25\n",
      "Explained variance by selected components: 0.91\n",
      "Number of components selected: 27\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 26\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 29\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 31\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 30\n",
      "Explained variance by selected components: 0.90\n",
      "Number of components selected: 28\n",
      "Explained variance by selected components: 0.90\n"
     ]
    }
   ],
   "source": [
    "def pca_feature_selection(X_train, X_val, X_test, variance_threshold=0.9):\n",
    "    \"\"\"\n",
    "    The function applies PCA for dimensionality reduction by fitting on the\n",
    "    training set and transforming both the training and test sets. It keeps \n",
    "    a select number of components based on the defined variance \n",
    "    threshold. It returns the reduced training and test sets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Standardise the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Initialise PCA, specifying the variance threshold\n",
    "    pca_temp = PCA().fit(X_train_scaled)\n",
    "    cumulative_variance = pca_temp.explained_variance_ratio_.cumsum()\n",
    "    # Find the number of components to capture the specified variance\n",
    "    n_components = next(i for i, total_variance in enumerate(cumulative_variance) if total_variance >= variance_threshold) + 1\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA on the training set and transform both training and test sets\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    print(f\"Number of components selected: {n_components}\")\n",
    "    print(f\"Explained variance by selected components: {sum(pca.explained_variance_ratio_):.2f}\")\n",
    "\n",
    "    return X_train_pca, X_val_pca, X_test_pca\n",
    "\n",
    "# Perform feature selection with PCA on the X sets \n",
    "X_train_one_bed, X_val_one_bed, X_test_one_bed = pca_feature_selection(X_train_one_bed, X_val_one_bed, X_test_one_bed)\n",
    "X_train_two_bed, X_val_two_bed, X_test_two_bed = pca_feature_selection(X_train_two_bed, X_val_two_bed, X_test_two_bed)\n",
    "X_train_three_bed, X_val_three_bed, X_test_three_bed = pca_feature_selection(X_train_three_bed, X_val_three_bed, X_test_three_bed)\n",
    "X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house = pca_feature_selection(X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house)\n",
    "X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house = pca_feature_selection(X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house)\n",
    "X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house = pca_feature_selection(X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house)\n",
    "X_train_all_properties, X_val_all_properties, X_test_all_properties = pca_feature_selection(X_train_all_properties, X_val_all_properties, X_test_all_properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of all training, validation, and test sets\n",
    "ML_dfs = [\n",
    "    (X_train_one_bed, X_val_one_bed, X_test_one_bed, y_train_one_bed, y_val_one_bed, y_test_one_bed),\n",
    "    (X_train_two_bed, X_val_two_bed, X_test_two_bed, y_train_two_bed, y_val_two_bed, y_test_two_bed),\n",
    "    (X_train_three_bed, X_val_three_bed, X_test_three_bed, y_train_three_bed, y_val_three_bed, y_test_three_bed),\n",
    "    (X_train_two_bed_house, X_val_two_bed_house, X_test_two_bed_house, y_train_two_bed_house, y_val_two_bed_house, y_test_two_bed_house),\n",
    "    (X_train_three_bed_house, X_val_three_bed_house, X_test_three_bed_house, y_train_three_bed_house, y_val_three_bed_house, y_test_three_bed_house),\n",
    "    (X_train_four_bed_house, X_val_four_bed_house, X_test_four_bed_house, y_train_four_bed_house, y_val_four_bed_house, y_test_four_bed_house),\n",
    "    (X_train_all_properties, X_val_all_properties, X_test_all_properties, y_train_all_properties, y_val_all_properties, y_test_all_properties),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Best alpha: 14.384498882876601\n",
      "Validation MSE: 5009.8348, R^2: 0.1062, Validation MAE: 50.9748\n",
      "Predictions for 2025-2027: [[323.14012052 321.97013048 321.45366707 322.19933826]\n",
      " [324.09682223 323.02173817 322.54618006 323.2211289 ]\n",
      " [325.06802744 324.08878311 323.65467291 324.25810648]\n",
      " ...\n",
      " [331.76980198 331.91121442 332.04567406 331.73020899]\n",
      " [333.47896818 333.7738687  333.99678321 333.54874198]\n",
      " [335.12084015 335.56527067 335.8732931  335.29665727]]\n",
      "\n",
      "Dataset 2:\n",
      "Best alpha: 14.384498882876601\n",
      "Validation MSE: 7408.4227, R^2: 0.1357, Validation MAE: 64.4800\n",
      "Predictions for 2025-2027: [[437.40793109 436.9361802  436.56650009 437.45511115]\n",
      " [438.55238422 438.12690419 437.78043313 438.62108054]\n",
      " [439.71468483 439.33601035 439.01310014 439.80511486]\n",
      " ...\n",
      " [441.17116341 440.14605227 439.82039554 440.80790341]\n",
      " [443.56383461 442.62586383 442.34628355 443.24403381]\n",
      " [445.86638691 445.01370889 444.7784666  445.58950236]]\n",
      "\n",
      "Dataset 3:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 16176.8846, R^2: 0.4784, Validation MAE: 87.7155\n",
      "Predictions for 2025-2027: [[657.07790739 658.75175052 656.07078448 658.4670284 ]\n",
      " [659.29473533 661.33365205 658.79459999 660.85585744]\n",
      " [661.53143828 663.94391739 661.54861038 663.26820563]\n",
      " ...\n",
      " [653.88293897 652.20373731 649.9813098  655.03055127]\n",
      " [657.09503492 655.98630424 653.94680669 658.53851518]\n",
      " [660.24366762 659.70258065 657.84467892 661.98099023]]\n",
      "\n",
      "Dataset 4:\n",
      "Best alpha: 5.455594781168514\n",
      "Validation MSE: 7772.7384, R^2: 0.4460, Validation MAE: 65.3768\n",
      "Predictions for 2025-2027: [[500.40806524 495.86559804 493.44357952 498.92635272]\n",
      " [502.29115134 497.83100176 495.41255775 500.84516962]\n",
      " [504.20086675 499.82454856 497.40958937 502.79155454]\n",
      " ...\n",
      " [537.93565248 533.82399742 531.14984615 536.27283222]\n",
      " [543.70382567 539.62631543 536.92048055 542.06402033]\n",
      " [549.39713104 545.35168641 542.61577053 547.7769886 ]]\n",
      "\n",
      "Dataset 5:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 14286.3811, R^2: 0.5296, Validation MAE: 87.1446\n",
      "Predictions for 2025-2027: [[651.58086381 645.97204061 642.5141476  648.99072197]\n",
      " [654.7616994  649.25350419 645.87694612 652.22464389]\n",
      " [657.92179115 652.51785302 649.22414749 655.44072303]\n",
      " ...\n",
      " [650.11605528 640.56186873 639.79609927 644.91497948]\n",
      " [659.84534316 650.22377692 649.50128927 654.63553328]\n",
      " [669.53793926 659.8531715  659.17259541 664.31938616]]\n",
      "\n",
      "Dataset 6:\n",
      "Best alpha: 2.06913808111479\n",
      "Validation MSE: 29121.6526, R^2: 0.5752, Validation MAE: 120.6593\n",
      "Predictions for 2025-2027: [[695.60703875 689.52384114 684.85743524 693.44866731]\n",
      " [701.30827802 695.32471318 690.67199795 699.23995324]\n",
      " [706.99763614 701.11928078 696.47921842 705.02259334]\n",
      " ...\n",
      " [828.01573937 819.58581009 818.86543459 822.96038   ]\n",
      " [844.33333741 835.85919156 835.21640267 839.34383281]\n",
      " [860.60384483 852.07271037 851.50674918 855.66415799]]\n",
      "\n",
      "Dataset 7:\n",
      "Best alpha: 0.7847599703514607\n",
      "Validation MSE: 8610.2347, R^2: 0.0354, Validation MAE: 71.3527\n",
      "Predictions for 2025-2027: [[466.16663958 466.35652203 467.8108196  465.83565259]\n",
      " [467.54800523 467.9173769  469.48523813 467.35209629]\n",
      " [468.94166768 469.49456652 471.17803788 468.88368716]\n",
      " ...\n",
      " [489.21216268 490.03408189 490.75780436 490.4416075 ]\n",
      " [494.04409102 494.97564118 495.80140919 495.37013521]\n",
      " [498.75395292 499.7894404  500.71234048 500.17021956]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for Lasso\n",
    "alpha_range = np.logspace(-6, 2, 20) \n",
    "param_grid = {'alpha': alpha_range}\n",
    "\n",
    "# Loop through each set, tune the model on the validation set, and predict on the test set\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    # Initialise the Lasso model\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    # Set up the GridSearchCV to tune the 'alpha' hyperparameter\n",
    "    grid_search = GridSearchCV(lasso, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Train the model using the training set and validate on the validation set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Select the best model based on the validation set\n",
    "    best_lasso = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_lasso.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_lasso.predict(X_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Property Type {i+1}:\")\n",
    "    print(f\"Best alpha: {best_lasso.alpha}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1:\n",
      "Best n_estimators: 100, Best max_depth: None\n",
      "Validation MSE: 4237.1439, R^2: 0.2528, Validation MAE: 46.2835\n",
      "Predictions for 2025-2027: [[287.785 286.02  285.13  286.405]\n",
      " [286.485 284.31  283.61  285.005]\n",
      " [286.685 284.71  284.11  285.255]\n",
      " ...\n",
      " [298.68  304.69  307.61  299.27 ]\n",
      " [306.43  310.69  313.66  306.77 ]\n",
      " [320.84  323.46  325.56  320.64 ]]\n",
      "\n",
      "Dataset 2:\n",
      "Best n_estimators: 100, Best max_depth: None\n",
      "Validation MSE: 5965.4465, R^2: 0.3082, Validation MAE: 58.7633\n",
      "Predictions for 2025-2027: [[486.91 489.95 499.15 486.4 ]\n",
      " [486.08 489.95 499.15 486.1 ]\n",
      " [482.97 487.15 496.38 483.25]\n",
      " ...\n",
      " [395.24 395.68 397.85 396.53]\n",
      " [397.37 397.78 399.5  398.78]\n",
      " [399.7  400.18 401.95 401.33]]\n",
      "\n",
      "Dataset 3:\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 13930.0568, R^2: 0.5607, Validation MAE: 80.6894\n",
      "Predictions for 2025-2027: [[764.765 762.35  756.155 762.655]\n",
      " [760.45  758.66  753.445 759.495]\n",
      " [758.435 756.63  752.45  757.425]\n",
      " ...\n",
      " [691.345 674.335 652.72  689.31 ]\n",
      " [688.825 671.66  651.58  686.87 ]\n",
      " [683.175 668.92  650.685 682.75 ]]\n",
      "\n",
      "Dataset 4:\n",
      "Best n_estimators: 200, Best max_depth: 10\n",
      "Validation MSE: 5804.7373, R^2: 0.5942, Validation MAE: 57.8412\n",
      "Predictions for 2025-2027: [[581.03438832 576.48961752 576.017435   579.4793314 ]\n",
      " [579.44467274 575.0370411  574.47881525 577.70139015]\n",
      " [578.64328386 574.37259665 574.00886155 576.95898274]\n",
      " ...\n",
      " [490.07585683 488.01576594 486.01146909 489.54184153]\n",
      " [490.68978938 488.44277523 486.57704942 489.97336578]\n",
      " [489.3231756  486.98749589 484.91717315 488.67297531]]\n",
      "\n",
      "Dataset 5:\n",
      "Best n_estimators: 50, Best max_depth: 15\n",
      "Validation MSE: 9228.8630, R^2: 0.6998, Validation MAE: 72.4119\n",
      "Predictions for 2025-2027: [[726.738  733.063  720.9355 735.2605]\n",
      " [719.838  726.583  717.6755 728.4605]\n",
      " [719.733  730.06   719.443  731.891 ]\n",
      " ...\n",
      " [577.38   572.16   572.16   576.56  ]\n",
      " [573.08   567.76   567.96   572.26  ]\n",
      " [572.28   567.26   567.36   571.76  ]]\n",
      "\n",
      "Dataset 6:\n",
      "Best n_estimators: 100, Best max_depth: 10\n",
      "Validation MSE: 17084.7073, R^2: 0.7532, Validation MAE: 96.4188\n",
      "Predictions for 2025-2027: [[717.50025835 715.02926226 712.21925526 714.36656062]\n",
      " [713.52400835 712.92565115 711.18050526 711.88718562]\n",
      " [704.47757978 704.38529401 702.94371954 703.42289991]\n",
      " ...\n",
      " [750.32454134 745.6954759  746.20933527 749.44173748]\n",
      " [751.3091842  746.18294019 746.27831741 749.84713034]\n",
      " [752.33098748 747.08638281 747.5115961  750.99079154]]\n",
      "\n",
      "Dataset 7:\n",
      "Best n_estimators: 200, Best max_depth: None\n",
      "Validation MSE: 7357.1137, R^2: 0.1887, Validation MAE: 65.8928\n",
      "Predictions for 2025-2027: [[457.945 460.255 468.815 459.03 ]\n",
      " [459.37  461.855 469.68  460.555]\n",
      " [458.635 462.355 470.08  460.635]\n",
      " ...\n",
      " [474.285 476.495 478.155 477.585]\n",
      " [473.76  476.    477.53  476.975]\n",
      " [475.475 477.375 478.655 478.38 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters for Random Forest\n",
    "# ADD MORE\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],  # Number of trees\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Loop through each set, tune the model on the validation set, and predict on the test set\n",
    "for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(ML_dfs):\n",
    "    # Initialise the Random Forest model\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Set up the GridSearchCV to tune 'n_estimators' and 'max_depth'\n",
    "    grid_search = GridSearchCV(rf, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Train the model using the training set and validate on the validation set\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Select the best model based on the validation set\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "    # Evaluate on the validation set\n",
    "    y_val_pred = best_rf.predict(X_val)\n",
    "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_test_pred = best_rf.predict(X_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Dataset {i+1}:\")\n",
    "    print(f\"Best n_estimators: {best_rf.n_estimators}, Best max_depth: {best_rf.max_depth}\")\n",
    "    print(f\"Validation MSE: {val_mse:.4f}, R^2: {val_r2:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "    print(f\"Predictions for 2025-2027: {y_test_pred}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
